{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oriental-poverty",
   "metadata": {},
   "source": [
    "##### Esto es para poner en el ejemplo del Informe de tesis, sección \"Word embeddings (Word2vec) & WMD.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-trouble",
   "metadata": {},
   "source": [
    "Vamos a obtener la matriz luego de aplicar TF-IDF a un CV y a dos Descripciones de Puestos. \n",
    "Además, entrenaremos a TF-IDF con estos 3 textos en lugar de los 21217 textos.\n",
    "Luego aplicamos Cosine Similarity entre este CV y las dos descripciones y obtenemos sus resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-wisconsin",
   "metadata": {},
   "source": [
    "#### Import de librerias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "corresponding-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "#Calculo de TFIDF y Cosine:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances as pcd\n",
    "\n",
    "#Word2vec: \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#Para medir tiempos de ejecución:\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "humanitarian-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los DF y vectores de '2-TF_IDF_Cosine_&_Word2vec_WMD':\n",
    "\n",
    "df_compar_cand_jobs = pd.read_pickle('DF_Exportado_Jobs_And_CVs_Para2_1')\n",
    "df_compar_cand_jobs.shape\n",
    "\n",
    "#vector de CVs y Descripciones de Puestos para entrenar TF-IDF:\n",
    "array_CV_and_Jobs_train = np.load('array_CV_and_Jobs_train.npy',allow_pickle=True)\n",
    "\n",
    "#vector de CVs para aplicar Cosine Sim:\n",
    "array_unicode_CV_transf = np.load('array_unicode_CV_transf.npy',allow_pickle=True)\n",
    "\n",
    "#vector de Descripciones de Puestos para aplicar Cosine Sim:\n",
    "array_unicode_Jobs_transf = np.load('array_unicode_Jobs_transf.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "indonesian-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21217\n",
      "3131\n",
      "3131\n"
     ]
    }
   ],
   "source": [
    "print(len(array_CV_and_Jobs_train))\n",
    "print(len(array_unicode_CV_transf))\n",
    "print(len(array_unicode_Jobs_transf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dental-effectiveness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_Name</th>\n",
       "      <th>clean_Content_CV</th>\n",
       "      <th>tokens_Content_CV</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>clean_Job_Description</th>\n",
       "      <th>tokens_Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DataScientist_Karla_Lewis</td>\n",
       "      <td>data_scientist brooklyn ny data_scientist grub...</td>\n",
       "      <td>[data_scientist, brooklyn, ny, data_scientist,...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>master_degree stem field including_limited com...</td>\n",
       "      <td>[master_degree, stem, field, including_limited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DataScientist_Karla_Lewis</td>\n",
       "      <td>data_scientist brooklyn ny data_scientist grub...</td>\n",
       "      <td>[data_scientist, brooklyn, ny, data_scientist,...</td>\n",
       "      <td>Security Specialist</td>\n",
       "      <td>fast_paced environment combine technical secur...</td>\n",
       "      <td>[fast_paced, environment, combine, technical, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Candidate_Name  \\\n",
       "0  DataScientist_Karla_Lewis   \n",
       "6  DataScientist_Karla_Lewis   \n",
       "\n",
       "                                    clean_Content_CV  \\\n",
       "0  data_scientist brooklyn ny data_scientist grub...   \n",
       "6  data_scientist brooklyn ny data_scientist grub...   \n",
       "\n",
       "                                   tokens_Content_CV            Job_Title  \\\n",
       "0  [data_scientist, brooklyn, ny, data_scientist,...       Data Scientist   \n",
       "6  [data_scientist, brooklyn, ny, data_scientist,...  Security Specialist   \n",
       "\n",
       "                               clean_Job_Description  \\\n",
       "0  master_degree stem field including_limited com...   \n",
       "6  fast_paced environment combine technical secur...   \n",
       "\n",
       "                              tokens_Job_Description  \n",
       "0  [master_degree, stem, field, including_limited...  \n",
       "6  [fast_paced, environment, combine, technical, ...  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Karla_Lewis para Data Scientist y Security Specialist: \n",
    "    #filas 0 y 6 de df_compar_cand_jobs.\n",
    "    #valor con índice 0 de array_unicode_CV_transf\n",
    "    #valores con índices 0 y 6 de array_unicode_Jobs_transf\n",
    "\n",
    "new_df_compar_cand_jobs=df_compar_cand_jobs.iloc[[0,6]]\n",
    "new_df_compar_cand_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "mexican-organ",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_scientist brooklyn ny data_scientist grubhub current new_york ny implemented various time_series forecasting technique predict surge customer order lower average customer wait time minute github deployed recommendation engine production conditionally recommend menu item based past order history increase average order size designed model portland pilot program increase incentive driver peak ordering hour resulting_increase driver availability peak ordering time statistic rutgers university data_scientist new brunswick_nj adobe new_york ny worked product marketing team identify customer interaction free trial maximize likelihood conversion resulting conversion_rate increase built customer attrition random forest model improved monthly python numpy_panda scikit_learn retention basis point customer likely attrit kera flask servicing relevant product feature sql mysql postgres worked_closely product team build production git recommendation engine python improved average length user resulted incremental annual_revenue time_series forecasting productionizing model recommendation engine data analyst customer segmentation adobe new_york ny aws worked product manager perform cohort analysis identiﬁed opportunity reduce pricing segment user boost yearly revenue built operational reporting tableau ﬁnd area improvement contractor resulting annual incremental revenue implemented long_term pricing experiment improved customer lifetime value']\n",
      "############\n",
      "['master_degree stem field including_limited computer_science statistic_applied mathematics operation research engineering economics social science physic chemistry providing advanced analytics within business setting data science implementation within business setting working raw missing data understanding programming fundamental understanding statistic excellent_communication preferred data_visualization tool time series working knowledge relational_database standard sql_query method proficiency_least one general programming_language python java c_c working big_data within hadoop environment working r_sa statistical package advanced statistical econometric data_mining tool method linear model linear_regression generalized linear_regression logistic_regression nonlinear modeling_technique knowledge advanced nonlinear technique including smoothing ensemble_method leverage knowledge programming mathematics computer_science transform_way company business translate business opportunity data driven machine_learning big_data modeling solution revamp process produce data product develop collect data ad_hoc statistical_analysis identify method allow continuous automated statistical testing enhance predictability deployed model maintain relationship business area client business_intelligence technology team aligned meet team objective project manager develop detailed project timeline summarize present conclusion solution communicate complex analysis clearly audience']\n",
      "############\n",
      "['fast_paced environment combine technical security operation talent consulting deliver high value client security solution lead enterprise system focused network application penetration test engagement wide_variety client including federal_government commercial client across_multiple market sector working team seasoned security testing professional enhance_existing service offering security testing capability conduct hand technical testing beyond automated tool validation including full exploitation leveraging access within multiple environment window nix conduct scenario based security testing red teaming identify_gap detection_response capability client network develop comprehensive accurate report presentation technical executive_audience communicate finding strategy effectively client stakeholder including technical staff executive leadership legal_counsel apply security testing penetration_testing technique mindset wide_range project act_primary interface lead small penetration test team ranging additional tester manage delivery staff assignment needed become_part team security enthusiast perform cutting_edge research promote environment innovation knowledge sharing position open remote delivery location include district_columbia using administering troubleshooting least_two major flavor linux_ubuntu preferred window environment active_directory concept scripting editing existing code programming using one_following perl_python ruby_bash c_c c java security assessment tool including nessus accunetix metasploit burp_suite pro cobalt strike covenant knowledge application database web server design implementation knowledge network vulnerability_assessment web application security testing network penetration_testing red teaming security operation hunt knowledge open security testing standard project including owasp att ck convey result clearly formal technical report ba_b degree working commercial consulting service environment using administering troubleshooting ubuntu preferred assembly including reverse_engineering phishing social engineering tactic hardware reverse_engineering using jtag uart physical security assessment including use proxmark similar proximity card spoofing copying device display passion_enthusiasm security technology b_degree computer engineering c related_field preferred obtain offensive_security oswp oscp osce oswe sans certification gawn gpen gxpn within one hire']\n"
     ]
    }
   ],
   "source": [
    "array_1_candidate = [array_unicode_CV_transf[0]]\n",
    "print(array_1_candidate)\n",
    "print('############')\n",
    "\n",
    "#Separo en dos las posiciones porque luego en tfidf_vectorizer.transform tienen que tener el mismo tamaño:\n",
    "array_data_scientist = [array_unicode_Jobs_transf[0]]\n",
    "array_security = [array_unicode_Jobs_transf[6]]\n",
    "\n",
    "print(array_data_scientist)\n",
    "print('############')\n",
    "print(array_security)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "greater-rhythm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_scientist brooklyn ny data_scientist grubhub current new_york ny implemented various time_series forecasting technique predict surge customer order lower average customer wait time minute github deployed recommendation engine production conditionally recommend menu item based past order history increase average order size designed model portland pilot program increase incentive driver peak ordering hour resulting_increase driver availability peak ordering time statistic rutgers university data_scientist new brunswick_nj adobe new_york ny worked product marketing team identify customer interaction free trial maximize likelihood conversion resulting conversion_rate increase built customer attrition random forest model improved monthly python numpy_panda scikit_learn retention basis point customer likely attrit kera flask servicing relevant product feature sql mysql postgres worked_closely product team build production git recommendation engine python improved average length user resulted incremental annual_revenue time_series forecasting productionizing model recommendation engine data analyst customer segmentation adobe new_york ny aws worked product manager perform cohort analysis identiﬁed opportunity reduce pricing segment user boost yearly revenue built operational reporting tableau ﬁnd area improvement contractor resulting annual incremental revenue implemented long_term pricing experiment improved customer lifetime value',\n",
       " 'master_degree stem field including_limited computer_science statistic_applied mathematics operation research engineering economics social science physic chemistry providing advanced analytics within business setting data science implementation within business setting working raw missing data understanding programming fundamental understanding statistic excellent_communication preferred data_visualization tool time series working knowledge relational_database standard sql_query method proficiency_least one general programming_language python java c_c working big_data within hadoop environment working r_sa statistical package advanced statistical econometric data_mining tool method linear model linear_regression generalized linear_regression logistic_regression nonlinear modeling_technique knowledge advanced nonlinear technique including smoothing ensemble_method leverage knowledge programming mathematics computer_science transform_way company business translate business opportunity data driven machine_learning big_data modeling solution revamp process produce data product develop collect data ad_hoc statistical_analysis identify method allow continuous automated statistical testing enhance predictability deployed model maintain relationship business area client business_intelligence technology team aligned meet team objective project manager develop detailed project timeline summarize present conclusion solution communicate complex analysis clearly audience',\n",
       " 'fast_paced environment combine technical security operation talent consulting deliver high value client security solution lead enterprise system focused network application penetration test engagement wide_variety client including federal_government commercial client across_multiple market sector working team seasoned security testing professional enhance_existing service offering security testing capability conduct hand technical testing beyond automated tool validation including full exploitation leveraging access within multiple environment window nix conduct scenario based security testing red teaming identify_gap detection_response capability client network develop comprehensive accurate report presentation technical executive_audience communicate finding strategy effectively client stakeholder including technical staff executive leadership legal_counsel apply security testing penetration_testing technique mindset wide_range project act_primary interface lead small penetration test team ranging additional tester manage delivery staff assignment needed become_part team security enthusiast perform cutting_edge research promote environment innovation knowledge sharing position open remote delivery location include district_columbia using administering troubleshooting least_two major flavor linux_ubuntu preferred window environment active_directory concept scripting editing existing code programming using one_following perl_python ruby_bash c_c c java security assessment tool including nessus accunetix metasploit burp_suite pro cobalt strike covenant knowledge application database web server design implementation knowledge network vulnerability_assessment web application security testing network penetration_testing red teaming security operation hunt knowledge open security testing standard project including owasp att ck convey result clearly formal technical report ba_b degree working commercial consulting service environment using administering troubleshooting ubuntu preferred assembly including reverse_engineering phishing social engineering tactic hardware reverse_engineering using jtag uart physical security assessment including use proxmark similar proximity card spoofing copying device display passion_enthusiasm security technology b_degree computer engineering c related_field preferred obtain offensive_security oswp oscp osce oswe sans certification gawn gpen gxpn within one hire']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_array = array_1_candidate + array_data_scientist + array_security\n",
    "joined_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-blood",
   "metadata": {},
   "source": [
    "### Entrenamos TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "separate-insulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration train tf-idf: 0:00:00.005066\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos una instancia de tf-idf Vectorizer:\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "start_time_fit = datetime.now()\n",
    "\n",
    "# Generamos los vectores tf-idf para el Corpus:\n",
    "tfidf_vectorizer.fit(joined_array)                        #Entrenamos con los 3 textos en lugar de los 21217 textos  de array_CV_and_Jobs_train.\n",
    "\n",
    "end_time_fit = datetime.now()\n",
    "print('Duration train tf-idf: {}'.format(end_time_fit - start_time_fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "distributed-alloy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_scientist': 84,\n",
       " 'brooklyn': 40,\n",
       " 'ny': 219,\n",
       " 'grubhub': 140,\n",
       " 'current': 79,\n",
       " 'new_york': 215,\n",
       " 'implemented': 154,\n",
       " 'various': 368,\n",
       " 'time_series': 352,\n",
       " 'forecasting': 128,\n",
       " 'technique': 346,\n",
       " 'predict': 253,\n",
       " 'surge': 338,\n",
       " 'customer': 80,\n",
       " 'order': 230,\n",
       " 'lower': 187,\n",
       " 'average': 30,\n",
       " 'wait': 370,\n",
       " 'time': 351,\n",
       " 'minute': 203,\n",
       " 'github': 138,\n",
       " 'deployed': 90,\n",
       " 'recommendation': 281,\n",
       " 'engine': 106,\n",
       " 'production': 263,\n",
       " 'conditionally': 69,\n",
       " 'recommend': 280,\n",
       " 'menu': 199,\n",
       " 'item': 166,\n",
       " 'based': 34,\n",
       " 'past': 239,\n",
       " 'history': 147,\n",
       " 'increase': 161,\n",
       " 'size': 319,\n",
       " 'designed': 92,\n",
       " 'model': 205,\n",
       " 'portland': 250,\n",
       " 'pilot': 248,\n",
       " 'program': 267,\n",
       " 'incentive': 157,\n",
       " 'driver': 100,\n",
       " 'peak': 240,\n",
       " 'ordering': 231,\n",
       " 'hour': 148,\n",
       " 'resulting_increase': 295,\n",
       " 'availability': 29,\n",
       " 'statistic': 330,\n",
       " 'rutgers': 301,\n",
       " 'university': 362,\n",
       " 'new': 214,\n",
       " 'brunswick_nj': 41,\n",
       " 'adobe': 9,\n",
       " 'worked': 376,\n",
       " 'product': 262,\n",
       " 'marketing': 194,\n",
       " 'team': 343,\n",
       " 'identify': 150,\n",
       " 'interaction': 164,\n",
       " 'free': 131,\n",
       " 'trial': 357,\n",
       " 'maximize': 197,\n",
       " 'likelihood': 179,\n",
       " 'conversion': 74,\n",
       " 'resulting': 294,\n",
       " 'conversion_rate': 75,\n",
       " 'built': 43,\n",
       " 'attrition': 26,\n",
       " 'random': 277,\n",
       " 'forest': 129,\n",
       " 'improved': 155,\n",
       " 'monthly': 208,\n",
       " 'python': 275,\n",
       " 'numpy_panda': 218,\n",
       " 'scikit_learn': 305,\n",
       " 'retention': 296,\n",
       " 'basis': 35,\n",
       " 'point': 249,\n",
       " 'likely': 180,\n",
       " 'attrit': 25,\n",
       " 'kera': 169,\n",
       " 'flask': 125,\n",
       " 'servicing': 315,\n",
       " 'relevant': 287,\n",
       " 'feature': 121,\n",
       " 'sql': 325,\n",
       " 'mysql': 210,\n",
       " 'postgres': 252,\n",
       " 'worked_closely': 377,\n",
       " 'build': 42,\n",
       " 'git': 137,\n",
       " 'length': 175,\n",
       " 'user': 364,\n",
       " 'resulted': 293,\n",
       " 'incremental': 162,\n",
       " 'annual_revenue': 17,\n",
       " 'productionizing': 264,\n",
       " 'data': 82,\n",
       " 'analyst': 14,\n",
       " 'segmentation': 311,\n",
       " 'aws': 31,\n",
       " 'manager': 192,\n",
       " 'perform': 243,\n",
       " 'cohort': 57,\n",
       " 'analysis': 13,\n",
       " 'identiﬁed': 152,\n",
       " 'opportunity': 229,\n",
       " 'reduce': 283,\n",
       " 'pricing': 258,\n",
       " 'segment': 310,\n",
       " 'boost': 39,\n",
       " 'yearly': 379,\n",
       " 'revenue': 298,\n",
       " 'operational': 228,\n",
       " 'reporting': 290,\n",
       " 'tableau': 340,\n",
       " 'ﬁnd': 380,\n",
       " 'area': 20,\n",
       " 'improvement': 156,\n",
       " 'contractor': 73,\n",
       " 'annual': 16,\n",
       " 'long_term': 186,\n",
       " 'experiment': 118,\n",
       " 'lifetime': 178,\n",
       " 'value': 367,\n",
       " 'master_degree': 195,\n",
       " 'stem': 334,\n",
       " 'field': 123,\n",
       " 'including_limited': 160,\n",
       " 'computer_science': 66,\n",
       " 'statistic_applied': 331,\n",
       " 'mathematics': 196,\n",
       " 'operation': 227,\n",
       " 'research': 291,\n",
       " 'engineering': 107,\n",
       " 'economics': 102,\n",
       " 'social': 322,\n",
       " 'science': 304,\n",
       " 'physic': 246,\n",
       " 'chemistry': 51,\n",
       " 'providing': 272,\n",
       " 'advanced': 10,\n",
       " 'analytics': 15,\n",
       " 'within': 375,\n",
       " 'business': 45,\n",
       " 'setting': 316,\n",
       " 'implementation': 153,\n",
       " 'working': 378,\n",
       " 'raw': 279,\n",
       " 'missing': 204,\n",
       " 'understanding': 361,\n",
       " 'programming': 268,\n",
       " 'fundamental': 133,\n",
       " 'excellent_communication': 114,\n",
       " 'preferred': 255,\n",
       " 'data_visualization': 85,\n",
       " 'tool': 354,\n",
       " 'series': 312,\n",
       " 'knowledge': 170,\n",
       " 'relational_database': 285,\n",
       " 'standard': 329,\n",
       " 'sql_query': 326,\n",
       " 'method': 201,\n",
       " 'proficiency_least': 266,\n",
       " 'one': 224,\n",
       " 'general': 135,\n",
       " 'programming_language': 269,\n",
       " 'java': 167,\n",
       " 'c_c': 47,\n",
       " 'big_data': 38,\n",
       " 'hadoop': 142,\n",
       " 'environment': 113,\n",
       " 'r_sa': 276,\n",
       " 'statistical': 332,\n",
       " 'package': 237,\n",
       " 'econometric': 101,\n",
       " 'data_mining': 83,\n",
       " 'linear': 181,\n",
       " 'linear_regression': 182,\n",
       " 'generalized': 136,\n",
       " 'logistic_regression': 185,\n",
       " 'nonlinear': 217,\n",
       " 'modeling_technique': 207,\n",
       " 'including': 159,\n",
       " 'smoothing': 321,\n",
       " 'ensemble_method': 110,\n",
       " 'leverage': 176,\n",
       " 'transform_way': 355,\n",
       " 'company': 62,\n",
       " 'translate': 356,\n",
       " 'driven': 99,\n",
       " 'machine_learning': 188,\n",
       " 'modeling': 206,\n",
       " 'solution': 323,\n",
       " 'revamp': 297,\n",
       " 'process': 260,\n",
       " 'produce': 261,\n",
       " 'develop': 95,\n",
       " 'collect': 58,\n",
       " 'ad_hoc': 6,\n",
       " 'statistical_analysis': 333,\n",
       " 'allow': 12,\n",
       " 'continuous': 72,\n",
       " 'automated': 28,\n",
       " 'testing': 350,\n",
       " 'enhance': 108,\n",
       " 'predictability': 254,\n",
       " 'maintain': 189,\n",
       " 'relationship': 286,\n",
       " 'client': 54,\n",
       " 'business_intelligence': 46,\n",
       " 'technology': 347,\n",
       " 'aligned': 11,\n",
       " 'meet': 198,\n",
       " 'objective': 220,\n",
       " 'project': 270,\n",
       " 'detailed': 93,\n",
       " 'timeline': 353,\n",
       " 'summarize': 337,\n",
       " 'present': 256,\n",
       " 'conclusion': 68,\n",
       " 'communicate': 61,\n",
       " 'complex': 63,\n",
       " 'clearly': 53,\n",
       " 'audience': 27,\n",
       " 'fast_paced': 120,\n",
       " 'combine': 59,\n",
       " 'technical': 345,\n",
       " 'security': 309,\n",
       " 'talent': 342,\n",
       " 'consulting': 71,\n",
       " 'deliver': 88,\n",
       " 'high': 145,\n",
       " 'lead': 171,\n",
       " 'enterprise': 111,\n",
       " 'system': 339,\n",
       " 'focused': 127,\n",
       " 'network': 213,\n",
       " 'application': 18,\n",
       " 'penetration': 241,\n",
       " 'test': 348,\n",
       " 'engagement': 105,\n",
       " 'wide_variety': 373,\n",
       " 'federal_government': 122,\n",
       " 'commercial': 60,\n",
       " 'across_multiple': 3,\n",
       " 'market': 193,\n",
       " 'sector': 308,\n",
       " 'seasoned': 307,\n",
       " 'professional': 265,\n",
       " 'enhance_existing': 109,\n",
       " 'service': 314,\n",
       " 'offering': 223,\n",
       " 'capability': 48,\n",
       " 'conduct': 70,\n",
       " 'hand': 143,\n",
       " 'beyond': 37,\n",
       " 'validation': 366,\n",
       " 'full': 132,\n",
       " 'exploitation': 119,\n",
       " 'leveraging': 177,\n",
       " 'access': 0,\n",
       " 'multiple': 209,\n",
       " 'window': 374,\n",
       " 'nix': 216,\n",
       " 'scenario': 303,\n",
       " 'red': 282,\n",
       " 'teaming': 344,\n",
       " 'identify_gap': 151,\n",
       " 'detection_response': 94,\n",
       " 'comprehensive': 64,\n",
       " 'accurate': 2,\n",
       " 'report': 289,\n",
       " 'presentation': 257,\n",
       " 'executive_audience': 116,\n",
       " 'finding': 124,\n",
       " 'strategy': 335,\n",
       " 'effectively': 104,\n",
       " 'stakeholder': 328,\n",
       " 'staff': 327,\n",
       " 'executive': 115,\n",
       " 'leadership': 172,\n",
       " 'legal_counsel': 174,\n",
       " 'apply': 19,\n",
       " 'penetration_testing': 242,\n",
       " 'mindset': 202,\n",
       " 'wide_range': 372,\n",
       " 'act_primary': 4,\n",
       " 'interface': 165,\n",
       " 'small': 320,\n",
       " 'ranging': 278,\n",
       " 'additional': 7,\n",
       " 'tester': 349,\n",
       " 'manage': 191,\n",
       " 'delivery': 89,\n",
       " 'assignment': 23,\n",
       " 'needed': 211,\n",
       " 'become_part': 36,\n",
       " 'enthusiast': 112,\n",
       " 'cutting_edge': 81,\n",
       " 'promote': 271,\n",
       " 'innovation': 163,\n",
       " 'sharing': 317,\n",
       " 'position': 251,\n",
       " 'open': 226,\n",
       " 'remote': 288,\n",
       " 'location': 184,\n",
       " 'include': 158,\n",
       " 'district_columbia': 98,\n",
       " 'using': 365,\n",
       " 'administering': 8,\n",
       " 'troubleshooting': 358,\n",
       " 'least_two': 173,\n",
       " 'major': 190,\n",
       " 'flavor': 126,\n",
       " 'linux_ubuntu': 183,\n",
       " 'active_directory': 5,\n",
       " 'concept': 67,\n",
       " 'scripting': 306,\n",
       " 'editing': 103,\n",
       " 'existing': 117,\n",
       " 'code': 56,\n",
       " 'one_following': 225,\n",
       " 'perl_python': 244,\n",
       " 'ruby_bash': 300,\n",
       " 'assessment': 22,\n",
       " 'nessus': 212,\n",
       " 'accunetix': 1,\n",
       " 'metasploit': 200,\n",
       " 'burp_suite': 44,\n",
       " 'pro': 259,\n",
       " 'cobalt': 55,\n",
       " 'strike': 336,\n",
       " 'covenant': 78,\n",
       " 'database': 86,\n",
       " 'web': 371,\n",
       " 'server': 313,\n",
       " 'design': 91,\n",
       " 'vulnerability_assessment': 369,\n",
       " 'hunt': 149,\n",
       " 'owasp': 236,\n",
       " 'att': 24,\n",
       " 'ck': 52,\n",
       " 'convey': 76,\n",
       " 'result': 292,\n",
       " 'formal': 130,\n",
       " 'ba_b': 33,\n",
       " 'degree': 87,\n",
       " 'ubuntu': 360,\n",
       " 'assembly': 21,\n",
       " 'reverse_engineering': 299,\n",
       " 'phishing': 245,\n",
       " 'tactic': 341,\n",
       " 'hardware': 144,\n",
       " 'jtag': 168,\n",
       " 'uart': 359,\n",
       " 'physical': 247,\n",
       " 'use': 363,\n",
       " 'proxmark': 274,\n",
       " 'similar': 318,\n",
       " 'proximity': 273,\n",
       " 'card': 49,\n",
       " 'spoofing': 324,\n",
       " 'copying': 77,\n",
       " 'device': 96,\n",
       " 'display': 97,\n",
       " 'passion_enthusiasm': 238,\n",
       " 'b_degree': 32,\n",
       " 'computer': 65,\n",
       " 'related_field': 284,\n",
       " 'obtain': 221,\n",
       " 'offensive_security': 222,\n",
       " 'oswp': 235,\n",
       " 'oscp': 233,\n",
       " 'osce': 232,\n",
       " 'oswe': 234,\n",
       " 'sans': 302,\n",
       " 'certification': 50,\n",
       " 'gawn': 134,\n",
       " 'gpen': 139,\n",
       " 'gxpn': 141,\n",
       " 'hire': 146}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(len(feature_names))\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "unlikely-surfing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documento/Texto</th>\n",
       "      <th>access</th>\n",
       "      <th>accunetix</th>\n",
       "      <th>accurate</th>\n",
       "      <th>across_multiple</th>\n",
       "      <th>act_primary</th>\n",
       "      <th>active_directory</th>\n",
       "      <th>ad_hoc</th>\n",
       "      <th>additional</th>\n",
       "      <th>administering</th>\n",
       "      <th>...</th>\n",
       "      <th>web</th>\n",
       "      <th>wide_range</th>\n",
       "      <th>wide_variety</th>\n",
       "      <th>window</th>\n",
       "      <th>within</th>\n",
       "      <th>worked</th>\n",
       "      <th>worked_closely</th>\n",
       "      <th>working</th>\n",
       "      <th>yearly</th>\n",
       "      <th>ﬁnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karla Lewis CV</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114296</td>\n",
       "      <td>0.057148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057148</td>\n",
       "      <td>0.057148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist Job</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Security Specialist Job</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.083242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083242</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.083242</td>\n",
       "      <td>0.063308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Documento/Texto    access  accunetix  accurate  across_multiple  \\\n",
       "0           Karla Lewis CV  0.000000   0.000000  0.000000         0.000000   \n",
       "1       Data Scientist Job  0.000000   0.000000  0.000000         0.000000   \n",
       "2  Security Specialist Job  0.041621   0.041621  0.041621         0.041621   \n",
       "\n",
       "   act_primary  active_directory    ad_hoc  additional  administering  ...  \\\n",
       "0     0.000000          0.000000  0.000000    0.000000       0.000000  ...   \n",
       "1     0.000000          0.000000  0.068689    0.000000       0.000000  ...   \n",
       "2     0.041621          0.041621  0.000000    0.041621       0.083242  ...   \n",
       "\n",
       "        web  wide_range  wide_variety    window    within    worked  \\\n",
       "0  0.000000    0.000000      0.000000  0.000000  0.000000  0.114296   \n",
       "1  0.000000    0.000000      0.000000  0.000000  0.156719  0.000000   \n",
       "2  0.083242    0.041621      0.041621  0.083242  0.063308  0.000000   \n",
       "\n",
       "   worked_closely   working    yearly       ﬁnd  \n",
       "0        0.057148  0.000000  0.057148  0.057148  \n",
       "1        0.000000  0.208959  0.000000  0.000000  \n",
       "2        0.000000  0.063308  0.000000  0.000000  \n",
       "\n",
       "[3 rows x 382 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Si quiero obtener la matriz tengo que aplicar fit_transform (no fit y transform por separado)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(joined_array) \n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "#Insertamos en la primer fila los Documentos / Textos correspondientes.\n",
    "df.reset_index()\n",
    "primer_columna = ['Karla Lewis CV', 'Data Scientist Job', 'Security Specialist Job']\n",
    "df.insert(0,'Documento/Texto', primer_columna)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "concrete-concentration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documento/Texto</th>\n",
       "      <th>data_scientist</th>\n",
       "      <th>python</th>\n",
       "      <th>oswp</th>\n",
       "      <th>web</th>\n",
       "      <th>business</th>\n",
       "      <th>programming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Karla Lewis CV</td>\n",
       "      <td>0.171444</td>\n",
       "      <td>0.086925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist Job</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343445</td>\n",
       "      <td>0.104479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Security Specialist Job</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.083242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Documento/Texto  data_scientist    python      oswp       web  \\\n",
       "0           Karla Lewis CV        0.171444  0.086925  0.000000  0.000000   \n",
       "1       Data Scientist Job        0.000000  0.052240  0.000000  0.000000   \n",
       "2  Security Specialist Job        0.000000  0.000000  0.041621  0.083242   \n",
       "\n",
       "   business  programming  \n",
       "0  0.000000     0.000000  \n",
       "1  0.343445     0.104479  \n",
       "2  0.000000     0.031654  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Documento/Texto', 'data_scientist','python','oswp', 'web', 'business', 'programming']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-senegal",
   "metadata": {},
   "source": [
    "### Aplicación de Cosine Sim por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dirty-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06587885]\n",
      "[0.00993522]\n"
     ]
    }
   ],
   "source": [
    "A_karla = tfidf_vectorizer.transform(array_1_candidate)\n",
    "B_data_scientist = tfidf_vectorizer.transform(array_data_scientist) \n",
    "C_security = tfidf_vectorizer.transform(array_security) \n",
    "\n",
    "#A con B:\n",
    "cosine_karla_data_scientist = 1 - pcd(A_karla, B_data_scientist)   #array de tamaño n_samples\n",
    "print(cosine_karla_data_scientist)\n",
    "\n",
    "#A con C:\n",
    "cosine_karla_security = 1 - pcd(A_karla, C_security)   #array de tamaño n_samples\n",
    "print(cosine_karla_security)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-arabic",
   "metadata": {},
   "source": [
    "Obtuvimos lo esperado: un alto valor para DataScientist y un bajo valor en el puesto de Security Specialist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "handy-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-158-794c5e6bdcfa>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_compar_cand_jobs['tfidf_cosine'] = [0.06,0.01]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_Name</th>\n",
       "      <th>clean_Content_CV</th>\n",
       "      <th>tokens_Content_CV</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>clean_Job_Description</th>\n",
       "      <th>tokens_Job_Description</th>\n",
       "      <th>tfidf_cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DataScientist_Karla_Lewis</td>\n",
       "      <td>data_scientist brooklyn ny data_scientist grub...</td>\n",
       "      <td>[data_scientist, brooklyn, ny, data_scientist,...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>master_degree stem field including_limited com...</td>\n",
       "      <td>[master_degree, stem, field, including_limited...</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DataScientist_Karla_Lewis</td>\n",
       "      <td>data_scientist brooklyn ny data_scientist grub...</td>\n",
       "      <td>[data_scientist, brooklyn, ny, data_scientist,...</td>\n",
       "      <td>Security Specialist</td>\n",
       "      <td>fast_paced environment combine technical secur...</td>\n",
       "      <td>[fast_paced, environment, combine, technical, ...</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Candidate_Name  \\\n",
       "0  DataScientist_Karla_Lewis   \n",
       "6  DataScientist_Karla_Lewis   \n",
       "\n",
       "                                    clean_Content_CV  \\\n",
       "0  data_scientist brooklyn ny data_scientist grub...   \n",
       "6  data_scientist brooklyn ny data_scientist grub...   \n",
       "\n",
       "                                   tokens_Content_CV            Job_Title  \\\n",
       "0  [data_scientist, brooklyn, ny, data_scientist,...       Data Scientist   \n",
       "6  [data_scientist, brooklyn, ny, data_scientist,...  Security Specialist   \n",
       "\n",
       "                               clean_Job_Description  \\\n",
       "0  master_degree stem field including_limited com...   \n",
       "6  fast_paced environment combine technical secur...   \n",
       "\n",
       "                              tokens_Job_Description  tfidf_cosine  \n",
       "0  [master_degree, stem, field, including_limited...          0.06  \n",
       "6  [fast_paced, environment, combine, technical, ...          0.01  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Añadimos a new_df_compar_cand_jobs los respectivos valores (redondeando previamente):\n",
    "new_df_compar_cand_jobs['tfidf_cosine'] = [0.06,0.01]\n",
    "new_df_compar_cand_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-understanding",
   "metadata": {},
   "source": [
    "### Si se agregaría uno nuevo con descripción similar a security...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "supported-miracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04948456]\n",
      "[0.46922668]\n"
     ]
    }
   ],
   "source": [
    "nuevo = ['security operation talent consulting deliver high value client security solution']\n",
    "\n",
    "D_NUEVO = tfidf_vectorizer.transform(nuevo)\n",
    "B_data_scientist = tfidf_vectorizer.transform(array_data_scientist) \n",
    "C_security = tfidf_vectorizer.transform(array_security) \n",
    "\n",
    "#D con B:\n",
    "cosine_nuevo_con_data_scientist = 1 - pcd(D_NUEVO, B_data_scientist)   #array de tamaño n_samples\n",
    "print(cosine_nuevo_con_data_scientist)\n",
    "\n",
    "#D con C:\n",
    "cosine_nuevo_con_security = 1 - pcd(D_NUEVO, C_security)   #array de tamaño n_samples\n",
    "print(cosine_nuevo_con_security)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-pantyhose",
   "metadata": {},
   "source": [
    "Obtuvimos lo esperado: un bajo valor para DataScientist y un alto valor en el puesto de Security Specialist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-settlement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
