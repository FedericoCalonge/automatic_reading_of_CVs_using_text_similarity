{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta serie de Jupyter Notebooks aplicaremos las técnicas de **WMD** y **Cosine Similarity** para medir distancias y obtener similitudes entre textos.  \n",
    "\n",
    "Previamente a aplicar **Cosine Similarity** usaremos **TF-IDF** (entrenado con nuestro propio Vocabulario) para obtener los vectores de los textos/documentos a comparar basándose en:\n",
    "* **TF - Term Frequency**: usado para encontrar cuántas veces aparece el término en el documento; y en\n",
    "* **IDF - inverse document frequency**: nos da la medida de cuántas veces aparece ese término en toda la colección.\n",
    "\n",
    "Además, previamente a aplicar **WMD** necesitaremos obtener nuestros **Word Embeddings** mediante **Word2vec** (entrenados con nuestro propio Vocabulario) para obtener los vectores de los textos/documentos a comparar, y así poder calcular la métrica de distancia WMD.\n",
    "\n",
    "Para obtener dichas similitudes entre textos, las comparaciones que realizaremos serán entre el **contenido de CVs de distintos Candidatos** y las **Descripciones de los Puestos de IT publicados por distintas empresas**. \n",
    "\n",
    "Los datasets utilizados y la implementación realizada del programa es para Curriculums y Descripciones de Puestos en idioma **Inglés**.\n",
    "\n",
    "<u> Luego de aplicar preprocesamiento y limpieza de datos nos quedarán los siguientes datasets: </u>\n",
    "* **624** CVs de Candidatos (en formato pdf y csv). \n",
    "* **20593** Descripciones de Puestos de IT (en formato csv). \n",
    "\n",
    "**Nota**: el total de CVs de Candidatos (**624**) y Descripciones de Puestos de IT (**20593**) serán utilizados para el entrenamiento y obtención de vectores mediante TF-IDF (para el futuro cálculo de Cosine Similarity) y para el entrenamiento de Word2Vec y obtención de los Word Embeddings (para el futuro cálculo de WMD). \n",
    "\n",
    "Por otro lado, para calcular Cosine Similarity y WMD, para utilizarlos en K-means y para entrenar a nuestro algoritmo KNN, utilizaremos únicamente **una porción de nuestros datasets**:\n",
    "\n",
    "* 1-Para el cálculo de Cosine Similarity y WMD:\n",
    "    * **301** CVs de Candidatos.\n",
    "    * **201** Descripciones de Puestos de IT.\n",
    "    * Nota: No obstante, al realizar los cálculos de distancias compararemos cada CV con cada Job Description, obteniendo un dataframe total de **3131** filas con sus respectivo valores de WMD y Cosine Sim.   \n",
    "    \n",
    "    \n",
    "* 2-Para el uso de K-means y entrenamiento con KNN (eliminamos un CV y una Descripción de Puesto IT que los utilizamos en '3-'):\n",
    "    * **300** CVs de Candidatos.\n",
    "    * **200** Descripciones de Puestos de IT.\n",
    "    * Nota: como se comentó previamente, nos quedarán **3000** filas / puntos para usar en K-means y entrenar KNN; llegando a representar estos 3000 puntos en un plano de 2 dimensiones.  \n",
    "    \n",
    "    \n",
    "* 3-Para la clasificación de nuevas muestras prediciendo con KNN:\n",
    "    * **1** CV de Candidatos.\n",
    "    * **1** Descripción de Puesto de IT.\n",
    "    * Nota: como se comentó previamente, nos quedarán **131** filas para clasificar.  \n",
    "    \n",
    "\n",
    "**¿Por qué utilizamos solo una porción de nuestros datasets?:** Esto es debido a los drawbacks de WMD y KNN.\n",
    "   * **WMD:** posee una alta complejidad en el cálculo de la distancia, teniendo un tiempo de ejecución muy elevado. Como ejemplo, al correrlo localmente, el cálculo de **WMD** para **3131 filas** tardó **7 horas**; frente a los **3 segundos** que tardó el cálculo de **Cosine Similarity** para la **misma cantidad de filas**.\n",
    "   * **KNN:** KNN es una gran opción para datasets pequeños con pocas variables de entrada; pero tiene problemas cuando la cantidad de entradas es muy grande. Cada variable de entrada puede considerarse una dimensión de un espacio de entrada p-dimensional. En grandes dimensiones, los puntos que pueden ser similares pueden tener distancias muy grandes. Además, cada vez que se va a hacer una predicción con KNN, busca al vecino más cercano en el conjunto de entrenamiento completo. Por esto, se debe utilizar un dataset pequeño para que el clasificador KNN completa su ejecución rápidamente.\n",
    "   \n",
    "En conclusión, al utilizar solo una porción de nuestros datasets para obtener los distintos cálculos de distancias y entrenar KNN, el cálculo de WMD se podrá realizar en un tiempo finito, y nuestro clasificador KNN funcionará rápida y eficientemente al realizar predicciones.\n",
    "\n",
    "<u> Ubicación Datasets: </u>\n",
    "* CVs: **/Datasets_CVs_And_Job_Descriptions/EN/CVs**.    \n",
    "* Descripciones Puestos: **/Datasets_CVs_And_Job_Descriptions/EN/Job_Descr**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice y Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook \"1-Preprocessing_&_Data_Cleaning\":\n",
    "* 1-Armado de Dataframes y Limpieza de datos.\n",
    "    * 1.1-Importando librerías necesarias.\n",
    "    * 1.2-Funciones necesarias para la Limpieza de datos. ( * )\n",
    "    * 1.3-Armado Dataframe Puestos en base a Datasets. ( ** )  \n",
    "        *1.3.1- (Archivo CSV) '1-10_examples_job_Desc.csv'    \n",
    "        *1.3.2- (Archivo CSV) '2-22000_examples_dice_com-job_us.csv'  \n",
    "    * 1.4-Limpieza Dataframe Puestos. \n",
    "    * 1.5-Armado Dataframe CVs en base a Datasets.  ( ** )   \n",
    "        *1.5.1- (Carpeta con archivos en PDF) '1-10_examples_CVs_PDF'  \n",
    "        *1.5.2- (Carpeta con archivos en PDF) '2-228_examples_CVs_PDF'  \n",
    "        *1.5.3- (Archivo CSV) '3-2484_examples_CVs.csv'  \n",
    "        *1.5.4- (Archivo CSV) '4-962_examples_CVs.csv'  \n",
    "    * 1.6-Limpieza Dataframe CVs.  \n",
    "    \n",
    "* 2-Export de DFs para usarlo en el siguiente Jupyter Notebook.\n",
    "\n",
    "( * ) **El procedimiento para la Limpieza de los CVs y los Puestos será el siguiente:**\n",
    "\n",
    "1. Eliminación de filas repetidas.\n",
    "2. Limpieza inicial:\n",
    "    * 2.1. Codificamos nuestros textos a utf-8.\n",
    "    * 2.2. Convertimos todo a minúscula.\n",
    "    * 2.3. Eliminamos datos no relevantes para nuestros análisis (mails, páginas web y common words).\n",
    "    * 2.4. Eliminamos signos de puntuación y caracteres especiales (incluyendo números).\n",
    "    * 2.5. Eliminamos stop words.\n",
    "3. Aplicamos Tokenización.\n",
    "4. Aplicamos Lematización.\n",
    "5. Obtenemos y usamos bi-gramas.\n",
    "\n",
    "( ** ) **Fuentes de los datasets:** \n",
    "\n",
    "* 1.3.1- (Archivo CSV) '1-10_examples_job_Desc.csv': Recolección propia del sitio Indeed (https://www.indeed.com/q-USA-jobs.html) para puestos de trabajo de IT.  \n",
    "* 1.3.2- (Archivo CSV) '2-22000_examples_dice_com-job_us.csv': CSV obtenido del sitio Kaggle (https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom). El CSV cuenta con descripciones de puestos obtenidos del sitio web de USA de postulación de trabajos del rubro de IT '**Dice.com**'.\n",
    "* 1.4.1- (Carpeta con archivos en PDF) '1-10_examples_CVs_PDF': Recolección propia de distintos sitios web con ejemplos de CVs de Candidatos para distintos Puestos.    \n",
    "* 1.4.2- (Carpeta con archivos en PDF) '2-228_examples_CVs_PDF': Documentos .docx convertidos a .pdf obtenidos del sitio Kaggle (https://www.kaggle.com/palaksood97/resume-dataset). Estos pdfs son Candidatos de la India con experiencia en el rubro de IT.  \n",
    "* 1.4.3- (Archivo CSV) '3-2484_examples_CVs.csv': CSV obtenido del sitio Kagle (https://www.kaggle.com/snehaanbhawal/resume-dataset). Este CSV cuenta con CVs obtenidos del sitio web de postulación de trabajos '**livecareer.com**'.\n",
    "* 1.4.4- (Archivo CSV) '4-962_examples_CVs.csv': CSV obtenido del sitio Kaggle (https://www.kaggle.com/gauravduttakiit/resume-dataset). Este CSV cuenta con CVs repartidos en distintas categorías de IT.\n",
    "\n",
    "**Cantidades finales de los datasets luego de aplicar preprocesamiento y limpieza de datos:**\n",
    "\n",
    "| Dataset                              | Cantidad Inicial | Cantidad Final | Cantidad Total Final Puestos | Cantidad Total Final CVs |\n",
    "|--------------------------------------|------------------|----------------|------------------------|--------------------|\n",
    "| 1-10_examples_job_Desc.csv           | 10               | 10             | -                      | -                  |\n",
    "| 2-22000_examples_dice_com-job_us.csv | 22000            | 20583          | -                      | -                  |\n",
    "| -                                    | -                | -              | 20593                  | -                  |\n",
    "| 1-10_examples_CVs_PDF                | 10               | 10             | -                      | -                  |\n",
    "| 2-228_examples_CVs_PDF               | 228              | 228            | -                      | -                  |\n",
    "| 3-2484_examples_CVs.csv              | 2484             | 289            | -                      | -                  |\n",
    "| 4-962_examples_CVs.csv               | 962              | 97             | -                      | -                  |\n",
    "| -                                    | -                | -              | -                      | 624                |\n",
    "  \n",
    "### Jupyter Notebook \"2-TF_IDF_Cosine_&_Word2vec_WMD\":  \n",
    "* 3-Imports.\n",
    "     * 3.1-Import librerias necesarias.\n",
    "     * 3.2-Import de DFs del Jupyter Notebook anterior.\n",
    "* 4-División DFs para usarlos en los distintos cálculos y entrenamientos.\n",
    "* 5-Realizando Comparaciones y obteniendo Similitudes.\n",
    "     * 5.1- TF-IDF & Cosine Similarity.\n",
    "     * 5.2- Word Embedding (Word2vec) & WMD.\n",
    "* 6-Observación valores de Cosine Similarity y WMD.\n",
    "     * 6.1-Candidate 'Bradly Johnston (ML Engineer)'\n",
    "     * 6.2-Job 'Security Specialist 2'.\n",
    "* 7-Export del DF para usarlo en el siguiente Jupyter Notebook.\n",
    "     \n",
    "### Jupyter Notebook \"3-Kmeans_&_KNN\":  \n",
    "* 8-Imports.\n",
    "     * 8.1-Import librerias necesarias.\n",
    "     * 8.2-Import de DFs del Jupyter Notebook anterior.\n",
    "     \n",
    "* 9-Preparando los datos para K-means y KNN.\n",
    "    * 9.1-Detección de outliers.\n",
    "\n",
    "* 10-K-means.\n",
    "\n",
    "* 11-KNN.\n",
    "    * 11.1-Esquema 1: Train Set, Test Set & Validation Set.\n",
    "    * 11.2-Esquema 2: Cross Validation.\n",
    "\n",
    "* 12-Clasificando nuevas muestras.\n",
    "    * 12.1-Candidato 'SecuritySpecialist_Denis Banik'.\n",
    "    * 12.2-Posición 'HCM Consultant 2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
