<p> Aprendizaje automático </p>
<p> El aprendizaje automático o aprendizaje automatizado o aprendizaje de máquinas (del inglés, machine learning) es el subcampo de las ciencias de la computación y una rama de la inteligencia artificial, cuyo objetivo es desarrollar técnicas que permitan que las computadoras aprendan. Se dice que un agente aprende cuando su desempeño mejora con la experiencia; es decir, cuando la habilidad no estaba presente en su genotipo o rasgos de nacimiento.[1]​ De forma más concreta, los investigadores del aprendizaje de máquinas buscan algoritmos y heurísticas para convertir muestras de datos en programas de computadora, sin tener que escribir los últimos explícitamente. Los modelos o programas resultantes deben ser capaces de generalizar comportamientos e inferencias para un conjunto más amplio (potencialmente infinito) de datos. </p>
<p> En muchas ocasiones el campo de actuación del aprendizaje automático se solapa con el de la estadística inferencial, ya que las dos disciplinas se basan en el análisis de datos. Sin embargo, el aprendizaje automático incorpora las preocupaciones de la complejidad computacional de los problemas[2]​. Muchos problemas son de clase NP-hard, por lo que gran parte de la investigación realizada en aprendizaje automático está enfocada al diseño de soluciones factibles a esos problemas. El aprendizaje automático también está estrechamente relacionado con el reconocimiento de patrones. El aprendizaje automático puede ser visto como un intento de automatizar algunas partes del método científico mediante métodos matemáticos. Por lo tanto es un proceso de inducción del conocimiento. </p>
<p> Algunos sistemas de aprendizaje automático intentan eliminar toda necesidad de intuición o conocimiento experto de los procesos de análisis de datos, mientras otros tratan de establecer un marco de colaboración entre el experto y la computadora. De todas formas, la intuición humana no puede ser reemplazada en su totalidad, ya que el diseñador del sistema ha de especificar la forma de representación de los datos y los métodos de manipulación y caracterización de los mismos. Sin embargo, las computadoras son utilizadas por todo el mundo con fines tecnológicos muy buenos. </p>
<p> El aprendizaje automático tiene como resultado un modelo para resolver una tarea dada. Entre los modelos se distinguen[3]​ </p>
<p> Los modelos geométricos, construidos en el espacio de instancias y que pueden tener una, dos o múltiples dimensiones. Si hay un borde de decisión lineal entre las clases, se dice que los datos son linealmente separables. Un límite de decisión lineal se define como w * x = t, donde w es un vector perpendicular al límite de decisión, x es un punto arbitrario en el límite de decisión y t es el umbral de la decisión. </p>
<p> Los modelos probabilísticos, que intentan determinar la distribución de probabilidades descriptora de la función que enlaza a los valores de las características con valores determinados. Uno de los conceptos claves para desarrollar modelos probabilísticos es la estadística bayesiana. </p>
<p> Los modelos lógicos, que transforman y expresan las probabilidades en reglas organizadas en forma de árboles de decisión. </p>
<p> Los modelos pueden también clasificarse como modelos de agrupamiento y modelos de gradiente. Los primeros tratan de dividir el espacio de instancias en grupos. Los segundos, como su nombre lo indican, representan un gradiente en el que se puede diferenciar entre cada instancia. Clasificadores geométricos como las máquinas de vectores de apoyo son modelos de gradientes. </p>
<p> Los diferentes algoritmos de Aprendizaje Automático se agrupan en una taxonomía en función de la salida de los mismos. Algunos tipos de algoritmos son: </p>
<p> El algoritmo produce una función que establece una correspondencia entre las entradas y las salidas deseadas del sistema. Un ejemplo de este tipo de algoritmo es el problema de clasificación, donde el sistema de aprendizaje trata de etiquetar (clasificar) una serie de vectores utilizando una entre varias categorías (clases). La base de conocimiento del sistema está formada por ejemplos de etiquetados anteriores. Este tipo de aprendizaje puede llegar a ser muy útil en problemas de investigación biológica, biología computacional y bioinformática. </p>
<p> Todo el proceso de modelado se lleva a cabo sobre un conjunto de ejemplos formado tan sólo por entradas al sistema. No se tiene información sobre las categorías de esos ejemplos. Por lo tanto, en este caso, el sistema tiene que ser capaz de reconocer patrones para poder etiquetar las nuevas entradas. </p>
<p> Aprendizaje semisupervisado </p>
<p> Este tipo de algoritmos combinan los dos algoritmos anteriores para poder clasificar de manera adecuada. Se tiene en cuenta los datos marcados y los no marcados. </p>
<p> El algoritmo aprende observando el mundo que le rodea. Su información de entrada es el feedback o retroalimentación que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. </p>
<p> El aprendizaje por refuerzo es el más general entre las tres categorías. En vez de que un instructor indique al agente qué hacer, el agente inteligente debe aprender cómo se comporta el entorno mediante recompensas (refuerzos) o castigos, derivados del éxito o del fracaso respectivamente. El objetivo principal es aprender la función de valor que le ayude al agente inteligente a maximizar la señal de recompensa y así optimizar sus políticas de modo a comprender el comportamiento del entorno y a tomar buenas decisiones para el logro de sus objetivos formales. </p>
<p> Los principales algoritmos de aprendizaje por refuerzo se desarrollan dentro de los métodos de resolución de problemas de decisión finitos de Markov, que incorporan las ecuaciones de Bellman y las funciones de valor. Los tres métodos principales son: la Programación Dinámica, los métodos de Monte Carlo y el aprendizaje de Diferencias Temporales.[4]​ </p>
<p> Entre las implementaciones desarrolladas está AlphaGo, un programa de IA desarrollado por Google DeepMind para jugar el juego de mesa Go. En marzo de 2016 AlphaGo le ganó una partida al jugador profesional Lee Se-Dol que tiene la categoría noveno dan y 18 títulos mundiales. Entre los algoritmos que utiliza se encuentra el árbol de búsqueda Monte Carlo, también utiliza aprendizaje profundo con redes neuronales. Puede ver lo ocurrido en el documental de Netflix “AlphaGo”. </p>
<p> Transducción </p>
<p> Similar al aprendizaje supervisado, pero no construye de forma explícita una función. Trata de predecir las categorías de los futuros ejemplos basándose en los ejemplos de entrada, sus respectivas categorías y los ejemplos nuevos al sistema. </p>
<p> Aprendizaje multi-tarea </p>
<p> Métodos de aprendizaje que usan conocimiento previamente aprendido por el sistema de cara a enfrentarse a problemas parecidos a los ya vistos. </p>
<p> El aprendizaje automático las personas lo llevamos a cabo de manera automática ya que es un proceso tan sencillo para nosotros que ni nos damos cuenta de cómo se realiza y todo lo que implica. Desde que nacemos hasta que morimos los seres humanos llevamos a cabo diferentes procesos, entre ellos encontramos el de aprendizaje por medio del cual adquirimos conocimientos, desarrollamos habilidades para analizar y evaluar a través de métodos y técnicas así como también por medio de la experiencia propia. Sin embargo, a las máquinas hay que indicarles cómo aprender, ya que si no se logra que una máquina sea capaz de desarrollar sus habilidades, el proceso de aprendizaje no se estará llevando a cabo, sino que solo será una secuencia repetitiva. </p>
<p> Este tipo de aprendizaje usa un árbol de decisiones como modelo predictivo. Se mapean observaciones sobre un objeto con conclusiones sobre el valor final de dicho objeto. </p>
<p> Los árboles son estructuras básicas en la informática. Los árboles de atributos son la base de las decisiones. Una de las dos formas principales de árboles de decisiones es la desarrollada por Quinlan de medir la impureza de la entropía en cada rama, algo que primero desarrolló en el algoritmo ID3 y luego en el C4.5. Otra de las estrategias se basa en el índice GINI y fue desarrollada por Breiman, Friedman et alia. El algoritmo de CART es una implementación de esta estrategia.[5]​ </p>
<p> Los algoritmos de reglas de asociación procuran descubrir relaciones interesantes entre variables. Entre los métodos más conocidos se hallan el algoritmo a priori, el algoritmo Eclat y el algoritmo de Patrón Frecuente. </p>
<p> Los algoritmos genéticos son procesos de búsqueda heurística que simulan la selección natural. Usan métodos tales como la mutación y el cruzamiento para generar nuevas clases que puedan ofrecer una buena solución a un problema dado. </p>
<p> Las redes de neuronas artificiales (RNA) son un paradigma de aprendizaje automático inspirado en las neuronas de los sistemas nerviosos de los animales. Se trata de un sistema de enlaces de neuronas que colaboran entre sí para producir un estímulo de salida. Las conexiones tienen pesos numéricos que se adaptan según la experiencia. De esta manera, las redes neurales se adaptan a un impulso y son capaces de aprender. La importancia de las redes neurales cayó durante un tiempo con el desarrollo de los vectores de soporte y clasificadores lineales, pero volvió a surgir a finales de la década de 2000 con la llegada del aprendizaje profundo. </p>
<p> Las MVS son una serie de métodos de aprendizaje supervisado usados para clasificación y regresión. Los algoritmos de MVS usan un conjunto de ejemplos de entrenamiento clasificado en dos categorías para construir un modelo que prediga si un nuevo ejemplo pertenece a una u otra de dichas categorías. </p>
<p> El análisis por agrupamiento (clustering en inglés) es la clasificación de observaciones en subgrupos —clusters— para que las observaciones en cada grupo se asemejen entre sí según ciertos criterios. </p>
<p> Las técnicas de agrupamiento hacen inferencias diferentes sobre la estructura de los datos; se guían usualmente por una medida de similitud específica y por un nivel de compactamiento interno (similitud entre los miembros de un grupo) y la separación entre los diferentes grupos. </p>
<p> El agrupamiento es un método de aprendizaje no supervisado y es una técnica muy popular de análisis estadístico de datos. </p>
<p> Una red bayesiana, red de creencia o modelo acíclico dirigido es un modelo probabilístico que representa una serie de variables de azar y sus independencias condicionales a través de un grafo acíclico dirigido. Una red bayesiana puede representar, por ejemplo, las relaciones probabilísticas entre enfermedades y síntomas. Dados ciertos síntomas, la red puede usarse para calcular las probabilidades de que ciertas enfermedades estén presentes en un organismo. Hay algoritmos eficientes que infieren y aprenden usando este tipo de representación. </p>
<p> En el aprendizaje automático podemos obtener 3 tipos de conocimiento, que son: </p>
<p> 1. Crecimiento </p>
<p> Es el que se adquiere de lo que nos rodea, el cual guarda la información en la memoria como si dejara huellas. </p>
<p> 2. Reestructuración </p>
<p> Al interpretar los conocimientos el individuo razona y genera nuevo conocimiento al cual se le llama de reestructuración. </p>
<p> 3. Ajuste </p>
<p> Es el que se obtiene al generalizar varios conceptos o generando los propios. </p>
<p> Los tres tipos se efectúan durante un proceso de aprendizaje automático pero la importancia de cada tipo de conocimiento depende de las características de lo que se está tratando de aprender. </p>
<p> El aprendizaje es más que una necesidad, es un factor primordial para satisfacer las necesidades de la inteligencia artificial. </p>
<p> Distinción entre Aprendizaje supervisado y no supervisado[editar] </p>
<p> El aprendizaje supervisado se caracteriza por contar con información que especifica qué conjuntos de datos son satisfactorios para el objetivo del aprendizaje. Un ejemplo podría ser un software que reconoce si una imagen dada es o no la imagen de un rostro: para el aprendizaje del programa tendríamos que proporcionarle diferentes imágenes, especificando en el proceso si se trata o no de rostros. </p>
<p> En el aprendizaje no supervisado, en cambio, el programa no cuenta con datos que definan qué información es satisfactoria o no. El objetivo principal de estos programas suele ser encontrar patrones que permitan separar y clasificar los datos en diferentes grupos, en función de sus atributos. Siguiendo el ejemplo anterior un software de aprendizaje no supervisado no sería capaz de decirnos si una imagen dada es un rostro o no pero sí podría, por ejemplo, clasificar las imágenes entre aquellas que contienen rostros humanos, de animales, o las que no contienen. La información obtenida por un algoritmo de aprendizaje no supervisado debe ser posteriormente interpretada por una persona para darle utilidad. </p>
<p> A continuación se muestran una serie de temas que podrían formar parte del temario de un curso sobre aprendizaje automático. </p>
<p> Modelado de funciones de densidad de probabilidad condicionadas: clasificación y regresión </p>
<p> Árboles de decisión: El aprendizaje por árboles de decisión usa un árbol de decisión como modelo predictivo que mapea observaciones a conclusiones sobre el valor de un objeto dado. </p>
<p> Optimización: La mayoría de los métodos descritos arriba usan algoritmos de optimización o son por sí mismos instancias de problemas de optimización. </p>
<p> El aprendizaje automático nació de la búsqueda de inteligencia artificial. Ya en los primeros días de la IA como disciplina académica, algunos investigadores se interesaron en hacer que las máquinas aprendiesen. Trataron de resolver el problema con diversos métodos simbólicos, así como lo que ellos llamaron 'redes neurales' que eran en general perceptrones y otros modelos básicamente basados en modelos lineares generalizados como se conocen en las estadísticas. </p>
<p> Muchos lenguajes de programación pueden usarse para implementar algoritmos de aprendizaje automático. Los más populares para 2015 eran R y Python.[6]​ R es muy usado ante todo en el campo académico, mientras que Python es más popular en la empresa privada. </p>
<p> Entre los paquetes de software que incluyen algoritmos de aprendizaje automatizado, se hallan los siguientes: </p>
<p> TensorFlow: plataforma multilenguaje y multiplataforma desarrollada por Google y licenciada como Apache 2. </p>
<p> Apache Mahout: plataforma de Java de algoritmos escalables de aprendizaje automático, en especial en las áreas de filtro colaborativo, clustering y clasificación </p>
<p> dlib: una biblioteca bajo licencia Boost para desarrollar en C++ </p>
<p> ELKI: una plataforma para Java con licencia AGPLv3 </p>
<p> R: lenguaje de programación estadístico con numerosas bibliotecas relacionadas al aprendizaje automático (e1071, rpart, nnet, randomForest, entre otras) </p>
<p> scikit-learn: biblioteca en Python que interactúa con NumPy y SciPy </p>
<p> Spark MLlib: una librería que forma parte de Apache Spark, una plataforma para computación de grupos </p>
<p> Los algoritmos de aprendizaje automático a menudo pueden verse afectados por el sesgo que puedan tener los datos (Ver sesgo algoritmico). Por ejemplo no se podrán clasificar todos aquellas entradas de las que no se haya recibido ninguna información en la fase de entrenamiento. De hecho cuando el entrenamiento se realiza con datos clasificados por el ser humano el aprendizaje automático tiende a crear los mismos segos que hay en la sociedad. Algunos ejemplos de esto son cuando en 2015 el algoritmo de Google photos identificaba algunas personas negras con gorilas, o en 2016 cuando el bot de Twitter de Microsoft desarrollo comportamientos racistas y machistas a base de observar el tráfico de datos en dicha red social. Por este motivo en los últimos años ha habido una tendencia a desarrollar métodos para aumentar la equidad, es decir, para reducir el sesgo en este tipo algoritmos por parte de los expertos en IA. Citando a Fei-fei Li "La IA no tiene nada de especial. Se inspira en personas, es creada por personas, y lo más importante impacta en las personas. Es una herramienta muy poderosa que tan solo hemos comenzado a entender, y esa es una gran responsabilidad" [7]​ </p>
<p> Aprendizaje profundo </p>
<p> Aprendizaje profundo (en inglés, deep learning) es un conjunto de algoritmos de aprendizaje automático (en inglés, machine learning) que intenta modelar abstracciones de alto nivel en datos usando arquitecturas computacionales que admiten transformaciones no lineales múltiples e iterativas de datos expresados en forma matricial o tensorial. [1]​ </p>
<p> El aprendizaje profundo es parte de un conjunto más amplio de métodos de aprendizaje automático basados en asimilar representaciones de datos. Una observación (por ejemplo, una imagen) puede ser representada en muchas formas (por ejemplo, un vector de píxeles), pero algunas representaciones hacen más fácil aprender tareas de interés (por ejemplo, "¿es esta imagen una cara humana?") sobre la base de ejemplos, y la investigación en esta área intenta definir qué representaciones son mejores y cómo crear modelos para reconocer estas representaciones. </p>
<p> Varias arquitecturas de aprendizaje profundo, como redes neuronales profundas, redes neuronales profundas convolucionales, y redes de creencia profundas, han sido aplicadas a campos como visión por computador, reconocimiento automático del habla, y reconocimiento de señales de audio y música, y han mostrado producir resultados de vanguardia en varias tareas. </p>
<p> Índice </p>
<p> No existe una única definición de aprendizaje profundo. En general se trata de una clase de algoritmos ideados para el aprendizaje automático. A partir de este punto común, diferentes publicaciones se centran en distintas características, por ejemplo: </p>
<p> Usar una cascada de capas con unidades de procesamiento no lineal para extraer y transformar variables. Cada capa usa la salida de la capa anterior como entrada. Los algoritmos pueden utilizar aprendizaje supervisado o aprendizaje no supervisado, y las aplicaciones incluyen modelización de datos y reconocimiento de patrones. </p>
<p> Estar basados en el aprendizaje de múltiples niveles de características o representaciones de datos. Las características de más alto nivel se derivan de las características de nivel inferior para formar una representación jerárquica. </p>
<p> Aprender múltiples niveles de representación que corresponden con diferentes niveles de abstracción. Estos niveles forman una jerarquía de conceptos. </p>
<p> Todas estas maneras de definir el aprendizaje profundo tienen en común: múltiples capas de procesamiento no lineal; y el aprendizaje supervisado o no supervisado de representaciones de características en cada capa. Las capas forman una jerarquía de características desde un nivel de abstracción más bajo a uno más alto. </p>
<p> Los algoritmos de aprendizaje profundo contrastan con los algoritmos de aprendizaje poco profundo por el número de transformaciones aplicadas a la señal mientras se propaga desde la capa de entrada a la capa de salida. Cada una de estas transformaciones incluye parámetros que se pueden entrenar como pesos y umbrales[2]​(p6). No existe un estándar de facto para el número de transformaciones (o capas) que convierte a un algoritmo en profundo, pero la mayoría de investigadores en el campo considera que aprendizaje profundo implica más de dos transformaciones intermedias[2]​(p7). </p>
<p> Conocidas como GPGPU, las tarjetas de vídeo para procesamiento general permiten ejecutar los algoritmos de entrenamiento y evaluación de los modelos de aprendizaje profundo. Debido a la naturaleza altamente paralelizable de estos problemas, la utilización de las GPGPU permite un aumento en el desempeño de varios órdenes de magnitud. </p>
<p> Los grandes proveedores de servicios en la nube han comenzado a ofrecer servicios de infraestructura especializados para procesamiento con GPU. Nvidia se ha asociado con diversos proveedores para ofrecer dichos servicios, Amazon, Azure e IBM por nombrar algunos.[3]​ </p>
<p> Acompañando a su plataforma TensorFlow, Google introdujo la Machine Learning Platform, que provee servicios de aprendizaje automático modernos con modelos preentrenados y un servicio para generar modelos personalizables. A diferencia de los otros proveedores, se presenta como una alternativa PaaS más que un IaaS. </p>
<p> Centro para el Aprendizaje Biológico y Computacional (CBCL - Center for Biological and Computational Learning) [5] (en inglés) </p>
<p> Tutorial de Stanford en aprendizaje profundo y aprendizaje de rasgos sin supervisión [6] (en inglés) </p>
<p> Conferencia NIPS 2013 (presentaciones sobre material relacionado de aprendizaje profundo) [9] (en inglés) </p>
<p> Tipos de Aprendiaje "Machine Learning" [10] (en español) </p>
<p> Ciencia de datos </p>
<p> La ciencia de datos es un campo interdisciplinario que involucra métodos científicos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea estructurados o no estructurados,[1]​ lo cual es una continuación de algunos campos de análisis de datos como la estadística, la minería de datos, el aprendizaje automático, y la analítica predictiva.[1]​ </p>
<p> También se define La ciencia de datos como "un concepto para unificar estadísticas, análisis de datos, aprendizaje automático, y sus métodos relacionados, a efectos de comprender y analizar los fenómenos reales",[2]​ empleando técnicas y teorías extraídas de muchos campos dentro del contexto de las matemáticas, la estadística, la ciencia de la información, y la informática. </p>
<p> El ganador del premio Turing, Jim Gray, imaginó la ciencia de datos como un "cuarto paradigma" de la ciencia (empírico, teórico, computacional, y ahora basado en datos), y afirmó que "todo lo relacionado con la ciencia está cambiando debido al impacto de la tecnología de la información y el diluvio de datos".[3]​ </p>
<p> En este nuevo paradigma, los investigadores se apoyan de sistemas y procesos que son muy diferentes a los utilizados en el pasado, como son modelos, ecuaciones, algoritmos, así como evaluación e interpretación de resultados.[1]​ </p>
<p> Índice </p>
<p> En 1962, John W. Tukey precedió al término “Ciencia de Datos” en su artículo “The Future of Data Analysis” al explicar una evolución de la estadística matemática. En este, definió por primera vez el análisis de datos como: “Procedimientos para analizar datos, técnicas para interpretar los resultados de dichos procedimientos, formas de planificar la recopilación de datos para hacer su análisis más fácil, más preciso o acertado, y toda la maquinaria y los resultados de las estadísticas matemáticas que se aplican al análisis de datos.”[4]​ En 1977 publicó “Exploratory Data Analysis”, argumentando que era necesario poner más énfasis en el uso de datos para sugerir hipótesis que probar en modelos estadísticos. </p>
<p> La ciencia de datos ha resultado para muchos una disciplina de reciente creación, pero en la realidad este concepto lo utilizó por primera vez el científico danés Peter Naur en la década de los sesenta como sustituto de las ciencias computacionales. En 1974 publicó el libro Concise Survey of Computer Methods [5]​ donde utiliza ampliamente el concepto ciencia de datos, lo que permitió la utilización que más libre en el mundo académico. </p>
<p> En 1977, el International Association for Statistical Computing (IASC) es establecido como una sección del International Statistical Institute (ISI). “Es la misión de la IASC relacionar la metodología estadística tradicional, tecnología computacional moderna, y el conocimiento de expertos del tema, para convertir datos en información y conocimiento".[6]​ </p>
<p> En 1996 el término ‘Ciencia de Datos’ fue utilizado por primera vez en una conferencia llamada "Ciencia de datos, clasificación y métodos relacionados", que tuvo lugar en una reunión de miembros de la ‘International Federation of Classification Societies’ (IFCS) con sede en Kobe, Japón.[6]​ En 1997, C.F. Jeff Wu dio una charla llamada "Statistics = Data Science?", donde describió al trabajo estadístico como una trilogía conformada por recolección de datos, análisis y modelado de datos, y la toma de decisiones, haciendo la petición de que la estadística fuese renombrada como ciencia de datos, y los estadísticos como científicos de datos.[7]​ </p>
<p> En 2001, William S. Cleveland introdujo a la ciencia de datos como una disciplina independiente, extendiendo el campo de la estadística para incluir los avances en computación con datos en su artículo "Data science: an action plan for expanding the technical areas of the field of statistics". Cleveland estableció seis áreas técnicas que en su opinión conformarían al campo de la ciencia de datos: investigaciones multidisciplinarias, modelos y métodos para datos, computación con datos, pedagogía, evaluación de herramientas, y teoría.[8]​ </p>
<p> En abril del 2002, el ‘International Council for Science: Committee on Data for Science and Technology’ (CODATA) empezó la publicación del Data Science Journal,[9]​ enfocada en problemas como la descripción de sistemas de datos, su publicación en Internet, sus aplicaciones, y sus problemas legales. Poco después, en enero del 2003, la Universidad de Columbia empezó a publicar The Journal of Data Science,[10]​ la cual ofreció una plataforma para que todos los profesionales de datos presentaran sus perspectivas e intercambiaran ideas. </p>
<p> En 2005, The National Science Board publicó "Long-Lived Digital Data Collections Enabling Research and Education in the 21st Century", definiendo a los científicos de datos como "científicos de computación e información, programadores de bases de datos y software, y expertos disciplinarios, [...] que son cruciales para la gestión exitosa de una colección digital de datos, cuya actividad primaria es realizar investigación creativa y análisis".[11]​ </p>
<p> Fue en el 2008 que Jeff Hammerbacher y DJ Patil lo reutilizaron para definir sus propios trabajos realizados en Facebook y LinkedIn, respectivamente,[12]​ </p>
<p> En 2009, los investigadores Yangyong Zhu y Yun Xiong del ‘Research Center for Dataology and Data Science’, publicaron “Introduction to Dataology and Data Science”, en donde manifiestan que “a diferencia de las ciencias naturales y las ciencias sociales, Datología y Ciencia de Datos toman datos en la red y su objeto de estudio”.[6]​ </p>
<p> En 2013 fue lanzado el ‘IEEE Task Force on Data Science and Advanced Analytics’,[13]​ mientras que la primera conferencia internacional de ‘IEEE International Conference on Data Science and Advanced Analytics’ fue lanzada en el 2014.[14]​ En 2015, el International Journal on Data Science and Analytics fue lanzado por Springer para publicar trabajos originales en ciencia de datos y analítica de big data.[15]​ </p>
<p> En septiembre de 1994, BusinessWeek publicó el artículo “Marketing de base de datos”, manifestando que las empresas recopilan una gran cantidad de información sobre los clientes, la cual es analizada para predecir la probabilidad de que compre un producto. Afirman que se utiliza ese conocimiento para elaborar un mensaje de marketing calibrado con precisión para que el individuo busque conseguirlo. Asimismo, explican que, en los ochentas, un entusiasmo provocado por la propagación de los lectores de códigos de barras terminó en una decepción generalizada pues muchas empresas fueron abrumadas por la gran cantidad de datos para lograr hacer algo útil con la información de sus clientes. Sin embargo, muchas empresas creen que no hay más remedio que desafiar la frontera marketing y bases de datos para desarrollar más las tecnologías necesarias.[16]​ </p>
<p> En 2014 empresa sueca de música en streaming Spotify compra The Echo Nest, una compañía especializada en ciencia de de datos musicales. Esta ahora es la encargada de almacenar y analizar la información de sus 170 millones de usuarios.[17]​ Con ayuda de dicha empresa, en 2015 Spotify lanzó un servicio de música personalizada llamado Discover Weekly que semanalmente recomienda a sus usuarios una selección de canciones que podría interesarles por medio de algoritmos y análisis de los datos de la música escuchada y el historial de búsqueda de la semana pasada. El servicio recibió una buena recepción generalizada[18]​ y actualmente figura un fuerte punto de venta ante la competencia de la empresa.[19]​ </p>
<p> Netflix, la empresa norteamericana de contenido multimedia en streaming ofrece a sus más de 120 millones de usuarios una plataforma capaz de analizar, mediante algoritmos, las costumbres de consumo de los usuarios para diferenciar los contenidos que estos buscan y lograr determinar qué nuevos contenidos les pueden interesar. Todd Yellin, vicepresidente de producto en Netflix, explicó que algunos de los datos almacenados pueden extenderse desde la hora del día se conectan sus usuarios, cuánto tiempo pasan dentro de la plataforma, su lista de contenidos recientemente vistos (para analizar incluso el orden específico de estos). Toda la información que se almacena es utilizada específicamente para ser analizada, aprender del usuario y poder darle recomendaciones acertadas.[20]​ </p>
<p> En América Latina el Banco Interamericano de Desarrollo (BID) ha desarrollado estudios exploratorios en los que se analiza la ciencia de datos en la implementación y diseño de políticas públicas en la región, tomando casos en países como Argentina y Brasil, presentando recomendaciones para su implementación y mantenimiento. </p>
<p> Estas van desde temas como movilidad urbana sostenible, ciudades inteligentes, seguridad, propiedad de datos y privacidad. Entre las sugerencias presentadas en las investigaciones está la de lograr una “inteligencia del valor público, la cual “tiene la potencialidad de ser un componente estratégico para la toma de decisiones y el diseño, implementación y evaluación de políticas públicas”. Otra de ellas es la capacidad para lograr desde este campo una mejora de rendición de cuentas de los gobiernos ante la ciudadanía y promover un avance en cuanto a la curaduría de datos en las instituciones públicas.[21]​ </p>
<p> Textualmente, Big Data (o macrodatos) se refiere a enormes volúmenes de datos que no pueden procesarse de manera efectiva con las aplicaciones tradicionales que actualmente se aplican.[22]​ De acuerdo con la guía de Amazon Web Service, se considera al Big Data como una colección considerable de datos con dificultades para almacenarse en bases de datos tradicionales, y también para procesarse en servidores estándar y para analizarse con aplicaciones habituales. </p>
<p> El término se suele relacionar con ciencia de datos, pues esa suele ser su fuente de información para análisis; La ciencia de datos logra analizar los grandes conjuntos de datos desordenados e incompletos, para llegar a hallazgos que impulsan decisiones sobre operaciones y productos. </p>
<p> Las personas que se dedican a la ciencia de datos se les conoce como científico de datos, de acuerdo con el proyecto Master in Data Science define al científico de datos como una mezcla de estadísticos, computólogos y pensadores creativos, con las siguientes habilidades: </p>
<p> Recopilar, procesar y extraer valor de las diversas y extensas bases de datos. </p>
<p> Imaginación para comprender, visualizar y comunicar sus conclusiones a los no científicos de datos. </p>
<p> Capacidad para crear soluciones basadas en datos que aumentan los beneficios, reducen los costos. </p>
<p> Los científicos de datos trabajan en todas las industrias y hacen frente a los grandes proyectos de datos en todos los niveles. </p>
<p> El proceso que sigue un científico de datos para responder cuestiones que se le plantean se puede resumir en estos pasos: </p>
<p> Extraer datos, independientemente de la fuente y de su volumen. </p>
<p> Limpiar los datos, para eliminar lo que pueda sesgar los resultados. </p>
<p> Procesar los datos usando métodos estadísticos como inferencia estadística, modelos de regresión, pruebas de hipótesis, etc. </p>
<p> Diseñar experimentos adicionales en caso de ser necesario. </p>
<p> Crear visualizaciones gráficas de los datos relevantes de la investigación.[23]​ </p>
<p> El doctor en estadística Nathan Yau, precisó lo siguiente: el científico de datos es un estadístico que debería aprender interfaces de programación de aplicaciones (API), bases de datos y extracción de datos; es un diseñador que deberá aprender a programar; y es un computólogo que deberá saber analizar y encontrar datos con significado.[24]​ </p>
<p> En la tesis doctoral de Benjamin Fry explicó que el proceso para comprender mejor a los datos comenzaba con una serie de números y el objetivo de responder preguntas sobre los datos, en cada fase del proceso que él propone (adquirir, analizar, filtrar, extraer, representar, refinar e interactuar), se requiere de diferentes enfoques especializados que aporten a una mejor comprensión de los datos. Entre los enfoques que menciona Fry están: ingenieros en sistemas, matemáticos, estadísticos, diseñadores gráficos, especialistas en visualización de la información y especialistas en interacciones hombre-máquina, mejor conocidos por sus siglas en inglés “HCI” (Human-Computer Interaction). Además, Fry afirmó que contar con diferentes enfoques especializados lejos de resolver el problema de entendimiento de datos, se convierte en parte del problema, ya que cada especialización conduce de manera aislada el problema y el camino hacia la solución se puede perder algo en cada transición del proceso.[25]​ </p>
<p> en:Drew Conway en su página web explica con la ayuda de un diagrama de Venn, las principales habilidades que le dan vida y forma a la ciencia de datos, así como sus relaciones de conjuntos. </p>
<p> La ciencia de datos ha cobrado recientemente mucha importancia en nuestro acontecer como disciplina o profesión emergente (científico de datos), y se ha vuelto en foco de atención de cada vez más organizaciones a nivel mundial, tal como lo señaló el economista en jefe de Google Hal Varian, “El trabajo más sexy en los próximos 10 años será ser estadístico”, palabras sobre las que reflexionó Thomas H. Davenport para publicar en el 2012 su artículo: Data Scientist: The Sexiest Job of the 21st Century [26]​ donde describe el perfil que debe tener el científico de datos como el híbrido de un hacker de datos, un analista, un comunicador, y un consejero confiable, combinación extremadamente poderosa y poco común. Davenport, también señala que el científico de datos no se siente cómodo como se dice coloquialmente “con la correa corta”, es decir, debe tener la libertad de experimentar y explorar posibilidades. Además, Davenport en el mismo artículo presenta un decálogo de cómo encontrar el científico de datos que la organización necesita (ver página 74 del artículo). </p>
<p> El informe que publicó “McKinsey”[27]​ en el 2011, estimó que para el mundo de grandes datos en el que vivimos, espera que la demanda por talento experto en análisis de datos podría alcanzar de los 440,000 a 490,000 puestos de trabajo para el 2018.[cita requerida] </p>
<p> La cadena de producción de software tiene cinco pasos: (1) Conceptualización ; (2) Prueba de coherencia (interés, utilidad) ; (3) Desarrollo ; (4) Comercialización ; (5) Soporte y mejora del software (actualización y mantenimiento). </p>
<p> Infraestructura de clave pública CA: Autoridad de Certificación ; VA: Autoridad de Validación ; RA: Autoridad de Registro. </p>
<p> El desarrollador puede contribuir a la visión general del proyecto más a nivel de aplicación que a nivel de componentes, así como en las tareas de programación individuales. </p>
<p> Conforme pasa el tiempo, la separación entre el diseño de sistemas informáticos, el desarrollo de software, y la programación, se van haciendo más claras y diferenciadas. En el mercado laboral suele encontrarse una diferenciación entre programadores y desarrolladores, siendo estos últimos los que diseñan la estructura o jerarquía de clases. Incluso esos desarrolladores se convierten en arquitectos de sistemas informáticos, o sea, aquellos que diseñan la arquitectura a varios niveles o las interacciones entre componentes de un proyecto de software grande. </p>
<p> Trabajo en equipo: los proyectos son en general una colaboración entre varios desarrolladores, que tratan cada uno una parte específica del sistema que se desarrolla, y también de otros tipos de colaboradores, como los comerciales (que definen con el cliente la finalidad y las necesidades del producto), o como los diseñadores gráficos (que definen el aspecto de las pantallas y cuestiones relativas a la ergonomía), etc. </p>
<p> Concepción o diseño: a partir de un pliego de condiciones (user requirement specifications), se definen las especificaciones técnicas (estructura de datos, comunicación entre módulos, etcétera). </p>
<p> Pruebas: sirven para detectar las disconformidades de trabajadores y clientes, y los errores. </p>
<p> Mantenimiento: abarca la corrección de los errores después que comienza el uso comercial del programa informático, así como las mejoras que se revelen como necesarias para hacer evolucionar el producto. </p>
<p> Nota: Para que un programador se convierta en desarrollador, debe poseer experiencia y saber el manejo y la aplicación de metodologías de desarrollo; es sobre todo la experiencia y el conocimiento técnico, lo que ha impulsado la evolución del término 'programador' hacia el término 'desarrollador'. </p>
<p> Índice </p>
<p> Según el diccionario de la lengua francesa 'Larousse'[2]​ y la 'Office québécois de la langue française' (en español: 'Oficina quebecuense de la lengua francesa'), y aunque esto no es reconocido por el Centre national de ressources textuelles et lexicales), el término «développeur»[1]​ (en español: «desarrollador») se aplica (en el dominio de la informática), a una persona que concibe y desarrolla aplicaciones informáticas, o una empresa organizada y orientada a desarrollar ese tipo de aplicaciones y su asociado software. Sin embargo, en el caso de las personas, es posible distinguir a los desarrolladores por especialidad y formación, o sea, diferenciar entre los expertos en el arte del manejo, uso, y creación de software, y los especializados particularmente en todo lo relativo a Internet y al manejo de ordenadores (sistemas operativos, programas utilitarios, etc),[3]​ así como los especializados en el sector de las Tecnologías de la información y la comunicación (TICS), grupos todos estos en los que se encuentran desarrolladores con las características que se indican seguidamente: </p>
<p> El desarrollador informático o desarrollador web[4]​[5]​ responsable de los códigos-fuente elaborados en diferentes lenguajes de programación web (programación con el propósito de hacer páginas web, lo que a menudo es llamado programación web o desarrollo web), utilizando entre otros lenguajes de marcas, el HyperText Markup Language –HTML– (en español: lenguaje de marcas de hipertexto), el Cascading StyleSheets –CSS– (en español: Hojas de estilo en cascada), y el Extensible Markup Language –XML– (en español: Lenguaje de Marcas Extensible), etc. </p>
<p> Para responder adecuadamente y en la mejor forma a las necesidades del cliente, conviene que en una primera etapa el desarrollador establezca un «cuaderno de carga», a efectos de determinar y especificar las necesidades del cliente en materia de automatización, informatización, y control, asunto por asunto. En esta etapa, conviene que se aclaren las siguientes cuestiones: </p>
<p> En cuanto a la informatización, convendrá aclarar las ventajas de la misma, ya que hay casos en los que las cosas pueden llegar a ser más eficientes y con mejores resultados económicos sin informatización que con ella. Esta cuestión es crucial y fundamental, y deberá ser analizada con visión de largo alcance, pues de lo que generalmente se trata, es de concebir un sistema informático que acompañe al cliente en su evolución futura durante los próximos años. </p>
<p> Esta es una fase crucial puesto que no sólo se debe responder a las necesidades actuales del cliente, sino también diseñar un sistema informático que acompañará al cliente en la evolución de sus actividades. </p>
<p> En una segunda etapa se desarrolla una solución técnica (Hardware) y se crea un modelo (análisis orgánico) del futuro software (software) que gestionará el futuro sistema informático (si no existe). </p>
<p> Luego se deben escribir las líneas de código necesarias para el correcto funcionamiento (programación), participar en las fases de pruebas, confeccionar la documentación técnica, y hacer el seguimiento y el mantenimiento del producto. </p>
<p> El desarrollador puede también capacitar a los usuarios. </p>
<p> Sus principales responsabilidades son: </p>
<p> El trabajo de equipo: el equipo trabaja para la realización de la aplicación que corresponde a las necesidades del cliente. </p>
<p> Définition: logiciel, sitio digital 'Larousse, Dictionnaire de la Langue Française'. </p>
<p> La Ingeniería de Software es una de las ramas de las ciencias de la computación que estudia la creación de software confiable y de calidad, basándose en métodos y técnicas de ingeniería. Brindando soporte operacional y de mantenimiento, el campo de estudio de la ingeniería de software.[1]​ Integra ciencias de la computación, ciencias aplicadas y las ciencias básicas en las cuales se encuentra apoyada la ingeniería.[2]​ </p>
<p> Se citan las definiciones más reconocidas, formuladas por prestigiosos autores: </p>
<p> Ingeniería de software es el estudio de los principios y metodologías para el desarrollo y mantenimiento de sistemas software (Zelkovitz, 1978). </p>
<p> Ingeniería de software es la aplicación práctica del conocimiento científico al diseño y construcción de programas de computadora y a la documentación asociada requerida para desarrollar, operar y mantenerlos. Se conoce también como desarrollo de software o producción de software (Bohem, 1976). </p>
<p> La ingeniería de software trata del establecimiento de los principios y métodos de la ingeniería a fin de obtener software de modo rentable, que sea fiable y trabaje en máquinas reales (Bauer, 1972). </p>
<p> La ingeniería de software es la aplicación de un enfoque sistemático, disciplinado y cuantificable al desarrollo, operación, y mantenimiento del software.[3]​ </p>
<p> En 2004, la U. S. Bureau of Labor Statistics (Oficina de Estadísticas del Trabajo de Estados Unidos) contó 760 840 ingenieros de software de computadora.[4]​[actualizar] </p>
<p> El término "ingeniero de software", sin embargo, se utiliza de manera genérica en el ambiente empresarial, y no todos los que se desempeñan en el puesto de ingeniero de software poseen realmente títulos de ingeniería de universidades reconocidas.[5]​ </p>
<p> Algunos autores consideran que "desarrollo de software" es un término más apropiado que "ingeniería de software" para el proceso de crear software. Personas como Pete McBreen (autor de Software Craftmanship) cree que el término IS implica niveles de rigor y prueba de procesos que no son apropiados para todo tipo de desarrollo de software. </p>
<p> Indistintamente se utilizan los términos "ingeniería de software" o "ingeniería del software"; aunque menos común también se suele referenciar como "ingeniería en software".[6]​[7]​[8]​ En Hispanoamérica los términos más comúnmente usados son los dos primeros. </p>
<p> La creación del software es un proceso intrínsecamente creativo y la ingeniería del software trata de sistematizar este proceso con el fin de acotar el riesgo de fracaso en la consecución del objetivo, por medio de diversas técnicas que se han demostrado adecuadas sobre la base de la experiencia previa. </p>
<p> La ingeniería de software se puede considerar como la ingeniería aplicada al software, esto es, por medios sistematizados y con herramientas preestablecidas, la aplicación de ellos de la manera más eficiente para la obtención de resultados óptimos; objetivos que siempre busca la ingeniería. No es solo de la resolución de problemas, sino más bien teniendo en cuenta las diferentes soluciones, elegir la más apropiada. </p>
<p> La producción de software utiliza criterios y normas de la ingeniería de software, lo que permite transformarlo en un producto industrial usando bases de la ingeniería como métodos, técnicas y herramientas para desarrollar un producto innovador regido por metodologías y las buenas prácticas. Dicho producto es un medio que interviene en las funciones de sus usuarios para obtener un proceso productivo más eficaz y eficiente; hoy en día las empresas no podrían funcionar sin software porque este es un producto de uso masivo; por lo cual, el nivel de una empresa está determinado por la calidad de su infraestructura tecnológica y los productos desarrollados o adquiridos de acuerdo a sus necesidades. </p>
<p> Cuando aparecieron las primeras computadoras digitales en la década de 1940,[9]​ el desarrollo de software era algo tan nuevo que era casi imposible hacer predicciones de las fechas estimadas de finalización del proyecto y muchos de ellos sobrepasaban los presupuestos y tiempo estimados.. Los desarrolladores tenían que volver a escribir todos sus programas para correr en máquinas nuevas que salían cada uno o dos años, haciendo obsoletas las ya existentes. </p>
<p> El término ingeniería del software apareció por primera vez a finales de la década de 1950. La ingeniería de software fue estimulada por la crisis del software de las décadas de entre 1960 y 1980. La ingeniería del software viene a ayudar a identificar y corregir mediante principios y metodologías los procesos de desarrollo y mantenimiento de sistemas de software. </p>
<p> Aparte de la crisis del software de las décadas de entre 1960 y 1980, la ingeniería de software se ve afectada por accidentes que conllevaron a la muerte de tres personas; esto sucedió cuando la máquina de radioterapia Therac-25 emite una sobredosis masiva de radiación y afecto contra la vida de estas personas.[10]​ Esto remarca los riesgos de control por software,[11]​ afectando directamente al nombre de la ingeniería de software. </p>
<p> A principios de los 1980,[12]​ la ingeniería del software ya había surgido como una genuina profesión, para estar al lado de las ciencias de la computación y la ingeniería tradicional. Antes de esto, las tareas eran corridas poniendo tarjetas perforadas como entrada en el lector de tarjetas de la máquina y se esperaban los resultados devueltos por la impresora. </p>
<p> Debido a la necesidad de traducir frecuentemente el software viejo para atender las necesidades de las nuevas máquinas, se desarrollaron lenguajes de orden superior. A medida que apareció el software libre, las organizaciones de usuarios comúnmente lo liberaban. </p>
<p> Durante mucho tiempo, solucionar la crisis del software fue de suma importancia para investigadores y empresas que se dedicaban a producir herramientas de software. </p>
<p> Para la década de 1980, el costo de propiedad y mantenimiento del software fue dos veces más caro que el propio desarrollo del software, y durante la década de 1990, el costo de propiedad y mantenimiento aumentó 30 % con respecto a la década anterior. En 1995, muchos de los proyectos de desarrollo estaban operacionales, pero no eran considerados exitosos. El proyecto de software medio sobrepasaba en un 50 % la estimación de tiempo previamente realizada, además, el 75 % de todos los grandes productos de software que eran entregados al cliente tenían fallas tan graves, que no eran usados en lo absoluto o simplemente no cumplían con los requerimientos del cliente.[11]​ </p>
<p> Algunos expertos argumentaron que la crisis del software era debido a la falta de disciplina de los programadores. </p>
<p> Cada nueva tecnología y práctica de la década de 1970 a la de 1990 fue pregonada como la única solución a todos los problemas y el caos que llevó a la crisis del software. Lo cierto es que la búsqueda de una única clave para el éxito nunca funcionó. El campo de la ingeniería de software parece un campo demasiado complejo y amplio para una única solución que sirva para mejorar la mayoría de los problemas, y cada problema representa solo una pequeña porción de todos los problemas de software. </p>
<p> El auge del uso del Internet llevó a un vertiginoso crecimiento en la demanda de sistemas internacionales de despliegue de información en la World Wide Web. Los desarrolladores se vieron en la tarea de manejar ilustraciones, mapas, fotografías y animaciones, a un ritmo nunca antes visto, con casi ningún método para optimizar la visualización y almacenamiento de imágenes. También fueron necesarios sistemas para traducir el flujo de información en múltiples idiomas extranjeros a lenguaje natural humano, con muchos sistemas de software diseñados para uso multilenguaje, basado en traductores humanos. </p>
<p> La ingeniería de software contribuyó alrededor de 90 000 millones de dólares por año, ya que entró en juego el Internet. Esto hace que los desarrolladores tuviesen que manejar imágenes mapas y animaciones para optimizar la visualización/almacenamiento de imágenes (como el uso de imágenes en miniatura). El uso de los navegadores y utilización de lenguaje HTML cambia drásticamente la visión y recepción de la información. </p>
<p> Las amplias conexiones de red causaron la proliferación de virus informáticos y basura o spam en los correos electrónicos (E-mail). Esta situación puso en una carrera contra el tiempo a los desarrolladores con el fin de crear nuevos sistemas de bloqueo o seguridad de dichas anomalías en la informática, ya que se volvían sumamente tediosas y difíciles de arreglar[11]​ </p>
<p> Después de una fuerte y creciente demanda surge la necesidad de crear soluciones de software a bajo costo, lo que conlleva al uso de metodologías más simples y rápidas que desarrollan software funcional. Cabe señalar que los sistemas más pequeños tenían un enfoque más simple y rápido para poder administrar el desarrollo de cálculos y algoritmos de software. </p>
<p> La ingeniería de software aplica diferentes normas y métodos que permiten obtener mejores resultados, en cuanto al desarrollo y uso del software, mediante la aplicación correcta de estos procedimientos se puede llegar a cumplir de manera satisfactoria con los objetivos fundamentales de la ingeniería de software. </p>
<p> Entre los objetivos de la ingeniería de software están: </p>
<p> Mejorar el diseño de aplicaciones o software de tal modo que se adapten de mejor manera a las necesidades de las organizaciones o finalidades para las cuales fueron creadas. </p>
<p> Promover mayor calidad al desarrollar aplicaciones complejas. </p>
<p> Brindar mayor exactitud en los costos de proyectos y tiempo de desarrollo de los mismos. </p>
<p> Aumentar la eficiencia de los sistemas al introducir procesos que permitan medir mediante normas específicas, la calidad del software desarrollado, buscando siempre la mejor calidad posible según las necesidades y resultados que se quieren generar. </p>
<p> Una mejor organización de equipos de trabajo, en el área de desarrollo y mantenimiento de software. </p>
<p> Detectar a través de pruebas, posibles mejoras para un mejor funcionamiento del software desarrollado.[13]​ </p>
<p> Son todas aquellas personas que intervienen en la planificación de cualquier instancias de software (por ejemplo: gestor, ingeniero de software experimentado, etc.), El número de personas requerido para un proyecto de software solo puede ser determinado después de hacer una estimación del esfuerzo de desarrollo... </p>
<p> Es el entorno de las aplicaciones (software y hardware) el hardware proporciona el medio físico para desarrollar las aplicaciones (software), este recurso es indispensable.[14]​ </p>
<p> En los Estados Unidos, el software contribuyó a una octava parte de todo el incremento del PIB durante la década de 1990 (alrededor de 90 000 millones de dólares por año), y un noveno de todo el crecimiento de productividad durante los últimos años de la década (alrededor de 33.000 millones de dólares estadounidenses por año). La ingeniería de software contribuyó a US$ 1 billón de crecimiento económico y productividad en esa década. Alrededor del globo, el software contribuye al crecimiento económico de maneras similares, aunque es difícil de encontrar estadísticas fiables. [cita requerida] </p>
<p> Además, con la industria del lenguaje está hallando cada vez más campos de aplicación a escala global. </p>
<p> La ingeniería de software cambia la cultura del mundo debido al extendido uso de la computadora. El correo electrónico (e-mail), la WWW y la mensajería instantánea permiten a la gente interactuar de nuevas maneras. El software baja el costo y mejora la calidad de los servicios de salud, los departamentos de bomberos, las dependencias gubernamentales y otros servicios sociales. Los proyectos exitosos donde se han usado métodos de ingeniería de software incluyen a GNU/Linux, el software del transbordador espacial, los cajeros automáticos y muchos otros. </p>
<p> Es un lenguaje de modelado muy reconocido y utilizado actualmente que se utiliza para describir o especificar métodos. También es aplicable en el desarrollo de software. </p>
<p> Las siglas UML significan lenguaje unificado de modelado esto quiere decir que no pretende definir un modelo estándar de desarrollo, sino únicamente un lenguaje de modelado.[15]​ </p>
<p> Un lenguaje de modelado consta de vistas, elementos de modelo y un conjunto de reglas sintácticas, semánticas y pragmáticas que indican cómo utilizar los elementos. </p>
<p> BPMN (notación para el modelado de procesos de negocios)[editar] </p>
<p> El objetivo de la notación para el modelado de procesos de negocios es proporcionar de una manera fácil de definir y analizar los procesos de negocios públicos y privados simulando un diagrama de flujo. La notación ha sido diseñada específicamente para coordinar la secuencia de los procesos y los mensajes que fluyen entre los participantes del mismo, con un conjunto de actividades relacionadas. Características básicas de los elementos de BPMN </p>
<p> Objetos de flujo: eventos, actividades, rombos de control de flujo (gateways). </p>
<p> Objetos de conexión: flujo de secuencia, flujo de mensaje, asociación. </p>
<p> Swimlanes (carriles de piscina): pool, lane. </p>
<p> Artefactos: objetos de datos, grupo, anotación.[15]​ </p>
<p> Un diagrama de flujo de datos permite representar el movimiento de datos a través de un sistema por medio de modelos que describen los flujos de datos, los procesos que transforman o cambian los datos, los destinos de datos y los almacenamientos de datos a la cual tiene acceso el sistema. </p>
<p> Su inventor fue Larry Constantine, basado en el modelo de computación de Martin y Estrin: flujo gráfico de datos. Con los diagramas de flujo de datos determina la manera en que cualquier sistema puede desarrollarse, ayuda en la identificación de los datos de la transacción en el modelo de datos y proporciona al usuario una idea física de cómo resultarán los datos a última instancia.[16]​ </p>
<p> Las Herramienta CASE son herramientas computacionales (software) que están destinadas a asistir en los procesos de ciclo de vida de un software, facilitan la producción del software, varias se basan principalmente en la idea de un modelo gráfico.[17]​ </p>
<p> Un objetivo de décadas ha sido el encontrar procesos y metodologías, que sean sistemáticas, predecibles y repetibles, a fin de mejorar la productividad en el desarrollo y la calidad del producto software, en pocas palabras, determina los pasos a seguir y como realizarlos para finalizar una tarea. </p>
<p> La ingeniería de software requiere llevar a cabo numerosas tareas agrupadas en etapas, al conjunto de estas etapas se le denomina ciclo de vida. Las etapas comunes a casi todos los modelos de ciclo de vida son las siguientes: </p>
<p> Se debe identificar sobre qué se está trabajando, es decir, el tema principal que motiva el inicio del estudio y creación del nuevo software o modificación de uno ya existente. A su vez identificar los recursos que se tienen, en esto entra el conocer los recursos humanos y materiales que participan en el desarrollo de las actividades. Es importante entender el contexto del negocio para identificar adecuadamente los requisitos. </p>
<p> Se tiene que tener dominio de la información de un problema, lo cual incluye los datos fuera del software (usuarios finales, otros sistemas o dispositivos externos), los datos que salen del sistema (por la interfaz de usuario, interfaces de red, reportes, gráficas y otros medios) y los almacenamientos de datos que recaban y organizan objetos persistentes de datos (por ejemplo, aquellos que se conservan de manera permanente). </p>
<p> También hay que ver los puntos críticos, lo que significa tener de una manera clara los aspectos que entorpecen y limitan el buen funcionamiento de los procedimientos actuales, los problemas más comunes y relevantes que se presentan, los motivos que crean insatisfacción y aquellos que deben ser cubiertos a plenitud. Por ejemplo: ¿El contenido de los reportes generados, satisface realmente las necesidades del usuario? ¿Los tiempos de respuesta ofrecidos, son oportunos?, etc. </p>
<p> Hay que definir las funciones que realizará el software ya que estas ayudan al usuario final y al funcionamiento del mismo programa. </p>
<p> Se tiene que tener en cuenta cómo será el comportamiento del software ante situaciones inesperadas como lo son por ejemplo una gran cantidad de usuarios usando el software o una gran cantidad de datos entre otros. </p>
<p> El análisis de requisitos puede parecer una tarea sencilla, pero no lo es debido a que muchas veces los clientes piensan que saben todo lo que el software necesita para su buen funcionamiento, sin embargo se requiere la habilidad y experiencia de algún especialista para reconocer requisitos incompletos, ambiguos o contradictorios. Estos requisitos se determinan tomando en cuenta las necesidades del usuario final, introduciendo técnicas que nos permitan mejorar la calidad de los sistemas sobre los que se trabaja.[19]​ </p>
<p> El resultado del análisis de requisitos con el cliente se plasma en el documento ERS (especificación de requisitos del sistema), cuya estructura puede venir definida por varios estándares, tales como CMMI. Asimismo, se define un diagrama de entidad/relación, en el que se plasman las principales entidades que participarán en el desarrollo del software. </p>
<p> La captura, análisis y especificación de requisitos (incluso pruebas de ellos), es una parte crucial; de esta etapa depende en gran medida el logro de los objetivos finales. Se han ideado modelos y diversos procesos metódicos de trabajo para estos fines. Aunque aún no está formalizada, ya se habla de la ingeniería de requisitos. </p>
<p> La IEEE Std. 830-1998 normaliza la creación de las especificaciones de requisitos de software (Software Requirements Specification). </p>
<p> Finalidades del análisis de requisitos: </p>
<p> Brindar al usuario todo lo necesario para que pueda trabajar en conjunto con el software desarrollado obteniendo los mejores resultados posibles. </p>
<p> Tener un control más completo en la etapa creación del software, en cuanto a tiempo de desarrollo y costos. </p>
<p> Utilización de métodos más eficientes que permitan el mejor aprovechamiento del software según sea la finalidad de uso del mismo. </p>
<p> Aumentar la calidad del software desarrollado al disminuir los riesgos de mal funcionamiento.[19]​ </p>
<p> No siempre en la etapa de "análisis de requisitos" las distintas metodologías de desarrollo llevan asociado un estudio de viabilidad y/o estimación de costes. El más conocido de los modelos de estimación de coste del software es el modelo COCOMO </p>
<p> Los software tienen la capacidad de emular inteligencia creando un modelo de ciertas características de la inteligencia humana pero solo posee funciones predefinidas que abarcan un conjunto de soluciones que en algunos campos llega a ser limitado. Aun cuando tiene la capacidad de imitar ciertos comportamientos humanos no es capaz de emular el pensamiento humano porque actúa bajo condiciones. </p>
<p> Otro aspecto limitante de los software proviene del proceso totalmente mecánico que requiere de un mayor esfuerzo y tiempos elevados de ejecución lo que lleva a tener que implementar el software en una máquina de mayor capacidad. </p>
<p> La especificación de requisitos describe el comportamiento esperado en el software una vez desarrollado. Gran parte del éxito de un proyecto de software radicará en la identificación de las necesidades del negocio (definidas por la alta dirección), así como la interacción con los usuarios funcionales para la recolección, clasificación, identificación, priorización y especificación de los requisitos del software. </p>
<p> Entre las técnicas utilizadas para la especificación de requisitos se encuentran: </p>
<p> Siendo los primeros más rigurosas y formales, los segundas más ágiles e informales. </p>
<p> La integración de infraestructura, desarrollo de aplicaciones, bases de datos y herramientas gerenciales, requieren de capacidad y liderazgo para poder ser conceptualizados y proyectados a futuro, solucionando los problemas de hoy. El rol en el cual se delegan todas estas actividades es el del Arquitecto. </p>
<p> El arquitecto de software es la persona que añade valor a los procesos de negocios gracias a su valioso aporte de soluciones tecnológicas. </p>
<p> La arquitectura de sistemas en general, es una actividad de planeación, ya sea a nivel de infraestructura de red y hardware, o de software. </p>
<p> Lo principal en este punto es poner en claro los aspectos lógicos y físicos de las salidas, modelos de organización y representación de datos, entradas y procesos que componen el sistema, considerando las bondades y limitaciones de los recursos disponibles en la satisfacción de las pacificaciones brindadas para el análisis. </p>
<p> Hay que tener en consideración la arquitectura del sistema en la cual se va a trabajar, elaborar un plan de trabajo viendo la prioridad de tiempo y recursos disponibles. En los diseños de salidas entra los que es la interpretación de requerimientos lo cual es el dominio de información del problema, las funciones visibles para el usuario, el comportamiento del sistema y un conjunto de clases de requerimientos que agrupa los objetos del negocio con los métodos que les dan servicio. </p>
<p> La arquitectura de software consiste en el diseño de componentes de una aplicación (entidades del negocio), generalmente utilizando patrones de arquitectura. El diseño arquitectónico debe permitir visualizar la interacción entre las entidades del negocio y además poder ser validado, por ejemplo por medio de diagramas de secuencia. Un diseño arquitectónico describe en general el cómo se construirá una aplicación de software. Para ello se documenta utilizando diagramas, por ejemplo: </p>
<p> Los diagramas de clases y de base de datos son los mínimos necesarios para describir la arquitectura de un proyecto que iniciará a ser codificado. Dependiendo del alcance del proyecto, complejidad y necesidades, el arquitecto elegirá cuales de los diagramas se requiere elaborar. </p>
<p> Las herramientas para el diseño y modelado de software se denominan CASE (Computer Aided Software Engineering) entre las cuales se encuentran: </p>
<p> Implementar un diseño en código puede ser la parte más obvia del trabajo de ingeniería de software, pero no necesariamente es la que demanda mayor trabajo y ni la más complicada. La complejidad y la duración de esta etapa está íntimamente relacionada al o a los lenguajes de programación utilizados, así como al diseño previamente realizado. </p>
<p> Para el desarrollo de la aplicación es necesario considerar cinco fases para tener una aplicación o programa eficiente, estas son: </p>
<p> Desarrollo de la infraestructura: Esta fase permite el desarrollo y la organización de los elementos que formaran la infraestructura de la aplicación, con el propósito de finalizar la aplicación eficientemente. </p>
<p> Adaptación del paquete: El objetivo principal de esta fase es entender de una manera detallada el funcionamiento del paquete, esto tiene como finalidad garantizar que el paquete pueda ser utilizado en su máximo rendimiento, tanto para negocios o recursos. Todos los elementos que componen el paquete son inspeccionados de manera detallada para evitar errores y entender mejor todas las características del paquete. </p>
<p> Desarrollo de unidades de diseño de interactivas: En esta fase se realizan los procedimientos que se ejecutan por un diálogo usuario-sistema. Los procedimientos de esta fase tienen como objetivo principal: </p>
<p> Establecer específicamente las acciones que debe efectuar la unidad de diseño. </p>
<p> La creación de componentes para sus procedimientos. </p>
<p> Ejecutar pruebas unitarias y de integración en la unidad de diseño. </p>
<p> Desarrollo de unidades de diseño batch: En esta fase se utilizan una serie de combinación de técnicas, como diagrama de flujo, diagramas de estructuras, tablas de decisiones, etc. Cualquiera a utilizar será beneficioso para plasmar de manera clara y objetiva las especificaciones y que así el programador tenga mayor comprensión a la hora de programar y probar los programas que le corresponden. </p>
<p> Desarrollo de unidades de diseño manuales: En esta fase el objetivo central es proyectar todos los procedimientos administrativos que desarrollarán en torno a la utilización de los componentes computarizados.[21]​ </p>
<p> Consiste en comprobar que el software realice correctamente las tareas indicadas en la especificación del problema. Una técnica es probar por separado cada módulo del software (prueba unitaria), y luego probarlo de manera integral (pruebas de integración), para así llegar al objetivo. Se considera una buena práctica el que las pruebas sean efectuadas por alguien distinto al desarrollador que la programó, idealmente un área de pruebas; sin perjuicio de lo anterior el programador debe hacer sus propias pruebas. En general hay dos grandes maneras de organizar un área de pruebas, la primera es que esté compuesta por personal inexperto y que desconozca el tema de pruebas, de esta manera se evalúa que la documentación entregada sea de calidad, que los procesos descritos son tan claros que cualquiera puede entenderlos y el software hace las cosas tal y como están descritas. El segundo enfoque es tener un área de pruebas conformada por programadores con experiencia, personas que saben sin mayores indicaciones en qué condiciones puede fallar una aplicación y que pueden poner atención en detalles que personal inexperto no consideraría. </p>
<p> De acuerdo con Roger S. Pressman, el proceso de pruebas se centra en los procesos lógicos internos del software, asegurando que todas las sentencias se han comprobado, y en los procesos externos funcionales, es decir, la realización de pruebas para la detección de errores. Se requiere poder probar el software con sujetos reales que puedan evaluar el comportamiento del software con el fin de proporcionar realimentación a los desarrolladores. Es importante que durante el proceso de desarrollo del software no se pierda contacto con los interesados o solicitantes del desarrollo de software, de esta manera los objetivos del proyecto se mantendrán vigentes y se tendrá una idea clara de los aspectos que tienen que probarse durante el período de pruebas.[22]​ </p>
<p> Una implementación es la realización de una especificación técnica o algoritmos con un programa, componente software, u otro sistema de cómputo. Muchas especificaciones son dadas según a su especificación o un estándar. Las especificaciones recomendadas según el World Wide Web Consortium, y las herramientas de desarrollo del software contienen implementaciones de lenguajes de programación. El modelo de implementación es una colección de componentes y los subsistemas que contienen. Componentes tales como: ficheros ejecutables, ficheros de código fuente y todo otro tipo de ficheros que sean necesarios para la implementación y despliegue del sistema. </p>
<p> La etapa de implementación del diseño de software es el proceso de convertir una especificación del sistema en un sistema ejecutable. Siempre implica los procesos de diseño y programación de software, pero, si se utiliza un enfoque evolutivo de desarrollo, también puede implicar un refinamiento de la especificación del software. Esta etapa es una descripción de la estructura del software que se va a implementar, los datos que son parte del sistema, las interfaces entre los componentes del sistema, y algunas veces los algoritmos utilizados.[23]​ </p>
<p> Es todo lo concerniente a la documentación del propio desarrollo del software y de la gestión del proyecto, pasando por modelaciones (UML), diagramas de casos de uso, pruebas, manuales de usuario, manuales técnicos, etc; todo con el propósito de eventuales correcciones, usabilidad, mantenimiento futuro y ampliaciones al sistema. </p>
<p> Fase dedicada a mantener y mejorar el software para corregir errores descubiertos e incorporar nuevos requisitos. Esto puede llevar más tiempo incluso que el desarrollo del software inicial. Alrededor de 2/3 del tiempo de ciclo de vida de un proyecto[14]​ está dedicado a su mantenimiento. Una pequeña parte de este trabajo consiste eliminar errores (bugs); siendo que la mayor parte reside en extender el sistema para incorporarle nuevas funcionalidades y hacer frente a su evolución. </p>
<p> Desde el punto de vista de los ingenieros de software[editar] </p>
<p> Optimizar el conjunto y cada una de las fases del proceso de desarrollo </p>
<p> Desde el punto de vista de cliente o usuario final[editar] </p>
<p> Garantizar el nivel de calidad del producto final </p>
<p> Obtener el ciclo de vida adecuado para el proyecto </p>
<p> Confianza en los plazos del tiempo mostrados en la definición del proyecto </p>
<p> Modelos y ciclos de vida del desarrollo de software[editar] </p>
<p> La ingeniería de software, con el fin de ordenar el caos que era anteriormente el desarrollo de software, dispone de varios modelos, paradigmas y filosofías de desarrollo, estos los conocemos principalmente como modelos o ciclos de vida del desarrollo de software, esto incluye el proceso que se sigue para construir, entregar y hacer evolucionar el software, desde la concepción de una idea hasta la entrega y el retiro del sistema y representa todas las actividades y artefactos (productos intermedios) necesarios para desarrollar una aplicación.[25]​ </p>
<p> El ciclo de vida de un software contiene los siguientes procedimientos: </p>
<p> Definición de objetivos: definir el resultado del proyecto y su papel en la estrategia global.[26]​ </p>
<p> Análisis de los requisitos y su viabilidad: recopilar, examinar y formular los requisitos del cliente y examinar cualquier restricción que se pueda aplicar.[26]​ </p>
<p> Diseño general: requisitos generales de la arquitectura de la aplicación.[26]​ </p>
<p> Diseño en detalle: definición precisa de cada subconjunto de la aplicación.[26]​ </p>
<p> Programación (programación e implementación): es la implementación de un lenguaje de programación para crear las funciones definidas durante la etapa de diseño.[26]​ </p>
<p> Prueba de unidad: prueba individual de cada subconjunto de la aplicación para garantizar que se implementaron de acuerdo con las especificaciones.[26]​ </p>
<p> Integración: para garantizar que los diferentes módulos se integren con la aplicación. Este es el propósito de la prueba de integración que está cuidadosamente documentada.[26]​ </p>
<p> Prueba beta (o validación), para garantizar que el software cumple con las especificaciones originales.[26]​ </p>
<p> Documentación: sirve para documentar información necesaria para los usuarios del software y para desarrollos futuros.[26]​ </p>
<p> Implementación </p>
<p> Mantenimiento: para todos los procedimientos correctivos (mantenimiento correctivo) y las actualizaciones secundarias del software (mantenimiento continuo).[26]​ </p>
<p> En ingeniería de software el modelo en cascada ―también llamado desarrollo en cascada o ciclo de vida clásico― se basa en un enfoque metodológico que ordena rigurosamente las etapas del ciclo de vida del software, esto sugiere una aproximación sistemática secuencial hacia el proceso de desarrollo del software, que se inicia con la especificación de requisitos del cliente y continúa con la planificación, el modelado, la construcción y el despliegue para culminar en el soporte del software terminado.[27]​ </p>
<p> En ingeniería de software, el modelo de prototipos pertenece a los modelos de desarrollo evolutivo. Este permite que todo el sistema, o algunos de sus partes, se construyan rápidamente para comprender con facilidad y aclarar ciertos aspectos en los que se aseguren que el desarrollador, el usuario, el cliente estén de acuerdo en lo que se necesita así como también la solución que se propone para dicha necesidad y de esta manera minimizar el riesgo y la incertidumbre en el desarrollo, este modelo se encarga del desarrollo de diseños para que estos sean analizados y prescindir de ellos a medida que se adhieran nuevas especificaciones, es ideal para medir el alcance del producto, pero no se asegura su uso real. </p>
<p> Este modelo principalmente se aplica cuando un cliente define un conjunto de objetivos generales para el software a desarrollarse sin delimitar detalladamente los requisitos de entrada procesamiento y salida, es decir cuando el responsable no está seguro de la eficacia de un algoritmo, de la adaptabilidad del sistema o de la manera en que interactúa el hombre y la máquina. </p>
<p> Este modelo se encarga principalmente de ayudar al ingeniero de sistemas y al cliente a entender de mejor manera cuál será el resultado de la construcción cuando los requisitos estén satisfechos.[28]​ </p>
<p> El modelo en espiral, que Barry Boehm propuso originalmente en 1986, es un modelo de proceso de software evolutivo que conjuga la naturaleza iterativa de la construcción de prototipos con los aspectos controlados y sistemáticos del modelo en cascada, es decir, cuando se aplica este modelo, el software se desarrolla en una serie de entregas evolutivas (ciclos o iteraciones), cada una de estas entregando prototipos más completas que el anterior, todo esto en función del análisis de riesgo y las necesidades del cliente. Aunque el modelo espiral representa ventajas por sobre el desarrollo lineal, el cálculo de los riesgos puede ser muy complicado y por lo cual su uso en el ámbito real es muy escaso.[29]​ </p>
<p> Es un modelo en el que el software se muestra al cliente en etapas refinadas sucesivamente. Con esta metodología se desarrollan las capacidades más importantes reduciendo el tiempo necesario para la construcción de un producto; el modelo de entrega por etapas es útil para el desarrollo de la herramienta debido a que su uso se recomienda para problemas que pueden ser tratados descomponiéndolos en problemas más pequeños y se caracteriza principalmente en que las especificaciones no son conocidas en detalle al inicio del proyecto y por tanto se van desarrollando simultáneamente con las diferentes versiones del código. </p>
<p> En este modelo pueden distinguirse las siguientes fases: </p>
<p> Especificación conceptual. </p>
<p> Análisis de requisitos. </p>
<p> Diseño inicial. </p>
<p> Diseño detallado (codificación, depuración, prueba y liberación). </p>
<p> Cuando es por etapas, en el diseño global estas fases pueden repetirse según la cantidad de etapas que sean requeridas. </p>
<p> Entre sus ventajas tenemos: </p>
<p> Detección de problemas antes y no hasta la única entrega final del proyecto. </p>
<p> Eliminación del tiempo en informes debido a que cada versión es un avance. </p>
<p> Estimación de tiempo por versión, evitando errores en la estimación del proyecto general. </p>
<p> Desarrollo iterativo y creciente (o incremental) es un proceso de desarrollo de software, creado en respuesta a las debilidades del modelo tradicional de cascada, es decir, este modelo aplica secuencias lineales como el modelo en cascada, pero de una manera iterativa o escalada según como avance el proceso de desarrollo y con cada una de estas secuencias lineales se producen incrementos (mejoras) del software.[30]​ </p>
<p> Se debe tener en cuenta que el flujo del proceso de cualquier incremento puede incorporar el paradigma de construcción de prototipos, ya que como se mencionó anteriormente, este tipo de modelo es iterativo por naturaleza, sin embargo se diferencia en que este busca la entrega de un producto operacional con cada incremento que se le realice al software. </p>
<p> Este desarrollo incremental es útil principalmente cuando el personal necesario para una implementación completa no está disponible. </p>
<p> Este modelo ―como su nombre lo indica― utiliza las técnicas del diseño estructurado o de la programación estructurada para su desarrollo, también se utiliza en la creación de los algoritmos del programa. Este formato facilita la comprensión de la estructura de datos y su control.[31]​ Entre las principales características de este modelo se encuentran las siguientes: </p>
<p> Generalmente se puede diferenciar de una manera más clara los procesos y las estructuras de datos. </p>
<p> Existen métodos que se enfocan principalmente en ciertos datos. </p>
<p> La abstracción del programa es de un nivel mucho mayor. </p>
<p> Los procesos y estructuras de datos son representados jerárquicamente.[31]​ </p>
<p> Este modelo también presenta sus desventajas entre las cuales podemos mencionar algunas: </p>
<p> Se podía encontrar datos repetidos en diferentes partes del programa.[31]​ </p>
<p> Cuando el código se hace muy extenso o grande su manejo se complica demasiado.[32]​ </p>
<p> En el modelo estructurado las técnicas que comúnmente se utilizan son: </p>
<p> Estos modelos tienen sus raíces en la programación orientada a objetos y como consecuencia de ella gira entorno al concepto de clase, también lo hacen el análisis de requisitos y el diseño. Esto además de introducir nuevas técnicas, también aprovecha las técnicas y conceptos del desarrollo estructurado, como diagramas de estado y transiciones. El modelo orientado a objetos tiene dos características principales, las cuales ha favorecido su expansión: </p>
<p> Permite la reutilización de software en un grado significativo. </p>
<p> Su simplicidad facilita el desarrollo de herramientas informáticas de ayuda al desarrollo, el cual es fácilmente implementada en una notación orientada a objetos llamado UML.[34]​ </p>
<p> Modelo RAD (rapid application development)[editar] </p>
<p> El RAD (rapid application development: ‘desarrollo rápido de aplicaciones’), es un modelo de proceso de software incremental, desarrollado inicialmente por James Maslow en 1980, que resalta principalmente un ciclo corto de desarrollo. </p>
<p> Esta es una metodología que posibilita la construcción de sistemas computacionales que combinen técnicas y utilidades CASE (Computer Aided Software Engineering), la construcción de prototipos centrados en el usuario y el seguimiento lineal y sistemático de objetivos, incrementando la rapidez con la que se producen los sistemas mediante la utilización de un enfoque de desarrollo basado en componentes.[35]​ </p>
<p> Si se entienden bien los requisitos y se limita el ámbito del proyecto, el proceso RAD permite que un equipo de desarrollo cree un producto completamente funcional dentro de un periodo muy limitado de tiempo sin reducir en lo más mínimo la calidad del mismo.[36]​ </p>
<p> El modelo de desarrollo concurrente es un modelo de tipo de red donde todas las personas actúan simultáneamente o al mismo tiempo. Este tipo de modelo se puede representar a manera de esquema como una serie de actividades técnicas importantes, tareas y estados asociados a ellas. </p>
<p> El modelo de proceso concurrente define una serie de acontecimientos que dispararan transiciones de estado a estado para cada una de las actividades de la ingeniería del software. Por ejemplo, durante las primeras etapas del diseño, no se contempla una inconsistencia del modelo de análisis. Esto genera la corrección del modelo de análisis de sucesos, que disparara la actividad de análisis del estado hecho al estado cambios en espera. Este modelo de desarrollo se utiliza a menudo como el paradigma de desarrollo de aplicaciones cliente/servidor. Un sistema cliente/servidor se compone de un conjunto de componentes funcionales. Cuando se aplica a cliente/servidor, el modelo de proceso concurrente define actividades en dos dimensiones: una división de sistemas y una división de componentes. Los aspectos del nivel de sistemas se afrontan mediante dos actividades: diseño y realización. </p>
<p> La concurrencia se logra de dos maneras: </p>
<p> Las actividades del sistema y de componente ocurren simultáneamente y pueden modelarse con el enfoque orientado a objetos descrito anteriormente; </p>
<p> Una aplicación cliente/servidor típica se implementa con muchos componentes, cada uno de los cuales se pueden diseñar y realizar concurrentemente. </p>
<p> En realidad, el modelo de desarrollo concurrente es aplicable a todo tipo de desarrollo de software y proporciona una imagen exacta del estado actual de un proyecto. En vez de confinar actividades de ingeniería de software a una secuencia de sucesos, define una red de actividades, todas las actividades de la red existen simultáneamente con otras. Los sucesos generados dentro de una actividad dada o algún otro lado de la red de actividad inicia las transiciones entre los estados de una actividad. </p>
<p> Proceso unificado del desarrollo de software[editar] </p>
<p> El proceso unificado es un proceso de software genérico que puede ser utilizado para una gran cantidad de tipos de sistemas de software, para diferentes áreas de aplicación, diferentes tipos de organizaciones, diferentes niveles de competencia y diferentes tamaños de proyectos. </p>
<p> Provee un enfoque disciplinado en la asignación de tareas y responsabilidades dentro de una organización de desarrollo. Su meta es asegurar la producción de software de muy alta calidad que satisfaga las necesidades de los usuarios finales, dentro de un calendario y presupuesto predecible.[37]​ </p>
<p> El proceso unificado tiene dos dimensiones: </p>
<p> Un eje horizontal que representa el tiempo y muestra los aspectos del ciclo de vida del proceso a lo largo de su desenvolvimiento </p>
<p> Un eje vertical que representa las disciplinas, las cuales agrupan actividades de una manera lógica de acuerdo a su naturaleza. </p>
<p> La primera dimensión representa el aspecto dinámico del proceso conforme se va desarrollando, se expresa en términos de fases, iteraciones e hitos (milestones). </p>
<p> La segunda dimensión representa el aspecto estático del proceso: cómo es descrito en términos de componentes del proceso, disciplinas, actividades, flujos de trabajo, artefactos y roles. </p>
<p> El refinamiento más conocido y documentado del proceso unificado es el RUP (proceso unificado racional). </p>
<p> El proceso unificado no es simplemente un proceso, sino un marco de trabajo extensible que puede ser adaptado a organizaciones o proyectos específicos. De la misma manera, el proceso unificado de rational, también es un marco de trabajo extensible, por lo que muchas veces resulta imposible decir si un refinamiento particular del proceso ha sido derivado del proceso unificado o del RUP. Por dicho motivo, los dos nombres suelen utilizarse para referirse a un mismo concepto.[38]​ </p>
<p> El software se ha convertido en algo muy necesario en nuestra sociedad actual, es la máquina que conduce a la toma de decisiones comerciales, sirve para la investigación científica moderna, es un factor clave que diferencia productos y servicios modernos. Esto se da porque el software está inmerso en sistemas de todo tipo alrededor de nosotros. </p>
<p> El software de computadora es el producto que diseñan y construyen los ingenieros de software. Esto abarca programas que se ejecutan dentro de una computadora de cualquier tamaño y arquitectura, después de estar construido casi cualquier persona en el mundo industrializado, ya sea directa o indirectamente. </p>
<p> Los productos se pueden clasificar en: </p>
<p> Productos genéricos: Son los producidos por una organización para ser vendidos al mercado. </p>
<p> Productos hechos a medida: Sistemas que son desarrollados bajo pedido a un desarrollador específico. </p>
<p> Estos productos deben cumplir varias características al ser entregados, estas son: </p>
<p> Mantenibles: El software debe poder evolucionar mientras cumple con sus funciones. </p>
<p> Confiabilidad: No debe producir daños en caso de errores. </p>
<p> Eficiencia: El software no debe desperdiciar los recursos. </p>
<p> Utilización adecuada: Debe contar con una interfaz de usuario adecuada y su documentación. </p>
<p> Lo que constituye el producto final es diferente para el ingeniero y los usuarios, para el ingeniero son los programas, datos y documentos que configuran el software pero para el usuario el producto final es la información que de cierto modo soluciona el problema planteado por el usuario. </p>
<p> La ingeniería de software es una disciplina que está orientada a aplicar conceptos y métodos de ingeniería al desarrollo de software de calidad. </p>
<p> Los programas tienen muchas propiedades matemáticas. Por ejemplo la corrección y la complejidad de muchos algoritmos son conceptos matemáticos que pueden ser rigurosamente probados. El uso de matemáticas en la IS es llamado métodos formales. </p>
<p> Los programas son construidos en una secuencia de pasos. El hecho de definir propiamente y llevar a cabo estos pasos, como en una línea de ensamblaje, es necesario para mejorar la productividad de los desarrolladores y la calidad final de los programas. Este punto de vista inspira los diferentes procesos y metodologías que se encuentran en la IS. </p>
<p> El desarrollo de software de gran porte requiere una adecuada gestión del proyecto. Hay presupuestos, establecimiento de tiempos de entrega, un equipo de profesionales que liderar. Recursos (espacio de oficina, insumos, equipamiento) por adquirir. Para su administración se debe tener una clara visión y capacitación en gestión de proyectos. </p>
<p> Para el desarrollo de un sistema de software es necesaria la colaboración de muchas personas con diversas competencias, capacidades e intereses. Al conjunto de personas involucradas en el proyecto se les conoce como participantes. </p>
<p> Al conjunto de funciones y responsabilidades que hay dentro del proyecto o sistema se le conoce como roles o papeles. Los roles están asociados a las tareas que son asignadas a los participantes, en consecuencia, una persona puede desempeñar uno o múltiples roles, así también un mismo rol puede ser representado por un equipo.[39]​ </p>
<p> Es frecuente el uso de los términos "usuarios", "usuarios finales" y "clientes" como sinónimos, lo cual puede provocar confusión; estrictamente, el cliente (persona, empresa u organización) es quién especifica los requisitos del sistema,[40]​ en tanto que el usuario es quien utiliza u opera finalmente el producto software, pudiendo ser o no el cliente. </p>
<p> Esta clase de participantes están relacionados con todas las facetas del proceso de desarrollo del software. Su trabajo incluye la investigación, diseño, implementación, pruebas y depuración del software.[41]​ </p>
<p> En el contexto de ingeniería de software, el gestor de desarrollo de software es un participante, que reporta al director ejecutivo de la empresa que presta el servicio de desarrollo. Es responsable del manejo y coordinación de los recursos y procesos para la correcta entrega de productos de software, mientras participa en la definición de la estrategia para el equipo de desarrolladores, dando iniciativas que promuevan la visión de la empresa.[42]​ </p>
<p> El usuario final es quien interactúa con el producto de software una vez es entregado.[40]​ Generalmente son los usuarios los que conocen el problema, ya que día a día operan los sistemas. </p>
<p> Un ingeniero de software debe tener un código donde asegura, en la medida posible, que los esfuerzos realizados se utilizarán para realizar el bien y deben comprometerse para que la ingeniería de software sea una profesión benéfica y respetada. Para el cumplimiento de esta norma, se toman en cuenta ocho principios relacionados con la conducta y las decisiones tomadas por el ingeniero; donde estos principios identifican las relaciones éticamente responsables de los individuos, grupos y organizaciones donde participen. Los principios a los que deben sujetarse son sobre la sociedad, cliente y empresario, producto, juicio, administración, profesión, colegas y por último el personal. </p>
<p> Sociedad: Los ingenieros de software deben actuar de manera congruente con el interés social, aceptando la responsabilidad total de su trabajo, moderando los intereses con el bienestar social, aprobando el software solamente si se tiene una creencia bien fundamentada, cooperando en los esfuerzos para solucionar asuntos importantes de interés social, ser justo y veraz en todas las afirmaciones relativas al software o documentos asociados. </p>
<p> Cliente y empresario: Se debe actuar de manera tal que se llegue a conciliar los mejores intereses de los clientes y empresarios, congruentemente con el interés social. Estos deberán prestar servicios en sus áreas de competencia, siendo honestos y francos sobre las limitaciones, no utilizar un software que se obtenga ilegalmente o sin ética, usar la propiedad de los clientes o empresarios de manera autorizada, mantener secreto cualquier documento de información confidencial. </p>
<p> Producto: Hay que asegurarse que los productos y sus modificaciones cumplan con los estándares profesionales más altos posibles, procurando la alta calidad, costos aceptables y una agenda razonable asegurando que los costos y beneficios sean claros y aceptados por el empresario y el cliente. Asegurar que las metas y objetivos de cualquier proyecto sean adecuados y alcanzables. </p>
<p> Juicio: Se debe mantener una integridad e independencia en el juicio profesional, moderando todo juicio técnico por la necesidad de apoyar y mantener los valores humanos, mantener la objetividad profesional con respecto a cualquier software o documento relacionado, no involucrarse en prácticas financieras fraudulentas. </p>
<p> Administración: Se deberá asegurar una buena administración para cualquier proyecto en el cual se trabaje, utilizando procedimientos efectivos para promover la calidad y reducir riesgos, asegurándose también que se conozcan las políticas y procedimientos del empresario para proteger contraseñas, archivos e información confidencial. </p>
<p> Profesión: Se debe incrementar la integridad y reputación de la profesión en conjunto con el interés social, ayudando al desarrollo de un ambiente organizacional favorable para actuar, promoviendo el conocimiento público de la ingeniería de software, extendiendo el conocimiento de la ingeniería de software por medio de participaciones en organizaciones, reuniones y publicaciones profesionales. </p>
<p> Colegas: Cada ingeniero deberá apoyar y ser justos con los colegas, motivando a sus colegas sujetándose al código, ayudando también a su desarrollo profesional, reconocer los trabajos de otros y abstenerse a atribuirse de méritos indebidos, revisar los trabajos de manera objetiva, sincera y propiamente documentada. </p>
<p> Personal: Los ingenieros de software participaran toda su vida en el aprendizaje con la práctica y promoverán un enfoque ético de la profesión, mejorando su conocimiento de los avances en el análisis, especificación, diseño, desarrollo, mantenimiento, pruebas del software y documentos relacionados en conjunto con administración del proceso de desarrollo.[43]​ </p>
<p> ↑ Error en la cita: Etiqueta no válida; no se ha definido el contenido de las referencias llamadas </p>
<p> ↑ Pressman, Roger S.: Ingeniería del software: un enfoque práctico. Sexta edición, pág. 50-51. </p>
<p> ↑ Lawrence Peleeger, Shari: Ingeniería de software: modelo de prototipos. Universidad Estatal de Milagro. </p>
<p> ↑ Pressman, Roger S.: Ingeniería del software: un enfoque práctico. Sexta edición, pág. 58-60. </p>
<p> ↑ Pressman, Roger S.: Ingeniería del software: un enfoque práctico. Sexta edición, pág. 52-53. </p>
<p> ↑ [3], cuadro comparativo de programación estructurada y programación orientada objeto . </p>
<p> ↑ [4], Benet Campderrich Falgueras, Editorial UOC, 2002 - 320 páginas. </p>
<p> ↑ Campderrich Falgueras, Benet (2002): Ingeniería de software. Barcelona: Editorial UOC, 2002. 320 páginas. </p>
<p> Programador </p>
<p> Dos programadores trabajando en un IBM 704 en NACA , 1954. </p>
<p> Un programador es aquella persona que elabora programas de computadora.[1]​, es decir escribe, depura y mantiene el código fuente de un programa informático, que ejecuta el hardware de una computadora, para realizar una tarea determinada. </p>
<p> Los programadores también son denominados desarrolladores de software, aunque estrictamente forman parte de un equipo de personas de distintas especialidades (mayormente informáticas), y siendo que el equipo es propiamente el desarrollador. </p>
<p> En muchos países, un programador es también una categoría profesional reconocida. </p>
<p> Inicialmente, la profesión se formalizó desde el enfoque tayloriano de la especialización de funciones en la empresa. Así, el proceso de producción de software se concibe como un conjunto de tareas altamente especializadas donde está claramente definido el papel de cada categoría profesional: </p>
<p> El analista, tiene como cometido analizar un problema y describirlo con el propósito de ser solucionado mediante un sistema de información. </p>
<p> El programador, cuya única función consistía en trasladar las especificaciones del analista en código ejecutable para la computadora. Dichas especificaciones se recogen en un documento denominado cuaderno de carga, medio de comunicación entre ambos. </p>
<p> Hoy día se reconoce que este enfoque no es válido para organizar tareas de tipo intelectual, como es el desarrollo de software. De manera que la profesión de programador ha ido evolucionando. Las dificultades de comunicación entre analistas y programadores (un mero documento no basta para describir lo que se quiere hacer) dio origen a una categoría de profesional intermedia, denominada analista-programador. La concepción original del programador ha desaparecido siendo sustituida por la de un profesional mucho más formado y con unas funciones menos «mecánicas». </p>
<p> La profesión de analista también ha evolucionado, surgiendo el concepto diseñador (de software). Esto se debe a los avances de la ingeniería del software donde se reconoce que el análisis es una actividad compleja y distinta del diseño. Escuetamente, el análisis describe el problema (es decir, «qué» hacer) mientras que el diseño describe la solución («cómo» hacerlo). </p>
<p> En la mayoría de países industrializados esto ha dado lugar a la categoría diseñador o arquitecto del software. </p>
<p> Estrictamente hablando, la profesión de programador si conoce especialidades. No obstante, existen diversas ramas por las que se decantan los propios profesionales y que se ven reflejadas en la oferta de empleo. Así, es posible mencionar algunas: </p>
<p> Programadores de mainframe: aunque se cree extinta la actividad en los viejos grandes sistemas informáticos, lo cierto es que aún existen muchos en funcionamiento que requieren mantenimiento. La tecnología que manejan estos programadores es radicalmente distinta a la del resto, motivo por el que se puede considerar esta como la rama más especializada. Entre sus conocimientos se cuenta COBOL, RPG, JCL, base de datos jerárquicas, etc. </p>
<p> Programadores de "nuevas tecnologías": esta es una rama que gira en torno a Internet, los nuevos servicios como la Web 2.0 y los negocios por medios electrónicos o e-commerce. Entre sus conocimientos destacan lenguajes del lado del servidor como Java, ASP, .NET, JSP, PHP, Ruby, Python o Perl, y lenguajes del lado de cliente como HTML, XHTML, CSS, Javascript o AJAX (conjunto de tecnologías existentes como XML y Javascript). </p>
