job_title,company,location,job_description
"Information Technology - Associate Data Engineer, Data Services",Aritzia,"- Vancouver, BC","THE DEPARTMENT
The mission of the Data Services department is to enable a data-driven organization through a governed, scalable and agile data foundation.
THE OPPORTUNITY
As a member of the Data Services Department, you will be part of the team responsible for building and operating a modern, scalable platform for reporting, analytics and data science. As the Associate Data Engineer, you will support in and gain an understanding of how to ensure the data platform runs smoothly to help maximize data integrity, performance and ease of access to data. And, with the skills you gain in this role, the opportunities are endless – from a rewarding career in Data Services, to continued growth and development with Aritzia.

THE JOB
As the Associate Data Engineer, you will:
Develop a consistent, intuitive and scalable platform that supports data requirements for today and tomorrow.
Build and maintain resilient data pipelines that transform unorganized data into modeled and accessible information; support the end-to-end process from data acquisition through the distribution of data products for use in reporting and analytics, including ensuring successful completion of scheduled jobs and reports.
Develop robust reporting and dashboards that serve the business with insights for decision making; investigate issues and recommend enhancements to existing reports.
Ensure access to consistent and reliable data while balancing the need for security and privacy; regularly monitoring key quality and compliance metrics, and ensuring up-to-date metadata and documentation is available for end users.
QUALIFICATIONS
The Associate Data Engineer has:
A commitment to learn and apply Aritzia's Business and People Leadership principles
An ability to take on new opportunities and challenges, with a passion to continue to develop and grow oneself
A dedication to operational efficiency, investing in process and systems to improve team and business results
The intellectual and emotional intelligence to tactically partner cross-functionally in the pursuit of shared business outcomes
The skills to set clear objectives and hold oneself accountable to reach your full capability and maximize your capacity
Proven skills, education, applicable certifications, or the desire to learn SQL, Python, data visualization tools (e.g., Tableau, Looker, Power BI, Crystal Reports), GitHub, cloud platforms (e.g., Google Cloud, AWS, Azure), and the appetite to continuously learn and develop oneself
A commitment to quality and investing in results that add value and drive the business
A sense of urgency executing job responsibilities, prioritizing urgent and important work
An ability to identify, evaluate, and suggest business opportunities for today and tomorrow, considering top line and bottom line impacts
ARITZIA
Head to our About Us for the scoop on who we are and what we do.
Aritzia is an Equal Opportunity employer. Aritzia believes in providing an inclusive workplace where all individuals have opportunity to succeed.
We are committed to doing so by providing accessible employment practices. Requests for accommodation due to a disability can be made at any stage of the recruitment process and applicants are asked to make their accommodation needs known ."
Data Engineer (New Grad),Jarvis,"- Toronto, ON","Jarvis Consulting Group identifies high potential individuals and develops them into professional technology consultants working with the hottest technologies in some of Canada’s top companies.
The Opportunity
We’re looking for individuals with the right attitude and aptitude to become Data Engineers and work with some of Canada’s top companies to unlock the value of their data. If you want to become a data engineer working with top companies on high profile projects Jarvis just might be what you're looking for.

You’ll be involved in projects of high strategic importance to design, build and integrate data pipelines that provide a consistent flow of high quality data. Also, you’ll have the opportunity to explore the use of new and emerging big data related technologies and pioneer new patterns and practices.
What we offer
Comprehensive training on core BigData technologies provided by industry experts:
Cloud
DevOps
Java
Hadoop/Spark
Access to the most exciting and innovative projects within some of Canada’s top companies
Dedicated support to help you develop your career through coaching, professional networking opportunities and attaining industry recognized certifications
The opportunity to dramatically improve your skills by writing a lot of code
Who we're looking for
Curious - You’re interested in experimenting, learning, innovating and trying new things
Customer Focused – You strive to create value for your customers and always deliver with quality
Adaptable - You maintain a focus on results even as plans and priorities change and consistently deliver value
Humble - You recognize your strengths as well as your opportunities and are always willing to learn from others
Collaborative - You value the success of the group and freely share your knowledge, experience and insight
What we’re looking for
Experience and education
Diploma or degree in computer science, technology, engineering or a mathematical discipline
Aptitude for logical reasoning and quantitative problem solving
Good oral and written communication skills
Good collaboration and teamwork skills
Willing and able to commit to working with Jarvis for at least two years following a training period
Legally permitted to work in Canada (citizen/permanent resident/work permit)
Core Technical Skills
Familiar with Java or another object oriented programming language ( C++, Python, Ruby, etc)
Have completed at least one course related to designing algorithms and data structures
Why you should work with us
The ability to have your potential recognized and rewarded
Practical and relevant training utilizing the newest technologies
Support to help you establish your career and achieve your goals
The opportunity to work on innovative projects within a variety of Canada’s top companies
A competitive compensation package with support for continuous learning"
Sr. Data Engineer,TELUS,"- Vancouver, BC","Basic Information
Ref number
Req_00032679
Primary Location
TELUS Garden Vancouver
Description and Requirements


Your role:

As an active member of the TELUS International, you will be in the heart of a major TELUS transformation program supporting it as a Sr. Data Engineer

TELUS is looking for a qualified professional to fill a Sr. Data Engineer in our Digital Solutions team. This professional is responsible for building and maintaining optimized and highly available data pipelines that facilitate deeper analysis and reporting by the Data and Analytics department. The Senior Data Engineer builds data processing frameworks that handle the business's growing database.

Qualifications:
Graduate degree in Engineering or Computer Science
4+ years’ performing data engineering and analysis
Expert knowledge of RDBMS and SQL-related tools
3+ years’ architecting big data solutions employing tools such as Kafka, Hadoop, Spark, Hive, Pig, SolR
Knowledge of security mechanisms for distributed compute environment e.g., Kerberos, Federated Identity, LDAP/AD
Experience with data analysis tools such as SAS, Jupyter/Python, R, Tableau, PowerBI
Experience with containerization strategies e.g., Docker, Kubernetes, Mesos, Yarn - either on-premises or in cloud environments

Desired Qualifications:

Experience with ETL tools (e.g., Informatica, Talend, Pentaho)
Cloud Dataflow (good to have)
Experience with Deep Learning frameworks such as TensorFlow, MXNet, PyTorch
Experience with architecture and data governance including privacy and metadata management

Who is TELUS?

We're a high-performing team of individuals who collectively make TELUS one of the leading telecommunications companies in Canada. Our competitive consumer offerings include wireline, wireless, internet, and Optik TV™. We also deliver a compelling range of products and services for small, medium and large businesses; and have carved out a leadership position in the health, energy, finance and public sector markets with innovative industry-specific solutions.

Everyone belongs to TELUS. It doesn’t matter who you are, what you do, or how you do it, at TELUS, your unique contribution and talents will be valued and respected. Because of the more diverse perspectives, we have the more likely we are to crack the code on what our customers want and our communities' needs.

Additional Job Description
TELUS is looking for a qualified professional to fill a Sr. Data Engineer in our Digital Solutions team. This professional is responsible for building and maintaining optimized and highly available data pipelines that facilitate deeper analysis and reporting by the Data and Analytics department. The Senior Data Engineer builds data processing frameworks that handle the business's growing database."
Data Engineer,MobSquad,"- Vancouver, BC","ABOUT MOBSQUAD
We are a well-funded, hyper-growth, scale-up looking for an experienced Data Engineer. If you've ever dreamed of working with a top tier technology company scale-up, on leading edge technologies, backed by the very best venture capitalists in the world, then this is your chance.
Some details about MobSquad:
MobSquad solves the significant and growing technology talent shortage faced by US-based start-ups and scale-ups by enabling our clients to quickly have a turnkey ""virtual"" Canadian subsidiary, where Canadian-based software engineers serve our clients individually on an exclusive basis
We've been featured on the front page of The Washington Post, on NPR multiple times, The Financial Times (UK), The Globe and Mail, the Calgary Herald, BetaKit, CBC, Global News, and many other places. other media outlets
We're a Certified B Corporation, were recognized as the third Best Place to Work in Canada in 2020, and have made numerous contributions to charitable organizations as well as a financial commitment to the Upside Foundation. We believe we are playing a key role in enhancing Canada's innovation economy, and have received financial support from the Government of Canada, Province of Alberta, Province of Nova Scotia, and City of Calgary, to support this ambition
You can learn more about us on our website
ABOUT THE ROLE
As a Data Engineer, you will be part of a Canada-based team working remotely for a leading US scale-up. Your team will operate alongside many other talented developers and data scientists in Canada, and you will be an integral part of the tech community that MobSquad has built.
This role requires someone who has demonstrated an ability to develop, test, optimize, and maintain scalable databases, architectures, and pipelines that enable data scientists and software developers to easily analyze and work with data. The ideal candidate has worked closely with data scientists and data architects and is an expert at optimizing data flow for use across broader teams. The candidate should be able to generate ideas and create tools that add greater functionality and usability to data systems within the company.
ABOUT YOU
You have a bachelor's degree in Computer Science, Information Technology, Data Science, Applied Math, Physics, Engineering, or a comparable analytical field from an accredited institution
You are expert in modeling, working with database architectures, and relational databases
You have over three years of experience with big data tools, such as Hadoop (MapReduce, Hive, Pig), Spark, and Kafka
You have experience with SQL databases (PostgreSQL, MySQL) and NoSQL databases (Cassandra, MongoDB)
You have experience creating and working with ETL data transformation and integration processes
You have experience working with enterprise-grade cloud computing platforms such as Microsoft Azure, Amazon Web Services (EC2, EMR, RDS, Redshift), and Google Cloud
You have expertise in relevant programming languages (Python, R, C/C++, Java, Pearl, Scala)
You have a deep understanding of data modeling tools (ERWin, Enterprise Architect, Visio)
You have experience optimizing big data pipelines and extracting value from large disconnected datasets
You have the ability to develop high-quality code adhering to industry best practices (i.e., code review, unit tests, revision control)
WHAT YOU'LL GET @MOBSQUAD
A full-time position that offers competitive compensation
A benefits program delivered through our bespoke digital platform, giving you control, choice, and flexibility. We give you the ability to build your package of benefits covering health (e.g., medical, dental, vision), wellness (e.g., gym, workout gear, massage, transit), and RRSP (retirement savings)
A downtown office location with first-rate amenities, surrounded by great restaurants and easily-accessible transit
For international candidates, sponsorship for an immediate work permit, expedited permanent residency, and Canadian citizenship within four years
At MobSquad, we support and encourage building a work environment that is diverse, inclusive, and safe for all. We invite and welcome applicants of all backgrounds, regardless of race, religion, sexual orientation, gender identity, national origin, or disability."
"Big Data Engineer, Omnia AI Vancouver",Deloitte,"- Vancouver, BC","Job Type: Permanent
Primary Location: Vancouver, British Columbia, Canada
All Available Locations: Vancouver

Learn from deep subject matter experts through mentoring and on the job coaching.
Partner with clients to solve their most complex problems.
Be empowered to lead and have impact with clients, our communities and in the office.

“At Deloitte we value the opportunity to network and build relationships with skilled individuals, even in periods where we are not actively hiring. If you would like to apply to this future opportunity role, and have the required qualifications, you can expect to be contacted by the recruitment team within a few days""
You love to wrestle down data puzzles, you embrace the potential that data represents, you aspire to solve data problems no one else can, and above all, you want to use data to make impacts that matter – if that is you, then Omnia AI is where you want to be.
What will your typical day look like?

As a Big Data Engineer on our Data & Analytics Modernization team within the Omnia AI practice, you are passionate about data and technology solutions, are driven to learn about them and keep up with market evolution. You will play an active role throughout the entire engagement cycle, specializing in modern data solutions including data ingestion frameworks, data pipeline development, Hadoop-based data lake architectures and orchestration. You are enthusiastic about all things data, have strong problem-solving and analytical skills, are tech savvy and have a solid understanding of software development.
Specifically, in this role, you will:
Engineer Big Data ingestion and pipeline frameworks to populate on-premise or cloud-based data lakes
Translate business rules and requirements into data objects, produce associated data models and source to target mappings and write abstracted, reusable code components accordingly
Plan/schedule tasks, lead small development teams, and mentor junior colleagues
Facilitate technical meetings with client staff, and advise client with technical option analyses based on leading practices
About the team

Omnia AI, Deloitte’s Artificial Intelligence (AI) practice is comprised of a collaborative team of experts who use their hands-on experience with cutting-edge information assets to facilitate successful AI transformations. We develop AI-enabled solutions to address all aspects of a client’s transformative journey with disciplined focus on business outcomes.
Our Data & Analytics Modernization team helps clients design and implement the data platform architectures – be it in the cloud or on-premise – required to enable cutting-edge AI solutions. You will be part of a practice to deliver a breadth of solutions to solve our clients most challenging business problems, with a focus on Big Data, BI/DW, Data Integration, Data Governance, Master Data and Analytics applications. Each of these applications leverages a different mix of traditional and innovative technologies to achieve business outcomes.
Enough about us, let’s talk about you

You are someone with:
3+ years implementation experience leveraging Hadoop ecosystem technologies such as HDFS, MapReduce, Pig, Sqoop, Spark, Hive, Kafka, etc. on-premise and/or in the cloud (e.g. AWS, Azure, GCP)
3+ years experience with analysis, design, development, testing and deployment of data pipeline (ETL) services leveraging the Big Data technology stack for batch and/or real-time messaging/streaming environments
Experience writing complex SQL queries, extracting and importing disparate data from source systems, and data manipulation based on requirements
Experience with Agile development methods in data-oriented projects
Completed Bachelor’s Degree (or higher) in quantitative areas such as Computer Science, Information Management, Big Data & Analytics, or related field is desired
If you believe you have what it takes to be a successful member of our team, please apply now. We know your career is important to you and it's important to us, too. This role is just the first step of a highly successful career we can help you build.
The time is right for you to join Deloitte. Get your career off to great start. What impact will you make?
Why Deloitte?
Launch your career with The One Firm where you can make an impact that matters in a way that you never thought possible. With endless opportunities at every turn, and a culture built to support and develop our people to be the very best they can be, Deloitte is The One Firm for you to learn, grow, create, connect, and lead. We do this by making three commitments to you:
You will lead at every level: We grow the world’s best leaders so you can achieve the impact you seek, faster.
You can work your way: We give you the means to be flexible in how you need and want to work, and we have innovative spaces, arrangements and the mindset to help you be wildly successful.
You will feel included and inspired: We create a deep sense of belonging where you can bring your whole self to work.

The next step is yours
Sound like The One Firm. For You?
At Deloitte we are all about doing business inclusively – that starts with having diverse colleagues of all abilities! We encourage you to connect with us at accessiblecareers@deloitte.ca if you require an accommodation in the recruitment process, or need this job posting in an alternative format. We’d love to hear from you!
By applying to this job you will be assessed against the Deloitte Global Talent Standards. We’ve designed these standards to provide our clients with a consistent and exceptional Deloitte experience globally."
Data Engineer,AIR MILES,"- Toronto, ON","About the Opportunity:
As part of the Data Hub team at AIR MILES, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flows and collection for cross functional teams. The pipeline needs to be scalable, repeatable, and secure. You will work with some of the largest and most varied data sets (both batch and real-time) in Canada. You will expand and develop the AIR MILES Cloud analytical platform that enables business users, data analysts and data scientists to make data driven decisions, build innovative data products and roll out advanced analytics.

What will you bring?
Ability and desire to work in our collaborative environment: open team room, pair programming and fluid interactions with all products and operations teams.
Focusing on building solutions utilizing an agile approach: close relationships with Product Managers, communicating and digesting real time feedback, and working smartly to build story cards on daily basis.
Passionate about Big Data and the latest trends and developments. We strongly believe in and encourage continuous learning.
You are self-driven, need minimal supervision and comfortable pushing your own projects and getting things done.
Experience with Python, Spark, and SQL
Experience building ‘big data’ pipelines, architectures, and datasets
Experience with Amazon AWS and other cloud platforms
Experience with Databricks
Experience with Agile methodologies as well as familiar with CI/CD tools (Jenkins, Travis, github)
Experience in ETL and Data Modeling preferred
Experience in designing and implementing streaming applications is preferred
Fully understand standard architecture methodologies, processes and best practices

About AIR MILES
Today, there are more ways than ever to engage shoppers. At AIR MILES, we believe that understanding the people behind the purchase is key to winning their hearts – and their wallets. For over two decades and from more than fifty locations around the globe, we have paired expertise in shopper behavior with advanced analytics to uncover the data-driven insights that drive successful loyalty, marketing and merchandising solutions. At AIR MILES, we know that in coming together we are at our strongest – and that together we can help shape the future for our clients, their shoppers and our communities. AIR MILES is an Alliance Data company. For more information, visit www.loyalty.com
About ADS
Alliance Data® (NYSE: ADS) is a leading global provider of data-driven marketing and loyalty solutions serving large, consumer-based industries. The Company creates and deploys customized solutions, enhancing the critical customer marketing experience; the result is measurably changing consumer behavior while driving business growth and profitability for some of today's most recognizable brands. Alliance Data helps its clients create and increase customer loyalty through solutions that engage millions of customers each day across multiple touch points using traditional, digital, mobile and emerging technologies. An S&P 500 and Fortune 500 company headquartered in Plano, Texas, Alliance Data consists of three businesses that together employ more than 16,000 associates at approximately 100 locations worldwide. For more information, visit www.alliancedata.com
Alliance Data is an Equal Employment Opportunity employer. Accordingly, we will make reasonable accommodations to respond to the needs of people with disabilities in accordance with legislation.
Alliance Data participates in E-Verify.
Check us out – AIR MILES on Stack Overflow | LinkedIn | Glassdoor | Facebook |Twitter | Blog | Instagram
Company: AIR MILES

Primary Location: CAN - Canada-2084 - Ontario-50419 - Toronto-40-LOY - Toronto
Work Locations: 40-LOY - Toronto Toronto M5A 0L6
Job: Information Systems
Organization: LoyaltyOne
Schedule: Temporary
Job Type: Full-time
Job Posting: Jul 3, 2020, 11:00:19 AM
Division: Loyalty"
Data Engineer,Alberta Motor Association,"- Edmonton, AB","AMA:
IT’S all ABOUT THE DATA…
Posted: 2020-06-09
Closing: When Filled
Location: Edmonton Kingsway

You’re definitely a “Techie” who’s passionate about finding the right solutions and ensuring a smooth-running system environment. You are not afraid to pave the way for bigger and better things. As a Data Engineer for our AMA Insurance Information Technology department, you’ll be responsible for the collecting, sorting, processing, analyzing of huge sets of data, and creating the visualizations. We are looking for a someone whose primary focus will be on finding creative optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them.

WHAT MOVES YOU
You have good analytical and problem-solving skills for technology and business.
You’re an excellent communicator and an even better listener — it’s probably why you’re also a great negotiator.
You’re not only motivated to go above and beyond in your own role, you know how to motivate others and get everyone on board.
You love being able to share your knowledge and experience to help others grow and excel in their careers.
Your superior organizational skills leave no room for clutter — in your workspace or in your brain.
You’re the ultimate multi-tasker, able to juggle multiple projects and requests with ease in a deadline-oriented environment – and you make it look easy.
You thrive in a collaborative environment, where change is the name of the game.
You’re comfortable working directly with all levels of management to accomplish common goals.

WHAT YOU’LL DO
Report to the Manager, Data Engineering (Insurance).
Recommend practices in the design & implementation of solutions which enhance our customers experience.
Design & develop data framework to support customer events (Data Modelling, ETL, Data Cleansing, Data Enrichment).
Investigate new products, tools, and data technologies (Big Data, Data Mining, Deep Learning) to add value to the business; ensure applications are on the right platform and that we’re in a sustainable position moving forward.
Collaborate with business units and IT to define and document: data reporting and business analytics requirements, cubes, data dictionaries, data models and dashboards.
Ongoing development and enhancements of data integrity processes, data quality assurances, reporting, analysis, and formats.
Synthesize user requirements keeping in mind data, business process and system restrictions and assist in translating requirements.
Converting data into relevant consumable information to help business in making informed decisions through data mining, statistical and predictive modeling.
Provide operational support for existing data warehouse environment.
Use best practices to write well designed, testable, and efficient code.
Identify technical debt/operational improvements & implement solutions.
Meet with business partners to understand & gather requirement for strategic initiatives.
Work in a collaborative space with a team of people.

WHAT YOU’VE DONE
You have a technical diploma related to Computing Science, Statistics, Business or have an equivalent combination of education and experience.
You have a minimum of 3 years’ progressive experience in data analysis and/or business analysis.
You are familiar with Design & Engineering of Data Warehouses (Data Modelling, ETL, Data Cleansing, Data Enrichment).
Experience with Data Mining & Deep Learning using algorithms.
Experience with Big Data technologies such as Hadoop, Spark, Data Bricks, Azure Data Lake, Kafka, etc. would be an asset.
Experience with Business Intelligence technologies such as Power BI, Excel Pivot Tables, Cubes, SSRS, etc.
Proven track record with Agile/Scrum Methodologies.
Practical experience with software development and data engineering practices.
Skilled with test driven development (TDD), continuous integration and deployment.
Passion for using network, and internet security including encryption and authentication mechanisms.

WHAT YOU’LL GET
Competitive salary.
Flexible benefits.
Outstanding employer-paid Pension Plan.
Great AMA discounts.
Unlimited learning opportunities.
Growth opportunity as a leader
Paid vacation and floater day.

We thank all applicants for their interest; however, only those selected for an interview will be contacted."
Data Engineer,Softvision North America,"- Vancouver, BC","WE ARE A TRANSFORMATIONAL PARTNER
We marry design and engineering language in ways that produce impactful and memorable experience journeys. We partner all the way to continuously improve our clients’ digital maturity. Our Studio network brings the optimal combination of skill, scale, and cost for each stage of the product development lifecycle. And to do this we need great transformational people that want to impact the projects and organizations that they work with.

We are looking for an exceptional Data Engineer to join our team in Vancouver. This role will be focused on making sure our work meets the highest level of quality. You will work closely with a talented team and are not afraid to iterate with strategy and are open to constructive criticism. You will help people to uncover new insights and fine-tune operations to meet business goals. Here is a bit more about how this position impacts our organization and the experience/skills to do it:

What We Do

As a services company we get to work a wide range of projects from low level IoT projects and blockchain to apps, websites and backend services for household names (Reddit, Lululemon, Shell Oil, Starbucks, just to name a few).
Work with team members all across the world and closely collaborate to ensure the things we work on are successful.
Create amazing user experiences for our clients and for ourselves.
What We Provide

Open culture, opportunity to work on exciting technology and learn new things.
Cozy office in downtown Vancouver with snacks, drinks and a communal spirit.
Extended health benefits, personal development budget and flexible work schedule.
What We Expect of You

Apply broad knowledge of technology options, technology platforms, design techniques and approaches across the Data Engineering ecosystem to build systems that meet business needs
Build systems and datasets using software engineering best practices, data management fundamentals, data storage principles, recent advances in distributed systems, and operational excellence best practices
Analyze source systems, define underlying data sources and transformation requirements, design suitable data models and document the design/specifications
Demonstrate passion for quality and productivity by use of efficient development techniques, standards and guidelines
Effectively communicate with various teams and stakeholders, escalate technical and managerial issues at the right time and resolve conflicts
Peer review work. Actively mentor other members of the team, improving their skills, their knowledge of our systems and their ability to get things done
We’re looking for someone who has:
Expertise with Database technologies such as AWS Redshift, Oracle, Teradata, or others
Expertise with Visualization platforms such as MicroStrategy and AWS Quicksight
Ability to communicate effectively and work independently with little supervision to deliver on time quality products
Expertise in SQL and programming languages such as python
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision
3-5 years of hands on experience in working with data including but not limited to Data Warehouse (DWH) environment with data integration/ETL of large and complex data sets, Data modelling skills such as Star/Snowflake schema design for DWH, or building large scale data-processing systems with experience in Big Data technologies such as MapReduce, Hadoop, Spark, Kafka or AWS equivalents.
A Bachelor’s degree or higher in Computer Science, Engineering, Mathematics, Physics, or a related field
We are a thriving Community of top technology talent that is globally connected. We Engage, Make, Run and Evolve the technology that makes many brands that you know and love. So let’s take this journey together. No matter where you are on your digital career roadmap, we can help you grow and have fun doing it.

Cognizant Softvision is an Equal Opportunity Employer. No 3rd Party Agency Candidates."
Data Engineer,RBC,"- Toronto, ON","What is the opportunity?

The Data engineering team in I&TS will play a critical role to help with our transformation strategy. The team will centralize data sources, builds automated data pipelines, develop custom APIs and assist with building the analytical models. We are looking for an experienced data engineer who is quick to adapt to next generation technologies and enthusiastic about analyzing and automating data and building solid products and solutions

What will you do?

As part of the I&TS Data Engineering/Governance team you will support the implementation of the Data Governance strategy across multiple data sources across various geographies. This role will encompass an end to end view from data sourcing, lineage, quality, transformation and storage while, analytics and reporting.
Work in an Agile team of Data Engineers, Scientists, Developers and RBC business stakeholders.
Build container based solutions, APIs/Microservices and analytics portal for heterogeneous data sources on internal data lakes or cloud platforms
Collaborate with Data scientists, Process Engineers and Business Stakeholders to develop data pipelines, and assist with prescriptive and predictive analytics through consolidated data
Work with platform and infrastructure groups to configure and stand-up required hardware and software stacks.

What do you need to succeed?

Must Have:
2+ years of hands-on experience in following key technologies:
Data engineering solutions: Elasticsearch, Logstash, filebeats, Kafka, Python, Hadoop, Spark, SQLServer, mySQL, MongoDB
API: Node.JS, Django, and Microservices technologies
Security frameworks: LDAP, Kerberos, OAuth 2.0, Vault integration
Portal/analytics solutions: Angular, Kibana
Automation/DevOps: Jenkins, Selenium and similar technologies
Cloud technologies: Openshift, PCF, AWS, Azure
Source code control: GIT
Extensive experience working in agile/SaFe environment
Strong knowledge of algorithms and data structures.
Comfortable applying engineering best practices for Test Driven Development, integration testing, version control, Continuous Integration and Deployment
Undergraduate degree in Computer Science, Engineering or Mathematics

Nice to Have
Experience with Docker, Kubernetes, Cloud deployment.
Experience with Supervised and Unsupervised Machine learning.
Experience in the financial industry, especially trading related businesses.

What’s in it for you?

We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper
A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable
Leaders who support your development through coaching and managing opportunities
Ability to make a difference and lasting impact
Work in a dynamic, collaborative, progressive, and high-performing team
A world-class training program in financial services
Flexible work/life balance options
Opportunities to take on progressively greater accountabilities

JOB SUMMARY
City: Toronto
Address: 155 Wellington St W
Work Hours/Week: 37.5
Work Environment: Office
Employment Type: Permanent
Career Level: Experienced Hire/Professional
Pay Type: Salary + Variable Bonus
Position Level: PL08
Required Travel (%): 0-25
Exempt/Non-Exempt: N/A
People Manager: No
Application Deadline: 07/31/2020
Req ID: 253540"
"Senior Azure Data Engineer, Omnia AI, Vancouver",Deloitte,"- Vancouver, BC","Job Type: Permanent
Primary Location: Vancouver, British Columbia, Canada
All Available Locations: Calgary; Montreal; Toronto; Vancouver

Be encouraged to deepen your technical skills…whatever those may be.
Be expected to share your ideas and to make them a reality.
Be part of a firm where you are valued for your unique strengths and where everyone feels a sense of belonging.

Do you dream about data? Do you speak SQL as well as your mother language? Are you motivated by solving complex problems by leveraging best-in-class data pipelines? We want to hear from you!
What will your typical day look like?

Everyday you will work with our clients to solve their toughest problems in data engineering, data acquisition, and data standardization. You will do so by leveraging leading technology to accomplish large-scale implementations. You will facilitate design sessions with business stakeholders to define key data definitions, consolidate findings, and work with technology teams to develop appropriate data models. Everyday, you will design and implement optimal data pipeline architecture that is auditable, redundant, scalable and high-performing. Since we know you are a great self-starter, you willm also build the cloud infrastructure and pipeline required for optimal extraction, transformation, and loading of data from a wide variety of data sources (structured and un-structured, streaming and batch).
About the team

Omnia AI, Deloitte's Artificial Intelligence practice is comprised of specialized experts with hands-on experience, and cutting-edge information assets that facilitate successful Artificial Intelligence (AI) transformations. We develop AI-enabled solutions to address all aspects of a client’s transformative journey with disciplined focus on business outcomes.
As a member of our Data & Analytics Modernization team within our Omnia AI practice, you will help our clients design and implement the data platform architectures – be it in the cloud or on-premises – required to enable cutting-edge AI solutions. You will be part of a practice to deliver a breadth of solutions to solve our clients most challenging business problems, with a focus on Big Data, BI/DW, Data Integration, Data Governance, Master Data and Analytics applications. Each of these applications leverages a different mix of traditional and innovative technologies to achieve business outcomes.
Enough about us, let’s talk about you

With hands-on Microsoft Azure data ingestion experience, with at least 2 projects (minimum of 3 month each)
Who has leveraged at least 3 out of these 2 components: Azure Data Factory, Azure Event Hub, Databricks
With 1+ years of hands-on experience with data ingestion on Azure
With 3+ years of relevant technology consulting or industry experience in data & analytics delivery
With 2+ years of experience designing and developing data pipelines, data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
With Azure Data Engineer Certification
Why Deloitte?
Launch your career with The One Firm where you can make an impact that matters in a way that you never thought possible. With endless opportunities at every turn, and a culture built to support and develop our people to be the very best they can be, Deloitte is The One Firm for you to learn, grow, create, connect, and lead. We do this by making three commitments to you:
You will lead at every level: We grow the world’s best leaders so you can achieve the impact you seek, faster.
You can work your way: We give you the means to be flexible in how you need and want to work, and we have innovative spaces, arrangements and the mindset to help you be wildly successful.
You will feel included and inspired: We create a deep sense of belonging where you can bring your whole self to work.

The next step is yours
Sound like The One Firm. For You?
At Deloitte we are all about doing business inclusively – that starts with having diverse colleagues of all abilities! We encourage you to connect with us at accessiblecareers@deloitte.ca if you require an accommodation in the recruitment process, or need this job posting in an alternative format. We’d love to hear from you!
By applying to this job you will be assessed against the Deloitte Global Talent Standards. We’ve designed these standards to provide our clients with a consistent and exceptional Deloitte experience globally."
Data Engineer,Providence Health Care,"- Vancouver, BC","Reporting to the Manager, Operations Research and Analytics, the Data Engineer develops and establishes scalable, efficient, automated processes for large scale data analyses, and model development, validation and implementation. The position collaborates closely with other members of the Operations Research and Analytics team to create and deploy automated data pre-processing and advanced analytics models through the innovative understanding and use of large data sets to improve clinical processes and patient outcomes, and support data-driven decision making. The ideal candidate will have extensive experience in dimensional modeling, developing advanced analytics algorithms, optimizing processes with advanced analytics solutions, and excellent problem solving ability dealing with huge volumes of corporate and clinical data. The position will also stay apprised of current trends and research on all aspects of data engineering and advanced analytics techniques and works in collaboration with provincial and national colleagues.
Skills
Demonstrated strength in data modeling, ETL development, and data warehousing with solid knowledge of various industry standards such as dimensional modeling, and star schemas etc.
Demonstrated ability to build and optimize ‘big data’ data pipelines, architectures and data sets and computing tools such as Spark, MySQL, Azure, AWS, etc.
Demonstrated knowledge of methods and techniques involved in Advanced Analytics, data mining, statistics, and optimization (including neural networks, reinforcement learning, and adversarial learning).
Coding proficiency in at least one modern programming language (Python, Java, etc)
Demonstrated proficiency working with both relational (SQL) and non-relational databases (NoSQL).
Demonstrated understanding of data privacy, security and related tools such as anonymization and encryption.
Excellent oral and written communication skills and ability to clearly and fluently translate technical findings to non-technical partners and to communicate to multiple audiences using data storytelling and through graphics.
Demonstrated ability to work collaboratively in an interdisciplinary environment and to develop recommendations using facilitation and consensus building.
Strong analytical, critical thinking, and evaluation skills to discern and help solve the important problems facing health care, to identify new ways to leverage our data, and to direct efforts in the right direction.
Education
A Masters’ Degree in Computer Science, Mathematics, Engineering, or other quantitative degree is required plus at least five (5) years’ experience as a Data Engineer or related specialty (e.g., Software Developer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets. Experience with healthcare analytics is an asset.
Duties
1. Creates and maintains optimal data pipeline architecture. Builds analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.

2. Builds the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and big data technologies. Re-designs infrastructure for greater scalability.

3. Assembles large, complex data sets that meet functional/non-functional business requirements. Creates data tools for the operations research and analytics team to assist them in building and optimizing our product into an innovative industry leader.

4. Identifies, designs, and implements internal process improvements including establishing standards for development processes and technical requirements; automating manual processes; optimizing data delivery, etc. Develops and ensures compliance for data management processes, policies and standards. Implements and enforces controls to maintain data availability and quality.

5. Recognizes and adopts best practices in data integrity, test design, analysis, validation, and documentation. Tunes application and query performance using profiling tools and SQL.

6. Researches innovative solutions, develops new concepts, and implements proof-of-concept prototypes by applying expertise in data engineering, advanced analytics, process modeling, and optimization tools and techniques.

7. Works in close collaboration with other members of the Data Analytics and Health Informatics teams to develop and implement innovative solutions that address pressing operational and clinical challenges within the organization.

8. Reviews clinical data at aggregate levels on a regular basis using analytical reporting tools to support the identification of risks and data patterns or trends. Creates analytical reports and presentations to facilitate review and adoption of data-driven choices. Collaborates with project/program teams to address data-related questions and to recommend potential solutions.

9. Works closely with clinical and management teams across PHC to strategize, develop, and implement advanced modelling projects that translate into improved quality of care, clinical outcomes, reduced costs, temporal efficiencies, and process improvements.

10. Works with stakeholders including the Executive, and Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

11. Keeps up-to-date with the latest technology trends and methods by staying abreast of state-of-the-art literature in the fields of advanced analytics, statistical modeling, statistical process control and mathematical optimization.

12. Perform other duties as required."
Senior Data Engineer,Cigna,"- Mississauga, ON","The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture across the enterprise. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, data architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing Express Script Canada’s data architecture to support our next generation of products and data initiatives.

ESSENTIAL FUNCTIONS:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Optimize the value of technology investments in data management for the business by aligning the business architectures with technology architectures. Identify and drive key technology investments to meet business objectives, remediate technology and process gaps. Determine feasibility, cost and time required, compatibility with current system, and system capabilities.
Identify product, technology and process gaps in current data & technology architectures and recommend solutions to bridge gaps between the business and the data & technology deployed to support the business.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Coordinate with data users and key stakeholders across ESC’s Lines of Business to refine and achieve various long-term objectives for data architecture
Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
Work with data and analytics experts to strive for greater functionality in our data systems.

QUALIFICATIONS:
Minimum 6 years of experience in a large-scale, multi-platform, multi-tier processing environment with a Bachelor’s degree in Information Systems or related field
Extensive experience on projects implementing Master Data Management and Enterprise Content Management techniques and platforms.
Expert domain knowledge & experience in data warehousing, reporting and advanced analytics platforms, encompassing data model design, dimensional modeling, master data management, cross-cutting concerns, common integration technologies, patterns & standards and emerging technologies.
Advanced knowledge of data architecture and analytics principles and practices for application development and data warehousing purposes.
Advanced knowledge of data model development and governance methods and practices
Previous data management experience in a data warehouse and data lake environment.
Experience with data center migrations, enterprise database consolidation, data warehouse migration and/or consolidation to data lakes or integrated data architectures.
Experience building multi-cloud analytics and technology strategies.
Highly proficient in Relational Database Management Systems
(RDBMS) and Big Data, data architecture, data modeling (including dimensional),
data warehousing, object-oriented methodologies, and client/server development.
Advanced knowledge of PowerDesigner or similar Data Modelling Tool
Advanced knowledge of Talend or similar Data Integration Tool
Hands-on experience using data technologies to implement data profiling tools and modern database implementations including Enterprise Data Lakes, graphDB, key-value pair, column-store, Big Table, RDF, In-Memory DB, etc..
Working knowledge of Hadoop platforms (Hortonworks or Cloudera) and key technologies like Apache Nifi, Kafka, Spark, Pig, Hive, NoSQL databases like MongoDB, Cassandra or Hbase.
Experience implementing data visualization tools like Tableau, Qlik, etc.
Willingness to work a flexible schedule to accommodate project deadlines and travel requirements

Assets:
Knowledge of the group health insurance (pharmacy, dental, other health) industry or adjudication systems is an asset
Knowledge of advanced analytics tools like R, SAS, and machine learning algorithms.
Knowledge of one or more programming or scripting languages like Java, C, C#, .Net, Javascript, PHP, Python
Knowledge of the DAMA Book of Knowledge
Knowledge of Big Data & Logical Warehouse architecture
Knowledge of TOGAF, Zachman or other
architecture frameworks

ABOUT EXPRESS SCRIPTS CANADA

Express Scripts Canada, a registered business name of ESI Canada, an Ontario partnership indirectly controlled by Express Scripts, Inc. (Nasdaq: ESRX), is one of Canada’s leading providers of health benefits management services. From its corporate headquarters in Mississauga, Ontario, just outside Toronto, Express Scripts Canada provides a full range of integrated pharmacy benefit management (PBM) services to insurers, third-party administrators, plan sponsors and the public sector, including health-claims adjudication and processing services, Home Delivery Pharmacy Services, benefit-design consultation, drug-utilization review, formulary management, and medical and drug-data analysis services, to better facilitate the best possible health outcomes at the lowest possible cost.

It will be a condition of employment that the successful candidate receives the Enhanced Reliability Clearance from the Federal Government. The candidate will be required to provide supporting documentation in order to receive Clearance if required.

We offer a competitive salary, along with a positive work environment built on solid corporate values, integrity, mutual respect, collaboration, passion, service and alignment.

We are an equal opportunity employer that promotes a diverse, inclusive and accessible workplace. By embracing diversity, we build a more effective organization that empowers our employees to be the best that they can be.

We are committed to creating a working environment that is barrier-free and we are prepared to provide accommodation for people with disabilities. Thank you for your interest in this position, however only qualified candidates will be contacted for an interview. No telephone calls please.

For more information about Express Scripts Canada, visit its Web site at www.express-scripts.ca

About Cigna
Cigna Corporation (NYSE: CI) is a global health service company dedicated to improving the health, well-being and peace of mind of those we serve. We offer an integrated suite of health services through Cigna, Express Scripts, and our affiliates including medical, dental, behavioral health, pharmacy, vision, supplemental benefits, and other related products. Together, with our 74,000 employees worldwide, we aspire to transform health services, making them more affordable and accessible to millions. Through our unmatched expertise, bold action, fresh ideas and an unwavering commitment to patient-centered care, we are a force of health services innovation.

When you work with Cigna, you’ll enjoy meaningful career experiences that enrich people’s lives while working together to make the world a healthier place. What difference will you make? To see our culture in action, search #TeamCigna on Instagram."
Data Engineer II,"AMZN CAN Fulfillment Svcs, ULC","- Vancouver, BC","3+ years of professional experience in analytics, business analysis or comparable analytics position, including handling large/complex data sets, developing metrics, and developing trusting customer relationship, data manipulation tools and presentation tools (e.g. Quicksight, AWS, etc.).
Proficiency in Python, SQL, and Excel
Excellent written communication and presentation skills
Bachelor's degree in a quantitative field or similar.

Language required for job: English
#0000

Job Location: 510 W Georgia St, Vancouver, BC V6B 0M3, Canada

Terms of employment: Full time, permanent

Job Description
Do you feel passionate about data? Are you excited about figuring out ways slice, dice, and send data to multiple sources? Woot.com, the innovative company who invented the deal-of-the-day business model, is looking for a talented, self-motivated Business Intelligence Engineer with broad technical skills to dive deep into large amounts of data, have a great business sense, curiosity and a proven ability to deal with ambiguity, and the desire to influence key strategic decisions with data-driven analysis and insights.

Woot specializes in daily deals across multiple categories and offers our customers unique content giving them many reasons to visit our website and mobile apps frequently. Via integration into the development process, Data Pipeline programs influence course-corrections in product roadmaps that yield future benefits to customers while lowering the dependency on manual reporting.

As a Business Intelligence Engineer, you will help answer key questions such as “How do Website, Mobile App redesign initiatives help customers discover products in a more efficient manner? How will our customers react to changes in prices, product selection, delivery times, return/replacement policies etc.? Which customer would like this deal the best? What is the best marketing channel to promote a particular deal to our customer base? How can we improve our customer service? How can we improve our delivery experience? etc” These are among the most important questions at Woot today.

You will have an opportunity to shape the long-term outlook for Woot’s business intelligence and shape strategic decisions by working directly with the leadership team and key cross functional stakeholders. The ideal candidate will be able to formalize problem definitions from ambiguous requirements, extract insights from big data and develop cutting-edge solutions for non-standard problems. You are an individual with outstanding analytical abilities, excellent communication skills, and are comfortable working with technical teams and systems. You will draw upon advanced analytical, critical thinking, problem solving skills, software development experience, and a passion for creating maintainable, highly reliable, systems which operate 24/7. You will work with analytic tools, can write excellent SQL scripts, can partner with customers to answer key business questions, and you are an advocate for your customers.

Key Expectations:
Work closely with business stakeholders to understand business issues and develop the data driven insights to drive the business’ direction.
Work closely with internal Amazon teams to develop data ingestion models from varying customer and application touchpoint sources (such as survey data, application metrics, APMQ messages, customer service contacts among others) for text mining analysis.
Audit the tagging process (manual or automated) to make sure data is complete and accurately represented in the systems.
Understand that ad-hoc analysis is part of the landscape and be prepared to structure relevant analysis and demonstrate a sense of urgency in delivering insights to the leadership team.
Dive deep into data by leveraging SQL, MS Access, Excel, and other data manipulation tools and presentation tools (e.g. Quicksight, AWS, etc.). .
Develop and propose new metrics, recommend strategies to stakeholders to improve business results, and work across our organization to make actionable intelligence available to business stakeholders.
Contribute to the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
Interact with software and business groups to develop an understanding of their business requirements and operational processes.

Salary Range: $88,200 to $147,300/yr, commensurate with experience #0000

All candidates must meet all qualifications above.

Benefits: Amazon provides a full range of benefits for our global employees and their eligible family members. Eligible employees may also receive signing bonuses and Amazon Restricted Stock Units. This position is eligible for further pay increases and bonuses at the company's discretion. While they might vary from location to location, Amazon benefits for Canada may include:
Health Care
Savings Plans
Income Protection
Paid Time Off
Signing Bonuses
Employee Stock
#0000"
Data Engineer,SkipTheDishes,"- Calgary, AB","We’re revolutionizing the way humanity eats, and there’s a lot of room for optimization and growth. That’s where you come in. Your ingenuity will help us continue to drive innovation, making an impact on the reliability, performance, and scalability of Skip’s industry-leading technology.

Job Summary:
Think you have what it takes to join an elite team of software developers, engineers, and data scientists? If you want to make your mark on a national brand’s industry-leading technology, Skip’s Engineering Team is the place for you. They drive the innovation of Skip’s platform by improving the reliability, performance, and scalability of the network. GPS tracking, real-time order processing, scheduling, and balancing are just a few of the pieces the Engineering Team integrates into Skip’s world-class service.
As a member of our Data team, you will be responsible for the design and implementation of software for systems that power Skip’s Business Intelligence and Data Warehouse. Your experience with cloud technologies and big data provides you with the background to quickly implement changes. Your experience in data management and SQL allows you to contribute to the expanding expertise of the team and keep data components current as product feature change.
How we work:
We take ownership of our work and work closely with our team.
We move quickly, take risks, and know how to manage the risks.
Regularly refactoring to improve our existing systems — technical debt isn’t an excuse.
Unit tests and code reviews are at our core — confidence in our pull request is the result.
We constantly push our app’s technology, design, and architecture forward to meet new challenges.
We face challenges no one can predict — we meet them head-on as a team.
When we have an idea that serves a need, we run with it.
Our teams are kept tight and efficient.
Experience Needed:
Experience working with both large and real-time data sets.
Strong hands-on experience with Python.
Hands-on experience with backend python frameworks.
Experience in REST APIs.
Experience with AWS and/or Google Cloud Platform data components.
Experience working with tools, languages and protocols such as Redshift, Big Query and/or columnar databases.
Familiarity with scheduling tools like Airflow.
Strong in Dimensional Modeling concepts.
Experience using version control software, such as Git.
Work well both independently and as part of a team. You will actively participate in design discussions and code reviews.
Qualifications:
Computer science, software engineering or related degrees.
We are looking for people who:
Have a great attitude.
Thrive in a rapidly evolving environment.
Are eager to contribute to the growth and development of the team.
Work with an entrepreneurial sense of urgency.
Are receptive to feedback and challenging experiences.
Are actively involved in their ongoing personal growth and learning.
What It’s Like To Work At Skip
Picture this: you, dressed in your fave casual attire, amongst a team of friendly and passionate colleagues. You feel pride knowing your input and uniqueness are not only embraced but make an impact on a major Canadian company and its satisfied customers. As the company grows, so do you — you meet and surpass new challenges every day.
That’s just a small taste of what it’s like to work at one of Canada’s leading tech companies. If you’re hungry for opportunity, growth, and something meaningful in a dynamic, yet casual environment, we’d love to hear from you.
Note: All employees will be asked to sign a Consent for Disclosure of Personal Information in order to complete a background check. Job offers will be conditional upon results that the Company determines to be satisfactory."
Staff Data Engineer,Infoblox,"- Burnaby, BC","Infoblox is looking for a Staff Data Engineer to augment our growing Cyber Security Software Development Team. This growing team supports the Infoblox mission to thwart cybersecurity threats in our customer’s networks. This is an opportunity to work closely with data scientists and threat analysts to curate the data that makes this mission possible.
The ideal candidate is a savvy software engineer with experience in data engineering and a solid background in Spark and Python. Preferably you know that countMinSketch is not a children’s game. You are comfortable wearing several hats in a small organization with a wide range of responsibilities and have worked in a cloud environment, such as Amazon EMR. You know that Big Data is both a blessing and a curse; without good data engineering, it loses its potential. You are passionate about the nexus between data and computer science-driven to figure out how best to represent and summarize data in a way that informs good decisions and drives new products. When someone says, “my Spark job failed”, your first question is “what’s the skew?”. Come join our growing Cyber Threat Intelligence team and help us build world-class solutions!
Responsibilities:
Curate very large-scale data from a multitude of sources into appropriate sets for research and development for data scientists, threat analysts, and developers across the company.
Design, test, and implement storage solutions for various consumers of the data.
Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods.
Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics.
Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines.
Collaborate on the design, implementation, and deployment of applications with the rest of software engineering.
Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data.
Build and maintain tools for automation, deployment, monitoring, and operations.
Create test plans, test cases, and run tests with automated tools.
Requirements:
7+ years of experience with Python3, and 2+ years experience with Spark. Scala experience is helpful.
5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments.
3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL
3+ years of experience with developing ETL pipelines and data manipulation scripts
Proficient in Object-Oriented Design and S.O.L.I.D principles.
Strong emphasis on unit testing and code quality.
Proficient with AWS products (EMR S3, Lambda, VPC, EC2, API Gateway, etc).

Preferred Experience:
Very strong Python and PySpark experience.
Very strong back end development experience.
Strong experience with cloud deployments and CI/CD.
Experience with virtualization, containers, and orchestration (Docker, Kubernetes, XEN).
Experience with NoSQL Non-Relational databases (AWS DynamoDB).

Education:
MS or BS in Computer Science or a related field, or equivalent work experience required.


Perks:
Work with a world-class technology team in a rapidly growing company
A career path with opportunities to grow
Boutique office space with state of the art amenities, located in the heart of Metro Vancouver area; steps from SkyTrain and Metrotown Mall
Cross-functional break room stocked with snacks and beverages
And many, many more perks!

It’s an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies—and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team—our future looks bright, and so will yours. To check out what it’s like to be a Bloxer click here.

#LI-AB1"
Data Engineer II,"AMZN CAN Fulfillment Svcs, ULC","- Vancouver, BC","3+ years of professional experience in analytics, business analysis or comparable analytics position, including handling large/complex data sets, developing metrics, and developing trusting customer relationship, data manipulation tools and presentation tools (e.g. Quicksight, AWS, etc.).
Proficiency in Python, SQL, and Excel
Excellent written communication and presentation skills
Bachelor's degree in a quantitative field or similar.

Language required for job: English
#0000

Job Location: 510 W Georgia St, Vancouver, BC V6B 0M3, Canada

Terms of employment: Full time, permanent

Job Description
Do you feel passionate about data? Are you excited about figuring out ways slice, dice, and send data to multiple sources? Woot.com, the innovative company who invented the deal-of-the-day business model, is looking for a talented, self-motivated Business Intelligence Engineer with broad technical skills to dive deep into large amounts of data, have a great business sense, curiosity and a proven ability to deal with ambiguity, and the desire to influence key strategic decisions with data-driven analysis and insights.

Woot specializes in daily deals across multiple categories and offers our customers unique content giving them many reasons to visit our website and mobile apps frequently. Via integration into the development process, Data Pipeline programs influence course-corrections in product roadmaps that yield future benefits to customers while lowering the dependency on manual reporting.

As a Business Intelligence Engineer, you will help answer key questions such as “How do Website, Mobile App redesign initiatives help customers discover products in a more efficient manner? How will our customers react to changes in prices, product selection, delivery times, return/replacement policies etc.? Which customer would like this deal the best? What is the best marketing channel to promote a particular deal to our customer base? How can we improve our customer service? How can we improve our delivery experience? etc” These are among the most important questions at Woot today.

You will have an opportunity to shape the long-term outlook for Woot’s business intelligence and shape strategic decisions by working directly with the leadership team and key cross functional stakeholders. The ideal candidate will be able to formalize problem definitions from ambiguous requirements, extract insights from big data and develop cutting-edge solutions for non-standard problems. You are an individual with outstanding analytical abilities, excellent communication skills, and are comfortable working with technical teams and systems. You will draw upon advanced analytical, critical thinking, problem solving skills, software development experience, and a passion for creating maintainable, highly reliable, systems which operate 24/7. You will work with analytic tools, can write excellent SQL scripts, can partner with customers to answer key business questions, and you are an advocate for your customers.

Key Expectations:
Work closely with business stakeholders to understand business issues and develop the data driven insights to drive the business’ direction.
Work closely with internal Amazon teams to develop data ingestion models from varying customer and application touchpoint sources (such as survey data, application metrics, APMQ messages, customer service contacts among others) for text mining analysis.
Audit the tagging process (manual or automated) to make sure data is complete and accurately represented in the systems.
Understand that ad-hoc analysis is part of the landscape and be prepared to structure relevant analysis and demonstrate a sense of urgency in delivering insights to the leadership team.
Dive deep into data by leveraging SQL, MS Access, Excel, and other data manipulation tools and presentation tools (e.g. Quicksight, AWS, etc.). .
Develop and propose new metrics, recommend strategies to stakeholders to improve business results, and work across our organization to make actionable intelligence available to business stakeholders.
Contribute to the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
Interact with software and business groups to develop an understanding of their business requirements and operational processes.

Salary Range: $88,200 to $147,300/yr, commensurate with experience #0000

All candidates must meet all qualifications above.

Benefits: Amazon provides a full range of benefits for our global employees and their eligible family members. Eligible employees may also receive signing bonuses and Amazon Restricted Stock Units. This position is eligible for further pay increases and bonuses at the company's discretion. While they might vary from location to location, Amazon benefits for Canada may include:
Health Care
Savings Plans
Income Protection
Paid Time Off
Signing Bonuses
Employee Stock
#0000"
"Azure Data Engineer, Omnia AI - Montreal",Deloitte,"- Montréal, QC","Job Type: Permanent
Primary Location: Montreal, Quebec, Canada
All Available Locations: Montreal

Be encouraged to deepen your technical skills…whatever those may be.
Be expected to share your ideas and to make them a reality.
Be part of a firm where you are valued for your unique strengths and where everyone feels a sense of belonging.

Do you dream about data? Do you speak SQL as well as your mother language? Are you motivated by solving complex problems by leveraging best-in-class data pipelines? We want to hear from you!
What will your typical day look like?


Everyday you will work with our clients to solve their toughest problems in data engineering, data acquisition, and data standardization. You will do so by leveraging leading technology to accomplish large-scale implementations. You will facilitate design sessions with business stakeholders to define key data definitions, consolidate findings, and work with technology teams to develop appropriate data models. Everyday, you will design and implement optimal data pipeline architecture that is auditable, redundant, scalable and high-performing. Since we know you are a great self-starter, you willm also build the cloud infrastructure and pipeline required for optimal extraction, transformation, and loading of data from a wide variety of data sources (structured and un-structured, streaming and batch).
About the team

Omnia AI, Deloitte's Artificial Intelligence practice is comprised of specialized experts with hands-on experience, and cutting-edge information assets that facilitate successful Artificial Intelligence (AI) transformations. We develop AI-enabled solutions to address all aspects of a client’s transformative journey with disciplined focus on business outcomes.
Our Canadian Delivery Centre (CDC) team within our Omnia AI group is based out of Montreal and helps multiple clients replace, upgrade or maintain their information systems. We deliver a breadth of solutions to solve our clients most challenging business problems. Our teams deliver “Big Data”, Robotic Processing Automation, Digital Content, Data Integration, and Analytical Solutions. Each one of these solutions leverages a different mix of new technologies to achieve a business outcome.
We have:
Leading methods and tools
Top professionals to coach and develop you
An open mind for new ideas
Opportunities to work with leading Canadian and global companies
Partnerships with top software providers
A dynamic and energetic workplace
Enough about us, let’s talk about you

With hands-on Microsoft Azure data ingestion experience, with at least 2 projects (minimum of 3 month each)
Who has leveraged at least 3 out of these 2 components: Azure Data Factory, Azure Event Hub, Databricks
With 1+ years of hands-on experience with data ingestion on Azure
With 3+ years of relevant technology consulting or industry experience in data & analytics delivery
With 2+ years of experience designing and developing data pipelines, data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
With Azure Data Engineer Certification
Why Deloitte?
Launch your career with The One Firm where you can make an impact that matters in a way that you never thought possible. With endless opportunities at every turn, and a culture built to support and develop our people to be the very best they can be, Deloitte is The One Firm for you to learn, grow, create, connect, and lead. We do this by making three commitments to you:
You will lead at every level: We grow the world’s best leaders so you can achieve the impact you seek, faster.
You can work your way: We give you the means to be flexible in how you need and want to work, and we have innovative spaces, arrangements and the mindset to help you be wildly successful.
You will feel included and inspired: We create a deep sense of belonging where you can bring your whole self to work.

The next step is yours
Sound like The One Firm. For You?
At Deloitte we are all about doing business inclusively – that starts with having diverse colleagues of all abilities! We encourage you to connect with us at accessiblecareers@deloitte.ca if you require an accommodation in the recruitment process, or need this job posting in an alternative format. We’d love to hear from you!
By applying to this job you will be assessed against the Deloitte Global Talent Standards. We’ve designed these standards to provide our clients with a consistent and exceptional Deloitte experience globally."
IBM DataStage Big Data Edition-Engineer,Wipro LTD,"- Toronto, ON","Toronto, Canada
BE / BTech
1359213
Job Description
Key skills required for the job are:
IBM DataStage Big Data Edition-L3, (Mandatory) .
As a Senior Engineer, you are responsible for areas around Engineering, Global Infrastructure Services etc.
You are expected to have good practical understanding of technology, its application and be involved in implementation, integrated testing, debugging and documentation.
You are expected to be a Technical SME, and also handle Project Effort Estimation Related Activities, process compliance.
Minimum work experience: 3 - 5 YEARS

Roles and Responsibilities
Mandatory Skills: IBM DataStage Big Data Edition-L3
Experience Range: 3-5 YEARS
Employment Type: permanent"
Data Engineer,Breather,"- Quebec City, QC","The Role
The infrastructure team at Breather is looking for a talented and meticulous data engineer: someone that will take ownership of data quality and work to elevate it and make it as relevant as possible. That person will help interpreting the data as well as sit down with stakeholders to determine how to best improve their workflows. The ideal candidate can understand the specifics of Breather's internal data pipelines, intervene to fix problems, suggest and implement ways to improve them. This role offers both the opportunity to gather the big picture of the business and the leisure to suggest and implement technical solutions.

As a Data Engineer at Breather, you will:
Take ownership of the data ingestion pipelines from beginning to end
Help troubleshoot and guard against data integrity problems
Review, optimize and streamline Breather's whole ETL system
Work closely with various internal teams and stakeholders to improve their workflows and understanding of the available data
Audit and improve data warehouse models
Use various data transformation tools to aggregate several sources
Make and implement suggestions for new data ingestion architecture

You will help your team:
Coordinate with various teams to review and improve revenue data
Audit current transformation code to streamline it and update it to use the latest schema version
Coordinate with the real estate team to streamline their data input process
Advise on data requirements / best practices for new projects
Participate in the production and development of business reports

Helpful Skills and Experience you'll need to succeed:
2+ years experience as data engineer
Proficiency with the SQL language
Proficiency with the Python language
Solid understanding of relational database design
Good understanding of cloud technologies involved in an ETL in general
Strong communication skills
Ability to discern and negotiate with stakeholders their needs in order to design relevant, high quality solutions
Sincere desire to acquire and master the big picture of the company's business data
Natural tendency towards using industry best practices



What We Value
At Breather, our company values are not only the principles that link us together in the way we work, but they also keep us aligned on the same path, all working towards connected goals. These standards are what guide us on our collective journey through space(s).
Default to transparency
Create with care
Be a piece of the puzzle
Propose. Build. Deploy.

Perks and Benefits
We hope that you're excited by all the possibilities that come with working at Breather! In addition to our unique culture, we also offer these fun perks and benefits:
Annual health and wellness reimbursement
A generous Paid Time Off package per the calendar year
Annual learning reimbursement that benefits your career growth
Free access to the Breather network of over 500 spaces (and growing, fast!)
Access to comprehensive medical, vision, and dental coverage. Oh, and it's on us
Dog friendly office - bring your dog to work, they'll find plenty of friends waiting to play with them!
Be part of a well-funded, proven startup with big ambitions, competitive salary and company options
Fun work environment and company culture with an upbeat, first-class team plus a kitchen filled with healthy snacks, bagels and chips!"
Data Engineer,Manulife,"- Toronto, ON","Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description
Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference and a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
The fast-growing Manulife Canadian Data office is hiring a data engineer working on advanced analytics engineering, ETL, AI/machine learning production, and DataOps in Toronto or Waterloo office
As Data Engineer in this role you will work on:
AI/Machine learning engineering and production
ETL, Big data application and analytics products
Advanced analytics and DataOps solutions in the Big Data ecosystem
Responsibilities for this role include:
Build AI/machine learning engineering and production, including recommendation engine and real-time decision management system
Lead multiple multi-functional teams to build big data application and analytics products using cloud/bigdata/advanced analytics technologies to advance Manulife’s all lines of business
Participate in PoC/PoT efforts to integrate new Big Data management technologies, software engineering tools, and new patterns into existing structures
Exploratory data analysis; Query and process Big Data, provide reports, summarize and visualize the data
Research opportunities for data acquisition and new uses for existing data
Development of Big Data set processes for data modeling, mining and production
Consult, collaborate, and recommend solutions for batch and streaming use case patterns
Create and publish design documents, usage patterns, and cookbooks for technical community
Experience required for this role is as follows:
Over three years of experience in Pyspark/SparkSQL/Scala programming in cloud/spark environment
Strong experience in AI/Machine learning engineering and production
Hands-on working experience in building recommendation engine and real-time decision management system
Experience with DevOps or continuous delivery tools and processes a plus
Experience with data as service/API service/Microservice/Neo4j/Elasticsearch/sparkstreaming
Experience in Big Data performance analysis, tuning and capacity planning
Show passion, take accountability, and be willing to grow and lead
What about Perks?
Manulife has lots of perks including, but not limited to:
Competitive compensation
Retirement Savings Accounts including a RPP (Pension Plan), RRSP (Retirement Savings Plan), and TFSA (Tax Free Savings account)
Manulife Share Ownership Program with employer matching
Customizable Benefits Package including Health, Dental, Vision, and 100% of Mental Health expenses
Financial support for ongoing training, learning, and education
Monthly Innovation Days (Hackathons)
Wearing jeans to work every day
An abundance of career paths and opportunities to advance
A flexible work environment with flex hours, work from home arrangements, distributed teams, and condensed work week arrangements.
This is a full time permanent role that can be worked from Toronto or Waterloo, Ontario.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people achieve their dreams and aspirations by putting customers' needs first and providing the right advice and solutions. We operate primarily as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2017, we had approximately 34,000 employees, 73,000 agents, and thousands of distribution partners, serving more than 26 million customers. As of December 31, 2017, we had over $1.04 trillion (US$829.4 billion) in assets under management and administration, and in the previous 12 months we made $26.7 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.
About Manulife
Manulife Financial Corporation is a leading international financial services group that helps people make their decisions easier and lives better. With our global headquarters in Toronto, Canada, we operate as Manulife across our offices in Canada, Asia, and Europe, and primarily as John Hancock in the United States. We provide financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions. At the end of 2019, we had more than 35,000 employees, over 98,000 agents, and thousands of distribution partners, serving almost 30 million customers. As of March 31, 2020, we had $1.2 trillion (US$0.8 trillion) in assets under management and administration, and in the previous 12 months we made $30.4 billion in payments to our customers. Our principal operations are in Asia, Canada and the United States where we have served customers for more than 155 years. We trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.
Manulife is an equal opportunity employer. We strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. We are committed to fair recruitment, retention and advancement and we administer all of our practices and programs based on qualification and performance and without discrimination on any protected ground.
It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request any accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially."
Data Engineer,Keyrus,- Canada,"Keyrus Canada is looking for a Data Engineer.

Your main mission:
You will play a key role in providing our clients with business intelligence and ETL solutions to manage their data assets. You will work directly with our client's to add value to their data intelligence environment through the use of tools such as Alteryx and Tableau. You will be responsible for the entire end to end analytics solution, from backend data engineering, manipulation, and cleansing, to front-end data visualization.
Conduct business and functional requirements gathering and provide projects estimates.
Design and develop data models and ETL processes to feed data warehouses.
Design and develop reports and dashboards for informational/operational needs according to best practices.
Document business intelligence and information management solutions.
Support your colleagues in the implementation.
Perform/document tests (unit/integrated) for the solutions you have participated in.
Carry out activities in compliance with and promoting the Business Intelligence norms and standards established by the organization.
Coach your team members and colleagues.
Write all types of documents to feed an internal community of users and also to make presentations.

Our ideal candidate:
At least 3+ years of experience as a Business Intelligence Developer, working directly with tools such as Alteryx, Talend or SSIS for ETL as well as with a data visualization tool such as Tableau
Experience working with a relational database (SQL, Oracle, etc.) and data modeling
Knowledge of data visualization best practices and cloud warehouses (i.e. Snowflake, Redshift) would be an asset
Knowledge of cloud data pipelines setup and Python would be an asset
Experience working with business teams to translate functional requirements into technical requirements
Minimum of a Bachelor's Degree in IT or related field
Must be eligible to work in Canada without a present or future Keyrus-sponsored Visa/Work Permit

You love:
Contributing to make your solutions impactful
Expressing your ideas and share your opinions
Working in a growing company
Celebrating your success
Being yourself with your colleagues and have fun in your work
Working in an inspiring environment
Enjoy from many work benefits

Keyrus Canada, that’s:
A success-story in Data and Digital!
High added value projects to optimize performance and competitiveness of our clients as well as accelerating their transformation
20 years of expertise in consulting and integration of innovative solutions based on:
Data Intelligence (Business intelligence, Big Data & Analytics, information management, training externalization)
Digital Experience (Innovation and digital strategy, digital marketing and CRM, digital commerce, digital performance, digital experience)
Management & Transformation (strategy and innovation, transformation, project support and performance management)
3000 talents in 17 countries and 5 continents
A DNA focused on innovation and entrepreneurship

Why joining us?
To integrate in a community of curious and passionate experts and to evolve in a multi-cultural environment promoting international mobility.
Because you are a #DataGeek, #DigitalAddict, #InnovationLover!"
Big Data Engineer,Suncor Energy Services,"- Calgary, AB","LOCATION: Calgary, Alberta (CA-AB)
JOB NUMBER: 28716
Why you should join us:
We are living in an era of transformation – as a company, as an industry and as a global community. Suncor is evolving and we’re calling it Suncor 4.0. Technology and digital solutions are a big part of this, but unleashing the full potential of our people is what will get us where we need to be.
An essential part of our evolution is creating value through data and becoming a data-informed enterprise where employees can apply their knowledge, draw insights and make the best possible decisions. Hiring specialists like yourself will help us implement our Advanced Analytics strategy and transform our business. It’s all about strengthening our future through digital technology, unleashing the full potential of our people, and creating an engaging and productive workplace.
As a Data Engineer, you will play an active role throughout the entire engagement cycle, from architecture and design, to build test and deploy; specializing in rapid deployment of data pipelines and analytics solutions, including orchestration, automated testing and quality control, continuous integration/deployments, versioning and environment management.
If you’re passionate about data, and thirsty for new approaches that provide insights into the most complex challenges through the architecting of data for advanced analytics, this role is for you!
Are you ready to help us solve problems we’ve never been able to address before? Join us and be part of transforming Suncor’s future!
You will use your expertise to:
Work on multiple projects as a technical team member or lead, driving user story analysis and elaboration for data requirements
Ensure all automated processes preserve data by managing the alignment of data availability and integration processes
Develop, construct and automate data pipelines that allow for leading edge analytical models
Perform technology and product research to better define requirements, resolve important issues and improve the overall capability of the data ingestion and transformation stack
Translate complex functional and technical requirements into detailed, design, and high performing end to end data pipelines
Derive and develop the automation and promotion of data pipelines from development to test to production
Construct the production management and monitoring of data pipelines
We’d like to review your application if you have…
Must-haves (minimum requirements):
Seven years of experience in data management, data and environment provisioning, and test automation, using relevant Big Data technologies on Azure, AWS or Google platforms
A Bachelor’s degree or Masters in Computer Science, Computer Engineering, Software Engineering or a related technical field
Experience working in a cloud platform (e.g. Microsoft Azure, AWS, Google Cloud)
Experience in Extract, Transform and Load processes (ETL), SQL databases, data warehousing solutions, data modeling and scripting languages
Exceptional analytical and critical reasoning skills
Proven communication and social skills
Alignment with our values: safety above all else, stronger together, operational discipline, curiosity and lifelong learning, and act with integrity
Preference for:
Familiarity with analytical tools and languages such as Matlab, Python, R, SPSS
Familiarity with NoSQL databases
Where you’ll be working, your work schedule, and other important information:
You will work out of our Calgary head office, located in the Suncor Energy Centre at 150 – 6th Ave S.W.
This role can also be supported from one of our hub locations (e.g. Mississauga, Fort McMurray, Edmonton, Sarnia, Montreal)
One of our values is curiosity and lifelong learning – we challenge the status quo and learn from and support each other to make the world a better place
We are so invested in building advanced analytics skills within our organization that we've created the Analytics Academy to profide training to employees through instructor-led courses, e-learning, communities of practice, etc.
Why Suncor?
Supporting our people. Caring for our communities. Living our purpose.
With our operations declared an essential service, we are continuing to operate and are looking to fill business-critical roles at this time. We require qualified and safety-focused employees to help keep things running. In terms of our current recruitment process, we’re doing things a bit differently to adjust to our new ‘normal.’ We are using technology to conduct virtual interviews to adhere to physical distancing recommendations and we are well set up for remote onboarding and orientation.
We are bolstered by the unity across our community and the solidarity across the world. We continue to encourage you to support global efforts to limit the impact of COVID-19 with good hygience practices, physical distancing, and with care and consideration for the people around you. For more information on how we’re responding to COVID-19, click here. #StrongerTogether
We are Canada's leading integrated energy company with a business portfolio that includes oil sands development and upgrading, offshore oil and gas production, petroleum refining, and product marketing under the Petro-Canada brand. Our global presence offers rewarding opportunities for you to learn, contribute, and grow in a variety of career-building positions. We live by the value of safety above all else – if it isn't safe, we don't do it. Our strong track record of growth and a focus on sustainability mean tremendous potential for the future. Learn about our purpose and values.
In addition to rewarding job opportunities, we offer an attractive employee package, including:
Competitive base salary, compensation programs, and an annual incentive program
Flexible benefits package
Rewarding pension and savings plans
Stay connected to us:
Follow us on LinkedIn, Facebook and Twitter for the latest job postings and news
Visit our Report on Sustainability to see our progress on a number of environmental, social and economic topics and what we’re doing to position our company for the future
Join our Talent Community and sign up to receive customized job alerts
Read our Suncor Connections newsletter to see what we’re doing in the communities we live and work in
We are an equal opportunity employer and encourage applications from all qualified individuals. We are committed to providing a diverse and inclusive work environment where every employee feels valued and respected. We will consider accessibility accommodations to applicants upon request. Check out our social goal to learn how we are working to build greater mutual trust and respect with the Indigenous Peoples in Canada.
Please note that our job postings are typically open for two weeks, so don't delay, apply now.
JOB CATEGORY: Business Professionals"
Principal Data Engineer (National Data Resource),NHS Wales Informatics Service,"- Glamorgan, ON","This role will work at the Welsh Ambulance Service Trust (WAST) as part of the National Data Resource programme.

NHS Wales is creating world-leading National Data Resource (NDR). The NDR is being developed to better enable health and care professionals to improve patient experience and service outcomes. The NDR aims to deliver a more joined up approach to health and care data, using common language and technical standards. It will improve the way data is collected, shared and used across health and care organisations in Wales and will drive forward the interoperability of health and care systems. The NDR will provide improved analytics capability and will enable better decision making for clinicians, operational managers, data scientists and other decision makers and users of data.

NHS Wales Informatics Service is currently working with Welsh Government, health boards, trusts and other stakeholders to deliver the ambitious programme. A delivery team is being established and this is an exciting opportunity to be part of a truly transformative programme across Welsh health and care.

The ability to speak Welsh is desirable for this post; Welsh and/or English speakers are equally welcome to apply.

Closing Date: 9th July 2020"
Data Engineer or Scientist,Bombardier,"- Dorval, QC","Data Engineer or Scientist-MON16971

Description

BOMBARDIER

At Bombardier, our employees work together to evolve mobility worldwide - one good idea at a time. If you have a good idea, we’ll provide the environment where it will thrive and grow into a great product or customer experience. Your ideas are our fuel.

In your role, you will:

Integrate internal A.I. solutions within evolving Business Intelligence Solutions;

Nurture strong working networks with internal and external stakeholders to access large variety of data sources and enhance A.I. value creation;

Challenge status-quo for data ecosystem and collaboratively innovate solutions to enable A.I. across teams;

Support A.I. teams across multiple business areas by maintain A.I. solution ecosystem;

Manage day-to-day operations within data management and analytics platforms (e.g. Azure / AWS / GCP); and

Articulate software and hardware requirements for A.I. solutions to non-practitioners.

Qualifications

As our ideal candidate,

At least 3 years of work experience designing and implementing AI/data science algorithms/systems and managing the associated data by leveraging, connecting and operationalizing large scale enterprise data solutions and applications using best-known data management and analytics platforms (e.g. Azure / AWS / GCP)

Experience working in an Agile development environment implementing AI/data science algorithms leveraging state-of-the-art programming languages and libraries (e.g. Python, R, Tensorflow, pytorch, mxnet, pandas) and understanding of development and service delivery/management frameworks (e.g. DevOps)

Masters and/or PhD credentials in data science related fields and Engineering background is desired.

Experience deploying A.I. solutions within business functions to improve business competitiveness.

Proven results while navigating through global organizational environments.

Organizational savviness and good communications skills.

Bombardier is an equal opportunity employer and encourages women, Aboriginal people, persons with disabilities and members of visible minorities to apply.

Whether your candidacy is moving on to the next step of the hiring process or not, we will keep you informed by email or by phone. Join us at careers.bombardier.com

Your ideas move people.

#LI-RK1

Job: System Integration
Primary Location: CA-QC-Montreal Dorval
Organization: Aerospace
Schedule: Full-time
Employee Status: Regular

Job Posting: 05.02.2020, 9:53:18 AM

Unposting Date: Ongoing"
Staff Data Engineer,Infoblox,"- Burnaby, BC","Infoblox is looking for a Staff Data Engineer to augment our growing Cyber Security Software Development Team. This growing team supports the Infoblox mission to thwart cybersecurity threats in our customer’s networks. This is an opportunity to work closely with data scientists and threat analysts to curate the data that makes this mission possible.
The ideal candidate is a savvy software engineer with experience in data engineering and a solid background in Spark and Python. Preferably you know that countMinSketch is not a children’s game. You are comfortable wearing several hats in a small organization with a wide range of responsibilities and have worked in a cloud environment, such as Amazon EMR. You know that Big Data is both a blessing and a curse; without good data engineering, it loses its potential. You are passionate about the nexus between data and computer science-driven to figure out how best to represent and summarize data in a way that informs good decisions and drives new products. When someone says, “my Spark job failed”, your first question is “what’s the skew?”. Come join our growing Cyber Threat Intelligence team and help us build world-class solutions!
Responsibilities:
Curate very large-scale data from a multitude of sources into appropriate sets for research and development for data scientists, threat analysts, and developers across the company.
Design, test, and implement storage solutions for various consumers of the data.
Design and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methods.
Leverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analytics.
Convert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelines.
Collaborate on the design, implementation, and deployment of applications with the rest of software engineering.
Support data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage data.
Build and maintain tools for automation, deployment, monitoring, and operations.
Create test plans, test cases, and run tests with automated tools.
Requirements:
7+ years of experience with Python3, and 2+ years experience with Spark. Scala experience is helpful.
5+ years of experience in data engineering, data science, and related data-centric fields using large-scale data environments.
3+ years of experience in using SQL and working with modern relational databases, including MySQL or PostgreSQL
3+ years of experience with developing ETL pipelines and data manipulation scripts
Proficient in Object-Oriented Design and S.O.L.I.D principles.
Strong emphasis on unit testing and code quality.
Proficient with AWS products (EMR S3, Lambda, VPC, EC2, API Gateway, etc).

Preferred Experience:
Very strong Python and PySpark experience.
Very strong back end development experience.
Strong experience with cloud deployments and CI/CD.
Experience with virtualization, containers, and orchestration (Docker, Kubernetes, XEN).
Experience with NoSQL Non-Relational databases (AWS DynamoDB).

Education:
MS or BS in Computer Science or a related field, or equivalent work experience required.


Perks:
Work with a world-class technology team in a rapidly growing company
A career path with opportunities to grow
Boutique office space with state of the art amenities, located in the heart of Metro Vancouver area; steps from SkyTrain and Metrotown Mall
Cross-functional break room stocked with snacks and beverages
And many, many more perks!

It’s an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies—and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team—our future looks bright, and so will yours. To check out what it’s like to be a Bloxer click here.

#LI-AB1"
Data Engineer,RBC,"- Toronto, ON","What is the opportunity?

The Data engineering team in I&TS will play a critical role to help with our transformation strategy. The team will centralize data sources, builds automated data pipelines, develop custom APIs and assist with building the analytical models. We are looking for an experienced data engineer who is quick to adapt to next generation technologies and enthusiastic about analyzing and automating data and building solid products and solutions

What will you do?

As part of the I&TS Data Engineering/Governance team you will support the implementation of the Data Governance strategy across multiple data sources across various geographies. This role will encompass an end to end view from data sourcing, lineage, quality, transformation and storage while, analytics and reporting.
Work in an Agile team of Data Engineers, Scientists, Developers and RBC business stakeholders.
Build container based solutions, APIs/Microservices and analytics portal for heterogeneous data sources on internal data lakes or cloud platforms
Collaborate with Data scientists, Process Engineers and Business Stakeholders to develop data pipelines, and assist with prescriptive and predictive analytics through consolidated data
Work with platform and infrastructure groups to configure and stand-up required hardware and software stacks.

What do you need to succeed?

Must Have:
2+ years of hands-on experience in following key technologies:
Data engineering solutions: Elasticsearch, Logstash, filebeats, Kafka, Python, Hadoop, Spark, SQLServer, mySQL, MongoDB
API: Node.JS, Django, and Microservices technologies
Security frameworks: LDAP, Kerberos, OAuth 2.0, Vault integration
Portal/analytics solutions: Angular, Kibana
Automation/DevOps: Jenkins, Selenium and similar technologies
Cloud technologies: Openshift, PCF, AWS, Azure
Source code control: GIT
Extensive experience working in agile/SaFe environment
Strong knowledge of algorithms and data structures.
Comfortable applying engineering best practices for Test Driven Development, integration testing, version control, Continuous Integration and Deployment
Undergraduate degree in Computer Science, Engineering or Mathematics

Nice to Have
Experience with Docker, Kubernetes, Cloud deployment.
Experience with Supervised and Unsupervised Machine learning.
Experience in the financial industry, especially trading related businesses.

What’s in it for you?

We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper
A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable
Leaders who support your development through coaching and managing opportunities
Ability to make a difference and lasting impact
Work in a dynamic, collaborative, progressive, and high-performing team
A world-class training program in financial services
Flexible work/life balance options
Opportunities to take on progressively greater accountabilities

JOB SUMMARY
City: Toronto
Address: 155 Wellington St W
Work Hours/Week: 37.5
Work Environment: Office
Employment Type: Permanent
Career Level: Experienced Hire/Professional
Pay Type: Salary + Variable Bonus
Position Level: PL08
Required Travel (%): 0-25
Exempt/Non-Exempt: N/A
People Manager: No
Application Deadline: 07/31/2020
Req ID: 253540"
Intermediate Cloud Data Engineer,Symend,"- Calgary, AB","Are you looking for an opportunity to apply your skills and talent to spark both innovation and positive social change? Symend is a rapidly growing fintech company that combines behavioral science with advanced AI, data, and analytics capabilities to enable companies to engage at-risk customers more effectively and humanely. By creating better ways to approach individuals with empathy and dignity at the time when they need it most, Symend’s employees are helping to transform the debt recovery industry and improve lives in North America and around the world.
This role is a good fit for you if:
You want to get into a fast-paced organization that is growing incredibly fast. Sales is outrunning our data engineering team and we need your help to catch up!
You’re the kind of person that likes a good challenge, is ready to roll up your sleeves and dig into the problem and can pump out some code that makes data dance for you.
You take pride in the readable code you write, add comments to make sure others can follow it and you’re ready to step up to help when it goes sideways.
You’re cool with inheriting code that you didn’t write. You empathize that someone in the history of this code had to balance their own constraints; it is what it is and you’re here to help make it better.
You appreciate the weekly mix of building solutions towards a roadmap alongside root cause analysis of problems as they arise.
You have a pod, so you don’t go it alone and you make sure your pod lead and your manager are aware of the issues that are cropping up and the approach you’re using to solve it.
You have a very collaborative attitude and you want to learn as you go!
What you’ll be doing
Build data ingest and propagation pipelines, including the SQL procs and Azure functions for transformations;
Write unit tests and develop code that will pass the tests and can be understood by others;
Perform code reviews and be comfortable with your code being reviewed by others;
Adhere to the design standards that exist, and apply new design standards as they are produced;
Perpetually look for areas of improvement with a focus on throughput speeds and automations;
Perform root cause analyses on bugs and on long-running queries and stages in the data pipelines;
Perform data mapping activities to describe source data, target data and the high-level or detailed transformations that need to occur;

Treat the product team as partners, and support them by:
only accepting work that is achievable in the sprint;
informing them when an urgent request disrupts the sprint;

Treat the QA team as partners, and support them by:
Rigorously testing your own code;
Identifying opportunities for test automation;
Making yourself available to the QA team during release testing;

Treat the Ops team as partners, and support them by:
Providing high-level workflows on how a pipeline is meant to run;
Using descriptive naming conventions that make it easy to follow the code;

Treat the client-facing teams as your data customers, and support them by:
Working with your team lead and manager to understand the challenges that they face;

Realize that it’s okay to make mistakes, if:
We learn from them, and
We flag them and inform our team leads when they happen
What you need
2 years of experience in Azure cloud platform, Azure data factory and Azure data functions;
2 years of experience with agile sprints;
5 years of experience as an integration developer in a SQL Server environment with Visual Studio and git branching patterns;
Experience with data modelling patterns (eg. ODS, Kimball, 3NF, DataVault, data virtualization);
Experienced in database development, data modelling and administration skills with SQL Server;
Experience in cloud data warehouse technologies (eg. Snowflake, Synapse) would be an asset;
Experience with SSAS and Power BI would be an asset;
Experience in python-based data development would be an asset;
BSc/BA in Computer Science, Engineering or relevant field would be an asset;
Why Symend?
We could tell you all about our competitive compensation, flexible work environment, beer Fridays, and awesome team events, but working at Symend is so much more than that.
Working at Symend means being part of a driven and collaborative team that values trust, accountability and continuous learning. We work hard, but always make time for fun!
You get the chance to do work that matters on a product that truly changes lives
You get unique opportunities to serve leading global businesses including Tier-1 telecom providers, major utility companies and Fortune 500 financial institutions
You get to make history as we disrupt the debt recovery industry for the good and change the way companies engage with their clients
Ready to do something big?
Send us your resume and tell us why you’d be a fit for Symend.
About Symend
Symend is transforming the debt recovery industry by treating individuals with empathy and dignity at the time when they need it most. Combining behavioral science with advanced AI, data, and analytics capabilities, Symend’s customer engagement platform enables service providers and financial institutions to develop positive, individualized treatment programs for their at-risk customers. By approaching past-due customers with both evidence-based insight and compassion, Symend’s clients experience higher cure rates and reduced costs while building long-term relationships with customers and lifelong value for their companies and brands. Symend: The science of engagement.™"
Senior Data Engineer,BELL RESIDENTIAL SERVICES,"- Montréal, QC","Req Id: 255052

At Bell, we do more than build world-class networks, develop innovative services and create original multiplatform media content – we’re revolutionizing how Canadians communicate.

If you’re ready to bring game-changing ideas to life and join a community that values bold ideas, professional growth and employee wellness, we want you on the Bell team.

Bell Residential and Small Business provides the most innovative, industry-leading Internet, TV and Smart Home products and services to Canadians. Whether we’re tackling a new challenge, implementing a big idea, or selling an innovative, new feature our team puts in the effort to be Canada’s #1 communications company.

The BRS Business Intelligence team collaborates with partners across the organization and provides strategic guidance supported with data. We work with the latest and greatest BI technologies like Teradata, SAS, Hadoop and Microstrategy. We enable better results through an integrated market view of customers and competition.

Reporting to the Senior Manager of Data Science, the Senior Data Engineer is responsible for the management and optimization of our data repositories used for our marketing campaigns, reporting and analytics. The Senior Engineer will work as part of a team of highly competent BI professionals who like to get things done.

In an ever-changing and dynamic environment, the candidate should be flexible, have strong time management skills and be capable of balancing many projects simultaneously with short deadlines for completion. The Senior Engineer will work in a cross-functional team environment and will be asked to coach other Engineers and provide technical support and guidance, ensuring industry best practices are employed.

Primary Responsibilities:
The Senior Developer will be responsible for processes and data residing in our SAS, Hadoop and Teradata environments. He or she will execute the following tasks:
Meet with Business, Content, Product, Marketing and Reporting stakeholders to understand business needs and relate these to data requirements;
Develop a deep, multilayered understanding of the business issues and the data challenges within BRS;
Lead and manage projects with minimal guidance (requirements-to-solution cycle);
Shapes and develops requirements into practical solutions with clear explanations of technical terms for a non-technical audience
Develop and modify ETL processes taking into consideration long-term implications;
Respond to ad-hoc requests in a timely manner;
Investigate data/process anomalies; propose and implement solutions;
Develop SAS, Teradata (stored procedures) and/or Hadoop processes;
Ensure the validity and integrity of data;
Manage database & table security access;
Work with other Developers and Business Analysts to ensure timely and successful project completion;
Take ownership and accountability for deliverables and committed deadlines, communicate proactively and escalate when required;
Coach and mentor fellow Engineers and provide technical support and guidance;
Acts as a subject matter expert with a deep understanding of tools capabilities and technology infrastructure..

Competencies / Skills:
A minimum of 8 years experience in ETL development in SAS, Teradata and/or PL/SQL;
Experience in data warehousing which include data mapping and transformation, data source analysis and data profiling;
SQL programming experience: creating complex queries, tables, views, and procedures;
Advanced knowledge of the following software: Enterprise Guide, Excel, PowerPoint, Word;
Hadoop knowledge is an asset;
Knowledge of the telecommunications business is an asset;
Demonstrable history of cross-team collaboration;
Advanced technical and non-technical communications skills;
Self motivated. Ability to research situations, identify the key issues, and then gain approval to address the opportunity from upper management.
Extremely rigorous with regards to the data and information provided;
Ability to work on his/her own, with a minimum of supervision within a dynamic environment;
Ability to work and manage multiple projects simultaneously;
Ability to diagnose business problem, document, resolve, plan and deliver solution in short time;
Well organized;
Invested in continual self-development to stay current with evolving technological landscape;
Customer driven.

Bilingualism is an asset (English and French); adequate knowledge of French is required for positions in Quebec.

Additional Information:
Position Type: Management
Job Status: regular - full Time
Job Location: Canada : Ontario : Don Mills || Canada : Ontario : Toronto || Canada : Quebec : Montreal || Canada : Quebec : Verdun
Application Deadline: 07/12/2020

Please apply directly online to be considered for this role. Applications through email will not be accepted.

At Bell, we don’t just accept difference - we celebrate it. We’re committed to fostering an inclusive, equitable, and accessible workplace where every team member feels valued, respected, and supported, and has the opportunity to reach their full potential. We welcome and encourage applications from people with disabilities.

Accommodations are available on request for candidates taking part in all aspects of the selection process. For a confidential inquiry, simply email your recruiter directly or recruitment@bell.ca to make arrangements. If you have questions regarding accessible employment at Bell please email our Diversity & Inclusion Team at inclusion@bell.ca.

Created: Canada, QC, Montreal
LI-SP1

Read more about why Bell is considered one of Canada's Top 100 Employers."
"Senior Big Data Engineer, Analytics and Machine Learning",Global Relay,"- Vancouver, BC","Your Role::

As a Senior Big Data Engineer in the Analytics and Machine Learning group you will provide technical expertise over development of a new, large (100+TB) green field data project. The project is for storing and extracting value from the results of Data Science and natural language processing. This role involves working closely with the Technical Program Manager and the Development Lead to understand the business requirements and translate the requirements into a scalable data system. The data system will be used for a wide range of analytics and visualizations. One of the primary use goals is to surface the results of Machine Learning algorithms in innovative ways.

About You:

5+ years of experience with a high level programming language like Java or Python
Expertise with a Large-scale Graph database ~ JanusGraph, DGraph, OrientDB, Neo4j
Expertise with a Large-scale Timeseries database ~ InfluxDB, RiakTS, M3DB
Expertise in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.
Expertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.
Expertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.
Experience working on systems with petabytes of data and millions of transactions per day
Experience developing, maintaining and scaling an ever growing big data pipeline
(Nice to have) Experience with microservices
(Nice to have) Experience with Kubernetes
About Us::

Global Relay is the leading provider of cloud-based archiving, supervision, eDiscovery, and analytics for the global financial sector. We deliver services to 23,000 customers in 90 countries, including 22 of the top 25 global banks. Our market-leading archiving service supports email, instant messaging, Bloomberg®, Thomson Reuters, social media, mobile messaging, and more – with mobile, Outlook, and web access.

Our Global Operations & Development Center is located in Vancouver, BC, Canada. In addition, we have offices in eight other cities across the world, including major financial centers like New York, Chicago, and London.

Over the years, we have won several major awards, including:
Company of the Year from the BC Tech Association (2016)
Canada’s 50 Best Small and Medium Employers (2014, 2015)
Canada’s Top Employers for Young People (2014)
Canada’s Top 10 Most Admired Corporate Cultures (2013, 2016)
Canada’s Best Managed Companies (2013 - present)
Technology Fast 50 – Leadership (2014, 2018)

We provide fantastic opportunities to individuals passionate about business and technology. For those with international business aspirations, we offer invaluable opportunities for doing business with some of the world’s largest, most influential firms. Our company is also perfect for those who want to create cool technology using massively scalable, big-data architecture, with a strong focus on mobile.

To learn more about our business, culture, and community involvement, visit www.globalrelay.com.

Think you're a great fit for this job? Apply today with your cover letter and resume."
Data Engineer,Guavus,"- Quebec City, QC","Guavus is a young and fast-growing company whose mission is to provide Communication Service Providers (CSPs) with a competitive advantage in the ability to accurately understand their mobile subscribers' behaviors and extract value from this knowledge.

We are at pivotal point in our history where big data innovation can impact businesses and individuals in new and unforeseen ways, but we need exceptionally smart people to join our team who are:
Passionate about getting the job done,
Relentless about flawless execution,
Committed to solving problems creatively, and
Believe in the collective intelligence to design, build and engineer extraordinary products and solutions that are useful to all.

If this sounds like you, please reach out we'd love to hear from you.

Your Role
Guavus is looking for a highly motivated and talented Sr. Data Engineer to participate in the development of the most advanced solutions in the Big Data space by using agile methodologies. The developer will actively participate and collaborate with data team to design and implement data pipelines integrating advanced AI/ML models.

Responsibilities
Develop and maintain batch and streaming data pipelines with big data technologies such as Spark, Kafka, Hive, HDFS, HBase, Phoenix, Impala etc.
Work closely with data scientists to produce ML/AI pipelines.
Analyze and implement proof of concepts related to big data technologies
Analyze new technologies (DB, Storage, Compute Engines)
Produce quality code that is well documented



Qualifications and Experience
Four (4) years of experience in a data engineer position
Holder of a Degree in Computer Science or Engineering
Experience in Cloud and non-Cloud based Hadoop ecosystem
Experience in data warehousing and ETL development
Fluent in Java & with some Scala knowledge.
Fluent in SQL
Experience in performant and highly scalable applications
Experience in distributed framework and technologies e.g. Columnar Database, NoSQL and Hadoop
Experience in Linux and shell scripting.
Basic knowledge or interest in Python
Fluency in English, both written and spoken
Speaking French is an asset



Working conditions
This opportunity consists of full-time job and is located in the Mile Ex area of Montreal, Canada."
Data Scientist - Machine Learning Engineer,Sonraí Security,"- Fredericton, NB","Sonrai Security, a cloud based security company, is looking for a ""hands on"" data scientist / machine engineer, focused on developing classification algorithms and analytics. This position will be a key contributor, wearing many hats, in a small development team. This is an amazing opportunity to be part a company building a new innovative platform. In this position you will be: Completely integrated with the engineering team working hand and hand with engineers to integrate machine learning into critical components of the Sonrai platformWorking and experimenting with open source, proprietary, and self developed data classification algorithms on a multitude of data formatsContributing and making decisions on all aspects of software development (including design, architecture and implementation) The desired candidate will have: MS/PhD in Computer Science or a related field3+ years of industry experience in data science and machine learning preferredApplied in Machine Learning with demonstrated experience in unsupervised classificationStrong CS fundamentals, such as algorithms and data structures.Expertise in Python & SparkExcellent written and verbal communication skillsEnthusiasm for working hard and having fun in a dynamic environmentMention MrRobot in your cover letter to let us know you actually read this section and you're not just spamming out your resume Desired, but not required, skills include: Experience with graph databases & analysisExperience with machine learning toolkits like TensorFlowExperience with docker and/or serverless technologies and/or cloud servicesExperience with a variety of data stores and identity and access management systemsKnowledge of secure coding practices"
Senior Software Engineer - Apple Cloud Services / Big Data,Apple,"- Toronto, ON","Summary
Posted: Jul 1, 2020
Weekly Hours: 40
Role Number:200145149
Do you love solving complex challenges? Are you an inventive self-starter who takes pride in making ideas come to life on a global scale? Are you passionate about developing big-data platforms using creative algorithms to process petabytes of data with very low latency? If so, join Apple Cloud Services (ACS) Big Data team to design and build a scalable big-data platform that is used across Apple. As part of Apple Cloud Services (ACS) you will have a meaningful role in designing, developing, and deploying high-performance systems that handle billions of queries every single day. This enormous scale brings challenges that require extraordinarily creative problem solving. By focusing on and respecting the customer’s needs, you’ll be responsible for helping us build the technology that works for so many customers around the world. We are looking for Engineers and Architects who are passionate about crafting big-data products. This role requires deep understanding of developing products that are highly scalable, highly available and fully fault tolerant. Understanding of various big-data technology concepts such as Parallel Processing in MapReduce or Spark, indexing technology used in searching, query processing using NoSQL technologies is a requirement. This is a small and exclusive technology team focused on addressing all the big-data processing needs for iCloud. We are developing a platform that can accomplish any task with an intuitive, simple to use user interface, and easy deployment to production. Come check us out!
Key Qualifications
Coding experience in Java, preferably with critical, large-scale distributed systems
Ability to design large-scale, complex applications with excellent run-time characteristics such as low-latency, fault-tolerance and availability
2+ years experience in Hadoop/Spark ecosystems
A learning attitude to continuously improve self, team, and the organization
Deeply concerned about excellence and quality
Loves fast paced environment and learning new technologies
Fluent knowledge with at least two of the following: Spark streaming, MapReduce, HDFS, Cassandra, Kafka
Description
Come join a small team writing the software which forms the foundation for some of our most exciting products and services. - Architect, design and build a framework that can ingest billions of records in a minute, can process and aggregate petabyte scale of data - Develop a big data infrastructure that is highly scalable, available with end to end monitoring capabilities - Advise data engineers and Analysts on solving their data access requirements - Develop and deploy algorithms that tackle big data problems
Education & Experience
Bachelors, Masters or PhD in Computer Science
APPLE IS AN EQUAL OPPORTUNITY EMPLOYER THAT IS COMMITTED TO INCLUSION AND DIVERSITY. WE ALSO TAKE AFFIRMATIVE ACTION TO OFFER EMPLOYMENT AND ADVANCEMENT OPPORTUNITIES TO ALL APPLICANTS, INCLUDING MINORITIES, WOMEN, PROTECTED VETERANS, AND INDIVIDUALS WITH DISABILITIES. APPLE WILL NOT DISCRIMINATE OR RETALIATE AGAINST APPLICANTS WHO INQUIRE ABOUT, DISCLOSE, OR DISCUSS THEIR COMPENSATION OR THAT OF OTHER APPLICANTS."
Data Engineer,RBC,"- Toronto, ON","What is the opportunity?

The Data engineering team in I&TS will play a critical role to help with our transformation strategy. The team will centralize data sources, builds automated data pipelines, develop custom APIs and assist with building the analytical models. We are looking for an experienced data engineer who is quick to adapt to next generation technologies and enthusiastic about analyzing and automating data and building solid products and solutions

What will you do?

As part of the I&TS Data Engineering/Governance team you will support the implementation of the Data Governance strategy across multiple data sources across various geographies. This role will encompass an end to end view from data sourcing, lineage, quality, transformation and storage while, analytics and reporting.
Work in an Agile team of Data Engineers, Scientists, Developers and RBC business stakeholders.
Build container based solutions, APIs/Microservices and analytics portal for heterogeneous data sources on internal data lakes or cloud platforms
Collaborate with Data scientists, Process Engineers and Business Stakeholders to develop data pipelines, and assist with prescriptive and predictive analytics through consolidated data
Work with platform and infrastructure groups to configure and stand-up required hardware and software stacks.

What do you need to succeed?

Must Have:
2+ years of hands-on experience in following key technologies:
Data engineering solutions: Elasticsearch, Logstash, filebeats, Kafka, Python, Hadoop, Spark, SQLServer, mySQL, MongoDB
API: Node.JS, Django, and Microservices technologies
Security frameworks: LDAP, Kerberos, OAuth 2.0, Vault integration
Portal/analytics solutions: Angular, Kibana
Automation/DevOps: Jenkins, Selenium and similar technologies
Cloud technologies: Openshift, PCF, AWS, Azure
Source code control: GIT
Extensive experience working in agile/SaFe environment
Strong knowledge of algorithms and data structures.
Comfortable applying engineering best practices for Test Driven Development, integration testing, version control, Continuous Integration and Deployment
Undergraduate degree in Computer Science, Engineering or Mathematics

Nice to Have
Experience with Docker, Kubernetes, Cloud deployment.
Experience with Supervised and Unsupervised Machine learning.
Experience in the financial industry, especially trading related businesses.

What’s in it for you?

We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper
A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation, commissions, and stock where applicable
Leaders who support your development through coaching and managing opportunities
Ability to make a difference and lasting impact
Work in a dynamic, collaborative, progressive, and high-performing team
A world-class training program in financial services
Flexible work/life balance options
Opportunities to take on progressively greater accountabilities

JOB SUMMARY
City: Toronto
Address: 155 Wellington St W
Work Hours/Week: 37.5
Work Environment: Office
Employment Type: Permanent
Career Level: Experienced Hire/Professional
Pay Type: Salary + Variable Bonus
Position Level: PL08
Required Travel (%): 0-25
Exempt/Non-Exempt: N/A
People Manager: No
Application Deadline: 07/31/2020
Req ID: 253540"
Data Engineer (Cloud),StackPros,"- Toronto, ON","StackPros Inc is seeking a candidate for a full-time role within our Data Systems Team in Toronto, Ontario.
The Cloud Data Engineer will play a key role at StackPros, required to help create and maintain industry-leading quality and efficiency of service and software delivery.
StackPros will rely on the Data Engineer to support the Data Systems team, in both data engineering and data science-related workflows. The Data Engineer will be expected to meet and exceed StackPros’ quality standards, while helping the organization rapidly expand complex Machine Learning and related applications.
Key Responsibilities:
Data Engineering-Specific Responsibilities
Participate in continuous delivery pipeline to fully automate deployment of the highly available cloud platform that supports multiple projects
Design and develop ETL workflows and datasets to be used in data visualization tools
Write complex SQL queries with multiple joins to automate and manipulate data extracts
Perform end to end Data Validation to maintain accuracy of data sets
Build tools for deployment, monitoring and operations
Troubleshoot and resolve issues in the development, test and production environments
Develop re-useable processes that can be leveraged and standardized for multiple instances
Prepare technical specifications and documentation for projects
Stay up-to-date on relevant technologies, plug into user groups, understand trends and opportunities to ensure we are using the best possible techniques and tools
Understand, implement, and automate security controls, governance processes, and compliance validation
Design, manage, and maintain tools to automate operational processes
Data Science-Specific Responsibilities
Perform exploratory data analysis to identify patterns from historical data, generate and test hypotheses, and provide product owners with actionable insights
Design experiments for product initiatives and perform statistical analysis of the results with recommendations for next steps and future experiments
Create and design dashboards by using different data visualization tools to present reports and insights, and support business decision making
Help the DRVN Intelligence Data Systems team adopt and evolve Predictive Modeling, Machine Learning and Deep Learning processes to deliver to clients in the future
Qualifications:
3+ years experience in Data Engineering
Understanding of digital ecosystems including online data collection, cloud systems and analytics tools (Google Stack, Facebook, AWS, Salesforce, Adobe Suite etc.)
Strong technical understanding of a range of marketing concepts such as cookie-based data collection, setting and leveraging audience segments, attribution modelling, AB/N & multivariate
Excellent written & verbal communication skills are essential; candidate should be comfortable presenting and participating in group discussions of concepts with internal and external stakeholders
Candidate must exhibit an analytical, detail-oriented approach to problem solving
Experience with Jira / Atlassian project management tools is an asset
Company-Wide Responsibilities:
Maintain and exceed client satisfaction with StackPros Inc.’s deliverables, day-to-day work and overall value as a partner
Cultivate opportunities for company growth, always seek areas where StackPros Inc.’s role could be expanded
Adapt to ever-changing client needs and expectations
Maintain dedication toward achieving excellence in StackPros Inc.’s delivery against client needs, and overall success as an organization
Be an enthusiastic, positive and generally awesome team mate, mentor & constantly curious learner"
Data Engineer,Guavus,"- Montréal, QC","Guavus is a young and fast-growing company whose mission is to provide Communication Service Providers (CSPs) with a competitive advantage in the ability to accurately understand their mobile subscribers’ behaviors and extract value from this knowledge.
We are at pivotal point in our history where big data innovation can impact businesses and individuals in new and unforeseen ways, but we need exceptionally smart people to join our team who are:
Passionate about getting the job done,
Relentless about flawless execution,
Committed to solving problems creatively, and
Believe in the collective intelligence to design, build and engineer extraordinary products and solutions that are useful to all.
If this sounds like you, please reach out we’d love to hear from you.
Your Role
Guavus is looking for a highly motivated and talented Sr. Data Engineer to participate in the development of the most advanced solutions in the Big Data space by using agile methodologies. The developer will actively participate and collaborate with data team to design and implement data pipelines integrating advanced AI/ML models.
Responsibilities
Develop and maintain batch and streaming data pipelines with big data technologies such as Spark, Kafka, Hive, HDFS, HBase, Phoenix, Impala etc.
Work closely with data scientists to produce ML/AI pipelines.
Analyze and implement proof of concepts related to big data technologies
Analyze new technologies (DB, Storage, Compute Engines)
Produce quality code that is well documented
Qualifications and Experience
Four (4) years of experience in a data engineer position
Holder of a Degree in Computer Science or Engineering
Experience in Cloud and non-Cloud based Hadoop ecosystem
Experience in data warehousing and ETL development
Fluent in Java & with some Scala knowledge.
Fluent in SQL
Experience in performant and highly scalable applications
Experience in distributed framework and technologies e.g. Columnar Database, NoSQL and Hadoop
Experience in Linux and shell scripting.
Basic knowledge or interest in Python
Fluency in English, both written and spoken
Speaking French is an asset
Working conditions
This opportunity consists of full-time job and is located in the Mile Ex area of Montreal, Canada."
Senior Data Engineer,BenchSci,"- Toronto, ON","BenchSci exponentially increases the speed and quality of life-saving research by empowering scientists with the world’s most advanced biomedical artificial intelligence to run more successful experiments. Backed by F-Prime and Google’s AI fund, Gradient Ventures, BenchSci uses machine learning to diagnose pharmaceutical R&D health from hidden patterns in procurement data. A turnkey application of AI with immediate, quantifiable impact, BenchSci now optimizes reagent procurement and experimental success in 15 of the top 20 pharmaceutical companies and over 4,300 leading academic centers globally.

We are currently seeking a Senior Data Engineer to join our Data Team. As part of the job, you will work on evolving our data models in several styles of datastores, improve internal tooling to allow data self-service, and operationalize production-grade data pipelines.
What you’ll do:
Scale data pipelines to allow data to go from research to platform as fast as possible
Develop data access mechanisms for downstream applications consumption
Model and maintain the data integrity of the System of Records
Manage sources which contain both semi-structured as well as unstructured data
Develop and apply suitable frameworks to detect data drift, and then calibrate and redeploy them to production seamlessly
Collaborate closely with other engineers to solve interesting and challenging data problems
Who we’re looking for:
5+ years working as a professional developer
Experience with SQL
Experience with cloud reference architectures and developing specialized stacks on cloud services
Expertise in Spark 2.x, Dataset/DataFrame API and performance tuning
Experience with R or Pandas
You have strong cross-team communication and collaboration skills
A team player who strives to see teammates succeed together
Bonus points for:
Background in Life Science
Experience in Python
Experience with Airflow or other workflow management systems in a distributed setup
Experience with graph data modeling and scaling graph databases
Experience with Kubernetes in production
Experience with microservice architecture patterns
What’s in it for you:
Competitive salary with company benefits from day one
Dedicated learning and development budget (conferences, courses, etc.)
An opportunity to help transform and improve scientific research with a fun, energetic, and supportive team
Quarterly team events, annual retreats, and regular lunch and learns
Fully stocked kitchen with healthy snacks
Onsite gym and showering facilities
Casual dress code in a creative office environment (we have our own botanist!)
Office located in the heart of downtown Toronto (College/Bathurst)

Here at BenchSci, these are our core values:
Focused: We focus on what will drive the greatest impact at all times.
Advancement: We believe in continuous growth, and discovering new ways to do things better. This applies to our product and business, but also to ourselves.
Speed: We recognize that without a sense of urgency, our team, our product and our mission lose their value.
Tenacity: What we’re trying to do isn’t easy, but we hire the best people, and give them the autonomy, tools, and resources to succeed. The hard work is up to them.
Transparency: We believe that sharing diverse ideas and information creates strong teams. Our success stems from research, collaboration, feedback, and trust.
BenchSci is an equal opportunity employer. We value diversity and are committed to fostering an inclusive environment. All four of our cofounders are immigrants to Canada, as are many of our employees. We welcome your fresh perspectives and ideas."
"Senior Data Engineer, Data Science","Phreesia, Inc.",- Ontario,"Phreesia is looking for a Senior Data Engineer who is passionate about the software engineering side of Data Science to join our Data Science Team; someone familiar with building production tooling to support the entire data journey, from data extraction and transformation to deploying and validating machine learning models. You will have a tremendous impact on the next generation of data infrastructure for our Life Sciences Department.
You will report to the Director, Strategy & Analytics and work mainly with data scientists to build software and improve workflows, and collaborate with our data engineering. You will design our data infrastructure, and use it to develop extensible analytics pipelines, tools and visualizations for our data science products. Some of the primary upcoming initiatives include extending our Spark distributed computing infrastructure, streamlining CI/CD pipelines for the team's data products as well as supporting the next generation of predictive modeling and forecasting tools.

What You Will Do:
Build fault-tolerant, scalable batch and real-time distributed data processing
Design and configure hosted and cloud-based data and machine learning infrastructure
Develop technology to more efficiently and effectively curate large amounts of unstructured data
Tool our systems for observability, including logging, metrics monitoring, and dashboarding
Build data pipelines to make key datasets available to both data scientists and analysts throughout the company
Design, develop, refactor, package, harden, and deploy software products (Python, R, Shiny, Flask, Docker, PySpark, Scikit-Learn)
Develop tools and automate workflows
Guide the data science team's adoption of new software and frameworks

What You Will Need:
Bachelor's degree or higher in computer science or related discipline
6+ years of experience building data ingestion infrastructure
Experience working in a data science team with a strong engineering culture
Have taken a leading role in delivering complex software systems all the way from design to production
Strong production SQL skills
Experience with Docker, machine learning and data science toolsets in Python & R
Professional command of Spark/PySpark other streaming data pipelines
Experience managing an entire data flow, ingesting data from a variety of sources including SQL, NoSQL, streams, and external APIs
Hands-on experience and understanding of graph and/or search databases is preferred
Experience with cloud service providers such as AWS, GCP or Azure is preferred
Familiarity with Apache Airflow or similar workflow orchestration platforms is preferred
Healthcare experience and a basic understanding of clinical terms is a plus

Who We Are:
At Phreesia, we're committed to helping healthcare organizations succeed in a fast-changing landscape—and we need smart, passionate people to help us do it. Our innovative SaaS platform offers our clients a suite of applications to manage the intake process, giving them the tools to engage patients, improve efficiency, optimize staffing and enhance clinical care.
Basically, what you do here matters, and hard work does not go unnoticed. Not only does Phreesia care about our clients, we also care about our employees. In fact, we're a three-time winner of Modern Healthcare magazine's Best Places to Work in Healthcare award. If you're interested in consistent feedback and recognition, defined career paths, and the opportunity to work with driven and engaged colleagues in a dynamic industry, this may be the right opportunity for you.

Benefits and Perks:
Variety of health plan options, dental/ vision coverage, and short/long-term and life insurance plans
401(k) savings plan (USA) or RRSP plan (Canada)
Flexible working hours
Unlimited vacation
Mobile phone stipend and Internet reimbursement
100% paid maternity leave to our U.S. employees, as well as a generous maternity benefit to our employees in Canada.
Tuition and certification reimbursement, as well as other professional development opportunities

We strive to provide a diverse and inclusive environment and are an equal opportunity employer."
Senior Data Engineer - PriceMetrix,McKinsey & Company,"- Toronto, ON","QUALIFICATIONS
8+ years of software development experience using Agile methods, especially Scrum
Post-secondary education in Computer Science or a similar discipline
Proven success in leading the engineering of data analytics software
Expertise in developing business intelligence (BI) and data warehousing solutions using Microsoft platform: SQL Server; Azure PaaS / IaaS
Design and optimize solutions to satisfy demanding performance, scalability, and security requirements
Strong knowledge of dimensional modeling and physical design of databases
Knowledge of advanced ETL/ELT design and development using Integration Services (SSIS)
Apply DevOps practices to data analytics solutions: automated deployment and testing of databases, ETL/ELT processes, and statistical calculations
Experienced in database maintenance, performance monitoring, and debugging
Objectively assess potential solutions by balancing business value and technical complexity
Effectively communicate business benefits and implications of technical initiatives to non-technical and more senior colleagues
Highly organized and able to balance multiple parallel projects.
WHO YOU'LL WORK WITH
As a key member of the Product Development team at PriceMetrix, you will work in our office in downtown Toronto. You will collaborate with a talented team of Data Engineers, Web Engineers, QA Analysts, Product Managers, and others.
PriceMetrix is the first choice in practice management software for North American wealth managers. Our solutions use analytics and industry benchmarking to help advisers and their firms better serve their clients and grow their businesses. PriceMetrix directly measures aggregated data representing 10 million investors, $4 trillion in investment assets, 500 million transactions, and 2.5 million fee-based accounts. PriceMetrix combines its patented process for collecting and classifying data with proprietary measures of revenue, assets, and households to create the largest and most insightful retail wealth management data set available today.
WHAT YOU'LL DO
You will lead the engineering of major enhancements to our SaaS business intelligence platform.
As a member of a Scrum team, you will translate business requirements into pragmatic technical solutions. You will facilitate collaborative problem-solving to determine technical design, engineering tasks, and effort estimates; implementing, reviewing, and testing code changes; writing documentation.
You will partner with Infrastructure/IT and Platform Operations teams to improve deployment, configuration, operational efficiency, and troubleshooting.
You will contribute to the professional development of colleagues and improve the engineering methods by providing coaching and structured training and by proactively identifying and advocating for improvements."
Data Engineer,Telesat,"- Ottawa, ON","About Us
Backed by a 50-year legacy of engineering excellence, reliability and industry-leading customer service, Telesat has grown to be one of the largest and most successful global satellite operators. Telesat works collaboratively with its customers to deliver critical connectivity solutions that tackle the world’s most complex communications challenges, providing powerful advantages that improve their operations and drive growth.
In addition to our state-of-the-art global, geostationary satellite fleet, Telesat LEO, our Low Earth Orbit network scheduled to begin service in 2022, will revolutionize global broadband connectivity by delivering a combination of high capacity, security, resiliency and affordability with ultra-low latency and fiber-like speeds.
Telesat also provides industry-leading technical consultation and support services to satellite operators, insurers and other industry stakeholders around the globe.
Privately held and headquartered in Ottawa, Canada, with offices and facilities around the world, Telesat’s principal shareholders are Canada’s Public Sector Pension Investment Board and Loral Space & Communications Inc. (NASDAQ: LORL). For more information, visit www.telesat.com.
About the Role
The Data Engineer will be responsible for working with the LEO Prime to implementing the data warehouse, including an operational data analytics platform and Telesat’s LEO data warehouse. They will be primarily focused on the data extraction, ingestion and transformation of data from the various LEO segments and ensuring it is made available in the data warehouse for other teams and data analytics applications to use.
Main Responsibilities
The role has 2 primary responsibilities:
Operational platform & data warehouse: Working with the Data Analytics team, build out the operational data analytics platform and data warehouse on Telesats’ Cloud provider.
ELT: Extract, Load, Transform – working with Telesat’s LEO partner, build systems and processes to extract the data from the various segments, load it into the data warehouse(s), and transform it as required so that other systems, Data Scientists & Analysts and applications can use the data for operational, business and commercial purposes.
Education & Experience Required
2-5 years’ experience in implementation of data management, analysis and warehousing strategies that have positively impacted business objectives.
Bachelor's degree preferred - Computer Science with specialization in statistics, data analytics, or related discipline preferred.
Excellent communication (verbal, written, presentation) and interpersonal skills.
Communicates well with own team and across organizational boundaries to ensure the successful completion of shared goals.
You are advanced in the use of MS Excel to drive data driven reports to influence decisions. This would include creating/maintaining macros, pivot tables & charts.
Experience with various Data analytics tools (Scripting, R, Python, SQL, Power BI, Tableau, Databricks, and others)
Experience building data analytics systems based on cloud (GCP, AWS, Azure) technologies
In depth experience with storage management, including retention policy, security policy and backup management in a cloud environment
Agile development and Machine Learning experience considered a strong asset.
Decision Making & Supervision
Decision Making
In this role, the incumbent will have some influence on the architecture of the data storage, vendors involved, and infrastructure for the LEO program. They will need to make recommendations and decisions based on their experience.
Supervision Exercised
None
The successful candidate must be able to work in Canada and obtain clearance under the Canadian Controlled Goods program (CGP)."
Data Engineer,Cisco Systems,"- Calgary, AB","Are you excited by the challenge of protecting people against advanced computer security threats? Do you have the programming skills and experience to improve the advanced detection capabilities of our cloud security platform?
Who You’ll Work With

Billions of times a day, computers around the world communicate with the Cisco Advanced Malware Protection Cloud and rely on the AMP product to protect them against advanced forms of malware. Some malware is straightforward to identify, but antivirus has done that forever. Our customers need protection against malware that’s tricky to identify:
Viruses that generate different binaries on every machine they infect
Things that seem benign until you start looking closely at their behaviour
Malicious programs that have invented new ways of hiding themselves
To make the problem even more interesting: it may only become clear after a few days that a program or behavior was malicious. Does that sound fun to you?
The AMP Data Team is responsible for running advanced malware identification algorithms on incoming event data streams and storing and indexing that data. We index that data both for future detailed investigations of malware incidents, and to retrospectively detect previously unidentified malware in stored data. We strive for sub-second processing latencies in our streaming platforms and databases as they handle data volumes of >100TB/month and growing.
What You’ll do

Our team is looking for a backend Software Developer who will help scale our infrastructure as the business grows and continuously improve the software and infrastructure stack. You will also design and maintain automation and monitoring tools while working closely with Operations, QA and Production Support teams.
You will be tasked with maintaining a growing big data stack that includes technology such as:
Kafka
Cassandra
MongoDB
Flink
Storm
ElasticSearch Who You Are

You are self-motivated, results-driven and engaged. You’re passionate about back-end development and enjoy collaborating in a team-based environment. You have fun learning new technologies. You want to get to the bottom of things, understand what’s going on, and make a difference. You are someone who doesn’t give up when faced with complex problems.
You have 5+ years of experience writing high performant, production-quality code
Enjoys mentoring new hires and junior developers
You are able to evaluate technology choices based on use cases and performance needs
Have experience with modernizing legacy systems
Able to debug, diagnose, and resolve production problems
Familiar with AWS or similar cloud infrastructure
Experience with Linux command-line and system administration basics
Experience with CI/CD tools (e.g. GoCD, Jenkins)

Experience with Docker Nice to have:
Experience with CloudFormation, AWS Cloud Development Kit, Terraform, or equivalent
Experience with any of: Java, Scala, Go
Exposure to streaming platforms like Storm, Flink, or Kafka Streams
Background with distributed databases like Mongo, Cassandra or ElasticSearch
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Passion for technology and world changing? Be you, with us!"
Senior Data Engineer,BC IMC,"- Victoria, BC","DEPARTMENT DESCRIPTION
The Technology department is responsible for developing technology solutions that contribute to the achievement of BCI’s mission and long-term goals. The department manages the Corporation’s business applications and information technology infrastructure, providing support to a large group of financial professionals. The department is also responsible for authoring technology-related directives and conducting disaster recovery planning to minimize risk to the Corporation’s delivery of investment services.
The Data & Analytics function is responsible for the governance, architecture and engineering of BCI’s data assets. It also provides reporting, insights and data science capabilities to its customers.
POSITION DESCRIPTION
Reporting to the Director, Data & Analytics, the Senior Data Engineer is responsible for the design, development and implementation of the data and analytics products and projects that enable data science efforts in the organization. The Senior Data Engineer will deliver business value to multiple business areas across the organization and works closely with internal Technology and business area stakeholders. S/he will drive data modernization and innovation, and contributes to a strong data and analytics competency for BCI.
The position can be based in either Vancouver or Victoria with travel between the two cities.
QUALIFICATIONS
MUST HAVE:
Bachelor’s Degree in Technology, Computer Science, Mathematics or a related discipline
A minimum of 5 years of experience as a Data Engineer or Software Engineer professional
Experience with data warehouse and data lake design, development and sustainment
Coding skills and deep proficiency with SQL, Python, etc.
Competent with general scripting/software development
Understanding of data processing performance concerns and issues (configuring database server/data schema for performance, optimizing SQL)
Experience with reporting tools (e.g. Excel, Power BI, Tableau)
Experience with version control systems (e.g. Git)
Experience with cloud platforms
Strong knowledge of data modeling, data architecture and data structures
Strong understanding Agile and DevOps, including CI/CD technologies and practices
Excellent listening, communication, collaboration and problem-solving skills
PREFERRED:
Knowledge of the investment management industry
PRIMARY RESPONSIBILITIES
Collaborates with team members, other IT teams, and customers to understand the organization’s business objectives, data needs and infrastructure needs
Provides technical leadership and creates a culture of customer-centricity, accountability and high performance
Designs, develops and implements the data pipelines and ETL tools and workflows that enable data science efforts within the organization
Works with Data Architects and Data Scientists on the design, development and implementation of operational, transactional and analytical modeling
Proactively identifies risks and issues and proposes solutions to remove barriers
Applies knowledge of DevOps practices including continuous deployment, continuous integration, test-driven development and automated testing
Mentors junior engineers, follows best practices, performs code reviews and architects resilient infrastructure
Solves challenging problems about scale, statistics, infrastructure reliability, latency and more
Leads data mining and collections procedures
Robustly sources, structures, profiles, validates and transforms data for reporting, analysis and data science purposes
Engages with stakeholders to define, design and deliver data sourcing, analysis and reporting solutions
Makes recommendations about the methods used to collect, analyse and manage structured and unstructured data to drive outcomes
Develops solutions (and code) to automate and productionize data sourcing, data structuring and analytical modelling
Analyzes data sources, evaluating and remediating data quality, designing and implementing data sets that can be consumed and re-used by the analytics community across BCI
Helping the business interpret the results of analyses to determine the appropriate course of action
Proactively identifies opportunities to utilize data and analytics to business advantage and prototyping for ‘proof of value’
Assists in troubleshooting and guiding resolution of data analytics related problems in a timely and accurate fashion
Undertakes special projects or assignments as required
Performs other related duties as required
COMPETENCIES
Learning Agility
Effective performers continuously seek new knowledge. They are curious and want to know “why”. They learn quickly and use new information effectively. They create and foster a culture of interest, curiosity, and learning.
Relationship Building
Effective performers establish and proactively maintain a broad network of relationships (e.g. colleagues, co-workers, vendors, suppliers, etc.). They value these relationships and work effectively across the organization by maintaining positive working relationships with peers and others.
High Standards
Effective performers possess a high inner work standard and shows pride in their work. They consistently strive to ensure work is complete within deadlines and that all work performed is of a high quality.
Organization & Planning
Effective performers have strong organizing and planning skills that allow them to be highly productive and efficient. They manage their time wisely and effectively prioritize multiple competing tasks. They follow through on tasks to ensure changes in technology are communicated effectively.
Results Orientation
Effective performers maintain appropriate focus on outcomes and accomplishments. They are motivated by achievement, and persist until the goal is reached. They convey a sense of urgency to make things happen. They respect the need to balance short- and long-term goals. They are driven by a need for closure.
Communicativeness
Effective performers clearly and articulately convey technical and other information both orally and in writing to others in a manner appropriate to the listener. They write clearly, accurately and concisely, composing project, technical and other required documentation as required.
Change Mastery
Effective performers are adaptable. They embrace needed change and modify their behaviour when appropriate to achieve organizational objectives. They are effective in the face of ambiguity. They understand and use change management techniques to help ensure smooth transitions.
Business Thinking
Effective performers see the organization as a series of integrated and interlocking business processes. They understand how their work connects with and affects other areas of the organization."
"Big Data Engineer, Decision Analytics Services",EXL Service,"- Toronto, ON","Overview:
Big Data Engineer

EXL (NASDAQ:EXLS) is a leading operations management and analytics company that helps businesses enhance growth and profitability in the face of relentless competition and continuous disruption. Using our proprietary, award-winning Business EXLerator Framework™, which integrates analytics, automation, benchmarking, BPO, consulting, industry best practices and technology platforms, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 24,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), Latin America, Australia and South Africa.

EXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients’ decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries.

Please visit www.exlservice.com for more information about EXL Analytics.

Role Overview
Data Engineer will be part of core big data technology and design team. Person would be entrusted to developed solutions/design ideas, identify design ideas to enable the software to meet the acceptance and success criteria. Work with architects/BA to build data component on the Big data environment.
Responsibilities:
As a key member of the technical team alongside Engineers, Data Scientists and Data Users, you will be expected to define and contribute at a high-level to many aspects of our collaborative Agile development process:
Software design, development, automated testing of new and existing components in an Agile, DevOps and dynamic environment
Promoting development standards, code reviews, mentoring, knowledge sharing
Product and feature design, scrum story writing
Data Engineering and Management
Product support & troubleshooting
Implement the tools and processes, handling performance, scale, availability, accuracy and monitoring
Liaison with BAs to ensure that requirements are correctly interpreted and implemented. Liaison with Testers to ensure that they understand how requirements have been implemented – so that they can be effectively tested.
Participation in regular planning and status meetings. Input to the development process – through the involvement in Sprint reviews and retrospectives. Input into system architecture and design.
Peer code reviews.
3rd line support.
Qualifications:
Master/ BA/ BS degree in mathematics, engineering, computer science or related areas is preferre
8+ years professional software development experience and at least 4 years within Big data environments
4+ years of programming experience in Java, Scala, and Sparks
Proficient in SQL and relational database design.
Agile and DevOps experience – at least 2+ years
Experienced in Java or Scala and/or Python, Unix/Linux environment on-premises and in the cloud
Experienced in construction of robust batch and real-time data processing solutions on hadoop
Java development and design using Java 1.7/1.8. Advanced understanding of core features of Java and when to use them
Experience with most of the following technologies (Apache Hadoop, Scala, Apache Spark, Spark streaming, YARN, Kafka, Hive, HBase, Presto, Python, ETL frameworks, MapReduce, SQL, RESTful services).
Sound knowledge on working Unix/Linux Platform
Hands-on experience building data pipelines using Hadoop components Sqoop, Hive, Pig, Spark, Spark SQL.
Must have experience with developing Hive QL, UDF’s for analysing semi structured/structured datasets
Experience with time-series/analytics db's such as Elasticsearch or no SQL database.
Experience with industry standard version control tools (Git, GitHub), automated deployment tools (Ansible & Jenkins) and requirement management in JIRA
Exposure to Agile Project methodology but also with exposure to other methodologies (such as Kanban)
Understanding of data modelling techniques using relational and non-relational techniques
Coordination between global teams
Experience on Debugging the Code issues and then publishing the highlighted differences to the development team/Architects
Nice to have: ELK experience. Knowledge of cloud computing technology such as Google Cloud Platform(GCP)
What We Do
EXL Analytics offers an exciting, fast paced and innovative environment, which brings together a group of sharp and entrepreneurial professionals who are eager to influence business decisions. From your very first day, you get an opportunity to work closely with highly experienced, world class analytics consultants.You can expect to learn many aspects of businesses that our clients engage in.
You will also learn effective teamwork and time-management skills - key aspects for personal and professional growth
Analytics requires different skill sets at different levels within the organization. At EXL Analytics, we invest heavily in training you in all aspects of analytics as well as in leading analytical tools and techniques.
We provide guidance/ coaching to every employee through our mentoring program wherein every junior level employee is assigned a senior level professional as advisors.
Sky is the limit for our team members. The unique experiences gathered at EXL Analytics sets the stage for further growth and development in our company and beyond.
""EOE/Minorities/Females/Vets/Disabilities"""
Data Science Engineer,Alltech Consulting Services,"- Montréal, QC","POSITION DETAILS
Technology:

The Technology division partners with our business units and leading technology companies to redefine how we do business in ever more global and dynamic financial markets. Our sizeable investment in technology results in leading-edge tools, software, and systems. Our insights, applications, and infrastructure give a competitive edge to clients' businesses and to our own.

Technology Risk (TR) enables the Firm to manage risks through implementing proactive, comprehensive and consistent risk management practices which protect the franchise while capturing business opportunities. The TR team partners with the business by ensuring that Technology understands how to manage, escalate and monitor risk.

Position Description:

The Access Management (AM) Team within Technology Risk (TR) department is responsible for engineering and developing access management solutions for the firm. The AM platform is built using Service Oriented Architecture (SOA) and consists of several proprietary software products to manage and enforce entitlements within and outside the firm. Services are exposed using open standards including Restful JSON and SOAP. The server side products within the platform leverage Java, Sybase and DB2 with client-facing APIs in Java, C++, C# and Perl. Our UI layer utilizes AngularJS.

The successful candidate will act as a data scientist who understands the domain, is able to work with data to draw insights and visualize and present results in a way that helps us improve operational processes to effectively manage and reduce the risk of over entitlement given the size and dynamic nature of entitlement data. The position is required to build AI/ML solutions to assist in the development of entitlement analytic tools. These tools will be used to simplify entitlement reviews and help approvers in effective decision making by providing metrics. Observing the reviewers behavior in smaller, but more focused decisions will allow us to train models that will assist in improving the quality of decisions and managing access management risk in the future. The position involves skills in a number of emerging technologies, including big data and data mining solutions to identify data of interest to our models, AI/ML methodologies that will develop and train the models and UX expertise in how to present this information to users in a way that will enable them to make effective decisions as well as manage and control bias.

Skills that would be critical for the role are Python/R/Java and Apache suite of products such as Spark/Kafka/NiFi.. Knowledge and experience in machine learning libraries is required. Work with Tensorflow/Keras/scikit-learn/StatsModels or other similar libraries is also desired.

The position is based in Montreal and will require interaction with teams in Shanghai, India and New York. We're looking for a hands-on Developer who is smart, analytical, passionate and innovative."
Data Engineers - Hadoop,State Street,"- Vancouver, BC","Data Engineers
We are hiring Data Engineers in multiple locations to join our Currenex teams in (Redwood City, CA , Vancouver, BC CA or New York). Currenex is a market-leading technology provider offering the FX community high-performance technology and deep pools of liquidity for anonymous and disclosed trade execution. We provide our clients with a unique service— tailored access to disclosed and undisclosed liquidity together on a single screen. And with many significant market liquidity sources connected to our venue, we’re a premium source for FX liquidity in the market.
What you will do
Build, maintain and Continuously improve data management systems that are on our premises
Experience building and optimizing data pipelines and stream processing architectures of highly scalable big data stores
Implement data reliability, efficiency, and quality improvements.
Perform data modeling activities.
Take high level design from data architects or lead data engineers and translate them into detailed design.
Interact directly with business users to take their requirements and translate them into detailed design.
What we value
Your background demonstrates:
5 + years min experience within Data Engineering and Analytics
You possess experience in data integration , data lake , or data warehouse projects.
2-3 yrs proven experience in Spark, SQL, Python and Kafka
Experience in the Hadoop ecosphere, (Oozie, Hue, kudu, Hive ).
Bonus Points for
Coding proficiency in Java and Scala a plus
AWS preferred
Experience working with code repositories and continuous integration (i.e. Git, Bitbucket , etc..) also a plus"
Data Engineer,Capgemini,"- Brampton, ON","Job Description:
3-6 years
Expertise on Spark Scala.
Ability to develop ETL jobs to implement business logic using Scala (Spark Framework)
Conversant with Hive Database, Able to create HQL scripts and work on Hive tables for data analysis
Performance tuning of the existing Hadoop jobs, able to trouble shoot and fix existing bugs.
Good understanding of Oracle Exadata RDBMS, able to profile telecom data residing in Exadata and derive business rules.
Co lace with business, have working session with business to identify and freeze business logic.
Understanding / experience working on scrum based Agile set up.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
