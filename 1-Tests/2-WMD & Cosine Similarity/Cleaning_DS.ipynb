{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1-Importando librerias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fedricio/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fedricio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as  plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Dataset Puestos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>BSc/MSc/PhD in Physics, Mathematics, Biomedica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning Engineer 2</td>\n",
       "      <td>Collaborate with a multidisciplinary team to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Master’s degree or above in a STEM field, incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist 2</td>\n",
       "      <td>\\nReporting to the Director, Data &amp; Analytics,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Stack Web Developer</td>\n",
       "      <td>\\n\\n    Graduate Degree in Information Technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Full Stack Web Developer 2</td>\n",
       "      <td>\\n· Enter existing website codebases and exten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HCM Consultant</td>\n",
       "      <td>\\nThe Oracle Cloud HCM Absence Consultant will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HCM Consultant 2</td>\n",
       "      <td>\\nConducting Fit/Gap analysis and business /pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Security Specialist</td>\n",
       "      <td>Work in a fast-paced environment that combine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Security Specialist 2</td>\n",
       "      <td>\\n    Handling incoming requests for assistanc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Job_Title  \\\n",
       "0    Machine Learning Engineer   \n",
       "1  Machine Learning Engineer 2   \n",
       "2               Data Scientist   \n",
       "3             Data Scientist 2   \n",
       "4     Full Stack Web Developer   \n",
       "5   Full Stack Web Developer 2   \n",
       "6               HCM Consultant   \n",
       "7             HCM Consultant 2   \n",
       "8          Security Specialist   \n",
       "9        Security Specialist 2   \n",
       "\n",
       "                                     Job_Description  \n",
       "0  BSc/MSc/PhD in Physics, Mathematics, Biomedica...  \n",
       "1  Collaborate with a multidisciplinary team to g...  \n",
       "2  Master’s degree or above in a STEM field, incl...  \n",
       "3  \\nReporting to the Director, Data & Analytics,...  \n",
       "4  \\n\\n    Graduate Degree in Information Technol...  \n",
       "5  \\n· Enter existing website codebases and exten...  \n",
       "6  \\nThe Oracle Cloud HCM Absence Consultant will...  \n",
       "7  \\nConducting Fit/Gap analysis and business /pr...  \n",
       "8  Work in a fast-paced environment that combine ...  \n",
       "9  \\n    Handling incoming requests for assistanc...  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Jobs = pd.read_csv(\"Archivos 2-Especificos/Ejemplo1_Dataset_CVs_And_Job_Desc/EN/Job_Descr/Job_Positions_Scrapped/10_Positions_Only_Qualif.csv\")\n",
    "df_Jobs = df_Jobs.rename(columns={'job_title': 'Job_Title', 'job_description':'Job_Description'})\n",
    "\n",
    "df_Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Dataset CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de CVs extraidos: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_Name</th>\n",
       "      <th>Content_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_Chandler</td>\n",
       "      <td>Chandler Bing\\n \\nchandler.bing@vit.ac.in\\n \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_MeghnaLohani</td>\n",
       "      <td>\\n \\n \\n \\n \\nMeghna Lohani\\n \\nCampus Addres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_AmanSharma</td>\n",
       "      <td>Aman Sharma\\n \\nCampus Address                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Federico_Calonge_HCM</td>\n",
       "      <td>\\n                                           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_VAISHALI BIJOY</td>\n",
       "      <td>VAISHALI BIJOY\\n \\nVAISHALI.BIJOY2016@vitstude...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Test_Phoebe Buffay</td>\n",
       "      <td>Phoebe Buffay\\n \\n \\nPhobe.buffaycat@vit.ac.in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Candidate_Name                                         Content_CV\n",
       "0         Test_Chandler  Chandler Bing\\n \\nchandler.bing@vit.ac.in\\n \\n...\n",
       "1     Test_MeghnaLohani   \\n \\n \\n \\n \\nMeghna Lohani\\n \\nCampus Addres...\n",
       "2       Test_AmanSharma  Aman Sharma\\n \\nCampus Address                ...\n",
       "3  Federico_Calonge_HCM   \\n                                           ...\n",
       "4   Test_VAISHALI BIJOY  VAISHALI BIJOY\\n \\nVAISHALI.BIJOY2016@vitstude...\n",
       "5    Test_Phoebe Buffay  Phoebe Buffay\\n \\n \\nPhobe.buffaycat@vit.ac.in..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Leemos los CVs almacenados en nuestra carpeta y los extraemos uno por uno mediante la libreria PyPDF; devolviendonos\n",
    "#una secuencia de strings.\n",
    "import PyPDF2\n",
    "import os\n",
    "import collections\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "pathCVs='Archivos 2-Especificos/Ejemplo1_Dataset_CVs_And_Job_Desc/EN/PDFs_CVs' #Ruta Relativa, ubicacion de la carpeta.  #Antes, ruta absoluta: r'C:\\Users\\calon\\Desktop\\Notebooks\\Resume-Scoring-using-NLP-master\\Resume-Scoring-using-NLP-master\\Resumes'\n",
    "onlyfiles = [os.path.join(pathCVs, f) for f in os.listdir(pathCVs) if os.path.isfile(os.path.join(pathCVs, f))]\n",
    "\n",
    "print(\"Cantidad de CVs extraidos:\", len(onlyfiles))\n",
    "\n",
    "#Funcion para extraer las palabras del CV:\n",
    "import collections\n",
    "\n",
    "def pdfextract(file):\n",
    "    page_content = \"\"\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    c = collections.Counter(range(number_of_pages))\n",
    "    for i in c:\n",
    "        page = read_pdf.getPage(i)\n",
    "        page_content += page.extractText()\n",
    "    return (page_content.encode('utf-8'))\n",
    "\n",
    "def extract_text(file):\n",
    "    text = pdfextract(file).decode('utf-8')\n",
    "    return text\n",
    "\n",
    "#Obtenemos todas las palabras del CV sin preprocesar ni nada:\n",
    "df_Candidates=pd.DataFrame(columns = ['Candidate_Name','Content_CV'])\n",
    "i=0\n",
    "while i < len(onlyfiles):\n",
    "    file=onlyfiles[i]\n",
    "    base = os.path.basename(file)  #Test_Phoebe Buffay.pdf\n",
    "    filename = os.path.splitext(base)[0]  #Test_Phoebe Buffay\n",
    "    dat=extract_text(file)\n",
    "    data = [{'Candidate_Name':filename, 'Content_CV':dat}]\n",
    "    df_Candidates=df_Candidates.append(data, ignore_index=True)\n",
    "    i+=1\n",
    "\n",
    "df_Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpieza De los CVs**:\n",
    "1. Limpieza \"general\": Pasar todo a minúscula, eliminar signos de puntuación, espacios en blanco.\n",
    "2. Limpieza de stop words.\n",
    "3. Steaming para unir palabras: computer_science, machine_learning.\n",
    "4. Lematización. (VER)\n",
    "5. Limpieza en txt de datos personales (mails, telefonos).\n",
    "6. Limpieza de palabras innecesarias en archivo txt (Page, meses, años, etc).\n",
    "\n",
    "**Limpieza de los Puestos**:\n",
    "1. Limpieza \"general\": Pasar todo a minúscula, eliminar signos de puntuación, espacios en blanco.\n",
    "2. Limpieza de stop words.\n",
    "3. Steaming para unir palabras: computer_science, machine_learning.\n",
    "4. Lematización. (VER)\n",
    "5. Limpieza de palabras innecesarias en archivo txt (VER: por ej. mails que dejan las empresas, telefonos, etc.).\n",
    "6. Luego, para la obtención de Keywords (VER si es necesario): --> Por el problema que me tomaba las keywords como \"you\", \"I\" y demás..., podría hacer un txt con lenguajes y skills por \"defecto\" y me fijo si están en las keywords... sino no las pongo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para limpiar datos en columnas de un DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/fedricio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import regex as re #Usado en la función remove_punctuation_and_special_characters.\n",
    "from nltk.stem import WordNetLemmatizer   #Usado para lematización.\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize   #Usado para tokenizar.\n",
    "\n",
    "#Definimos nuestras stop words desde nltk:\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "                \n",
    "def lower_text(DF,clean_column):\n",
    "    DF[clean_column] = DF[clean_column].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "    \n",
    "def remove_stop_words(DF,clean_column):\n",
    "    DF[clean_column] = DF[clean_column].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "def remove_punctuation_and_special_characters(DF,clean_column):\n",
    "    DF[clean_column] = DF[clean_column].apply(lambda x: re.sub('[^\\w-]|(\\d+)',' ', x) if isinstance(x,str) else x)\n",
    "                                                                #Macheamos cualquier non-word excepto el -; y tambien macheamos los números (\\d+)\n",
    "    #Macheos:\n",
    "        #'\\W+'                  --> Cualquier non-word.\n",
    "        #'[()!@#$/,;.:\"]'       --> todos esos carácteres especiales que pongo ahi. \n",
    "        #'\\w+[-]\\w+'            --> machea a las palabras non-algo.\n",
    "        #'[^\\w-+]'              --> Machea todas las non-words excepto el guion medio y el +.\n",
    "        #'\\C\\+\\+'               --> Machea C++.\n",
    "        #[^\\w-+]|(\\C\\+\\+|team)  --> Machea todas las non-words excepto el guion medio y el +. Y tambien machea C++ y la palabra \"team\"\n",
    "        #[^\\w-+]|(\\C\\+\\+|\\d+)   --> Machea todas las non-words excepto el guion medio y el +. Y tambien machea C++ y los números (\\d+).\n",
    "        \n",
    "    #Para m.sc' --> reemplazar el . por espacio cuando antes y despues del punto esta todo \"junto\"... para diferenciarlo del punto de una oración. \n",
    "    \n",
    "#Tokenizar es el proceso de parsear Strings de texto en diferentes secciones (\"tokens\"). Debemos tokenizar teniendo en cuenta qué hacer con las comas, con los espacios en blanco, \n",
    "#con los \"-\", etc. Para esto usamos la función \"word_tokenize\" que se encargará de esta tarea. \n",
    "#Esta funcion NO divide las palabras que estan con un \"-\"... de esta manera la palabra \"scale-up\" queda así y NO se divide.\n",
    "#Tambien tendremos la palabra con una única letra (\"c\", del lenguaje c) macheado únicamente a un token \"c\". \n",
    "def tokenize_and_lemmatization(text_column):\n",
    "    tokens = word_tokenize(text_column) #Tokenizamos.\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [wordnet_lemmatizer.lemmatize(tok).lower() for tok in tokens] # Stem words.\n",
    "    return list(lemmatized_tokens)\n",
    "    \n",
    "def cleaning_DF(DF,column_to_clean):\n",
    "    clean_column='clean_'+column_to_clean\n",
    "    DF[clean_column]=DF[column_to_clean] #Copiamos el contenido de 'column_to_clean' en 'clean_column' para utilizarla en las funciones posteriores.\n",
    "    lower_text(DF,clean_column)\n",
    "    remove_punctuation_and_special_characters(DF,clean_column)\n",
    "    remove_stop_words(DF,clean_column)\n",
    "     \n",
    "def tokenize_and_lemmatize(DF,column_to_clean):\n",
    "    clean_column='clean_'+column_to_clean\n",
    "    tokens_column='tokens_'+column_to_clean\n",
    "    DF[tokens_column] = DF[clean_column].apply(tokenize_and_lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de datos del DF de Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborate with a multidisciplinary team to gain insight into complex biochemical systems.\n",
      "Design computational models to study various interactions such as interactions between genomes, proteins, and binding sites\n",
      "Extract various features from the computational models and communicate the results back to the team.\n",
      "Predict the behaviour of new protein structures on certain binding sites using the computational models.\n",
      "Work with the team of software engineers to embed your models into production.\n",
      "Perform other related duties in keeping with the purpose and accountabilities of the job.\n",
      "\n",
      "M.Sc. or Ph.D. in Engineering/Computer Science, or an equivalent combination of experience and knowledge.\n",
      "2+ years' experience applying machine learning and deep learning concepts to real-world problems.\n",
      "Solid programming skills with a focus on writing clean/maintainable code, with 2+ years of experience in Python (preferred), Java, or C++ programming.\n",
      "Good knowledge of machine learning libraries (Tensorflow, Keras, Pytorch, Sklearn, etc.).\n",
      "Strong analytical ability and mathematical skills.\n",
      "Advanced knowledge of machine learning theory and state of the art practices.\n",
      "Matured communication and critical thinking ability to influence and propose analytics strategies that challenge status quo thinking.\n",
      "Preferences:\n",
      "Strong background in natural language processing and/or the application of deep learning to genomic/proteomic data.\n",
      "Background in genetics, virology, microbiology, or biochemistry.\n",
      "Familiarity with statistical analysis (such as experiment design and hypothesis testing).\n",
      "Proficiency in Linux environments.\n",
      "Knowledge of the Agile project management methodology.\n",
      "Portfolio of machine learning projects available for review.\n",
      "\n",
      "Aptitude for interdisciplinary collaboration.\n",
      "Highly conscientious with strong follow-through.\n",
      "Capable of performing research on best practices and communicating results to a non-expert audience.\n",
      "Able to apply domain knowledge to ambiguous and novel situations.\n",
      "#######################################################\n",
      "collaborate multidisciplinary team gain insight complex biochemical systems design computational models study various interactions interactions genomes proteins binding sites extract various features computational models communicate results back team predict behaviour new protein structures certain binding sites using computational models work team software engineers embed models production perform related duties keeping purpose accountabilities job sc ph engineering computer science equivalent combination experience knowledge years experience applying machine learning deep learning concepts real-world problems solid programming skills focus writing clean maintainable code years experience python preferred java c programming good knowledge machine learning libraries tensorflow keras pytorch sklearn etc strong analytical ability mathematical skills advanced knowledge machine learning theory state art practices matured communication critical thinking ability influence propose analytics strategies challenge status quo thinking preferences strong background natural language processing application deep learning genomic proteomic data background genetics virology microbiology biochemistry familiarity statistical analysis experiment design hypothesis testing proficiency linux environments knowledge agile project management methodology portfolio machine learning projects available review aptitude interdisciplinary collaboration highly conscientious strong follow-through capable performing research best practices communicating results non-expert audience able apply domain knowledge ambiguous novel situations\n",
      "#######################################################\n",
      "['collaborate', 'multidisciplinary', 'team', 'gain', 'insight', 'complex', 'biochemical', 'system', 'design', 'computational', 'model', 'study', 'various', 'interaction', 'interaction', 'genome', 'protein', 'binding', 'site', 'extract', 'various', 'feature', 'computational', 'model', 'communicate', 'result', 'back', 'team', 'predict', 'behaviour', 'new', 'protein', 'structure', 'certain', 'binding', 'site', 'using', 'computational', 'model', 'work', 'team', 'software', 'engineer', 'embed', 'model', 'production', 'perform', 'related', 'duty', 'keeping', 'purpose', 'accountability', 'job', 'sc', 'ph', 'engineering', 'computer', 'science', 'equivalent', 'combination', 'experience', 'knowledge', 'year', 'experience', 'applying', 'machine', 'learning', 'deep', 'learning', 'concept', 'real-world', 'problem', 'solid', 'programming', 'skill', 'focus', 'writing', 'clean', 'maintainable', 'code', 'year', 'experience', 'python', 'preferred', 'java', 'c', 'programming', 'good', 'knowledge', 'machine', 'learning', 'library', 'tensorflow', 'kera', 'pytorch', 'sklearn', 'etc', 'strong', 'analytical', 'ability', 'mathematical', 'skill', 'advanced', 'knowledge', 'machine', 'learning', 'theory', 'state', 'art', 'practice', 'matured', 'communication', 'critical', 'thinking', 'ability', 'influence', 'propose', 'analytics', 'strategy', 'challenge', 'status', 'quo', 'thinking', 'preference', 'strong', 'background', 'natural', 'language', 'processing', 'application', 'deep', 'learning', 'genomic', 'proteomic', 'data', 'background', 'genetics', 'virology', 'microbiology', 'biochemistry', 'familiarity', 'statistical', 'analysis', 'experiment', 'design', 'hypothesis', 'testing', 'proficiency', 'linux', 'environment', 'knowledge', 'agile', 'project', 'management', 'methodology', 'portfolio', 'machine', 'learning', 'project', 'available', 'review', 'aptitude', 'interdisciplinary', 'collaboration', 'highly', 'conscientious', 'strong', 'follow-through', 'capable', 'performing', 'research', 'best', 'practice', 'communicating', 'result', 'non-expert', 'audience', 'able', 'apply', 'domain', 'knowledge', 'ambiguous', 'novel', 'situation']\n",
      "#######################################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Description</th>\n",
       "      <th>clean_Job_Description</th>\n",
       "      <th>tokens_Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>BSc/MSc/PhD in Physics, Mathematics, Biomedica...</td>\n",
       "      <td>bsc msc phd physics mathematics biomedical sof...</td>\n",
       "      <td>[bsc, msc, phd, physic, mathematics, biomedica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning Engineer 2</td>\n",
       "      <td>Collaborate with a multidisciplinary team to g...</td>\n",
       "      <td>collaborate multidisciplinary team gain insigh...</td>\n",
       "      <td>[collaborate, multidisciplinary, team, gain, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Master’s degree or above in a STEM field, incl...</td>\n",
       "      <td>master degree stem field including limited com...</td>\n",
       "      <td>[master, degree, stem, field, including, limit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist 2</td>\n",
       "      <td>\\nReporting to the Director, Data &amp; Analytics,...</td>\n",
       "      <td>reporting director data analytics senior data ...</td>\n",
       "      <td>[reporting, director, data, analytics, senior,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Stack Web Developer</td>\n",
       "      <td>\\n\\n    Graduate Degree in Information Technol...</td>\n",
       "      <td>graduate degree information technology similar...</td>\n",
       "      <td>[graduate, degree, information, technology, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Full Stack Web Developer 2</td>\n",
       "      <td>\\n· Enter existing website codebases and exten...</td>\n",
       "      <td>enter existing website codebases extend functi...</td>\n",
       "      <td>[enter, existing, website, codebases, extend, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HCM Consultant</td>\n",
       "      <td>\\nThe Oracle Cloud HCM Absence Consultant will...</td>\n",
       "      <td>oracle cloud hcm absence consultant responsibl...</td>\n",
       "      <td>[oracle, cloud, hcm, absence, consultant, resp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HCM Consultant 2</td>\n",
       "      <td>\\nConducting Fit/Gap analysis and business /pr...</td>\n",
       "      <td>conducting fit gap analysis business process m...</td>\n",
       "      <td>[conducting, fit, gap, analysis, business, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Security Specialist</td>\n",
       "      <td>Work in a fast-paced environment that combine ...</td>\n",
       "      <td>work fast-paced environment combine technical ...</td>\n",
       "      <td>[work, fast-paced, environment, combine, techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Security Specialist 2</td>\n",
       "      <td>\\n    Handling incoming requests for assistanc...</td>\n",
       "      <td>handling incoming requests assistance business...</td>\n",
       "      <td>[handling, incoming, request, assistance, busi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Job_Title  \\\n",
       "0    Machine Learning Engineer   \n",
       "1  Machine Learning Engineer 2   \n",
       "2               Data Scientist   \n",
       "3             Data Scientist 2   \n",
       "4     Full Stack Web Developer   \n",
       "5   Full Stack Web Developer 2   \n",
       "6               HCM Consultant   \n",
       "7             HCM Consultant 2   \n",
       "8          Security Specialist   \n",
       "9        Security Specialist 2   \n",
       "\n",
       "                                     Job_Description  \\\n",
       "0  BSc/MSc/PhD in Physics, Mathematics, Biomedica...   \n",
       "1  Collaborate with a multidisciplinary team to g...   \n",
       "2  Master’s degree or above in a STEM field, incl...   \n",
       "3  \\nReporting to the Director, Data & Analytics,...   \n",
       "4  \\n\\n    Graduate Degree in Information Technol...   \n",
       "5  \\n· Enter existing website codebases and exten...   \n",
       "6  \\nThe Oracle Cloud HCM Absence Consultant will...   \n",
       "7  \\nConducting Fit/Gap analysis and business /pr...   \n",
       "8  Work in a fast-paced environment that combine ...   \n",
       "9  \\n    Handling incoming requests for assistanc...   \n",
       "\n",
       "                               clean_Job_Description  \\\n",
       "0  bsc msc phd physics mathematics biomedical sof...   \n",
       "1  collaborate multidisciplinary team gain insigh...   \n",
       "2  master degree stem field including limited com...   \n",
       "3  reporting director data analytics senior data ...   \n",
       "4  graduate degree information technology similar...   \n",
       "5  enter existing website codebases extend functi...   \n",
       "6  oracle cloud hcm absence consultant responsibl...   \n",
       "7  conducting fit gap analysis business process m...   \n",
       "8  work fast-paced environment combine technical ...   \n",
       "9  handling incoming requests assistance business...   \n",
       "\n",
       "                              tokens_Job_Description  \n",
       "0  [bsc, msc, phd, physic, mathematics, biomedica...  \n",
       "1  [collaborate, multidisciplinary, team, gain, i...  \n",
       "2  [master, degree, stem, field, including, limit...  \n",
       "3  [reporting, director, data, analytics, senior,...  \n",
       "4  [graduate, degree, information, technology, si...  \n",
       "5  [enter, existing, website, codebases, extend, ...  \n",
       "6  [oracle, cloud, hcm, absence, consultant, resp...  \n",
       "7  [conducting, fit, gap, analysis, business, pro...  \n",
       "8  [work, fast-paced, environment, combine, techn...  \n",
       "9  [handling, incoming, request, assistance, busi...  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_DF(df_Jobs,'Job_Description')\n",
    "tokenize_and_lemmatize(df_Jobs,'Job_Description')\n",
    "print(df_Jobs['Job_Description'].iloc[1])  #Tomamos el job_description de la posición 1 como ejemplo.\n",
    "#non-expert  debería mantenerse.\n",
    "#deberia juntar el Computer Science\n",
    "print(\"#######################################################\")\n",
    "print(df_Jobs['clean_Job_Description'].iloc[1]) \n",
    "print(\"#######################################################\")\n",
    "print(df_Jobs['tokens_Job_Description'].iloc[1]) \n",
    "print(\"#######################################################\")\n",
    "df_Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de datos del DF de Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_Name</th>\n",
       "      <th>Content_CV</th>\n",
       "      <th>clean_Content_CV</th>\n",
       "      <th>tokens_Content_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_Chandler</td>\n",
       "      <td>Chandler Bing\\n \\nchandler.bing@vit.ac.in\\n \\n...</td>\n",
       "      <td>chandler bing chandler bing vit ac education l...</td>\n",
       "      <td>[chandler, bing, chandler, bing, vit, ac, educ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_MeghnaLohani</td>\n",
       "      <td>\\n \\n \\n \\n \\nMeghna Lohani\\n \\nCampus Addres...</td>\n",
       "      <td>meghna lohani campus address meghna lohani vit...</td>\n",
       "      <td>[meghna, lohani, campus, address, meghna, loha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_AmanSharma</td>\n",
       "      <td>Aman Sharma\\n \\nCampus Address                ...</td>\n",
       "      <td>aman sharma campus address aman sharma vitstud...</td>\n",
       "      <td>[aman, sharma, campus, address, aman, sharma, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Federico_Calonge_HCM</td>\n",
       "      <td>\\n                                           ...</td>\n",
       "      <td>calonge federico matias oracle consultant page...</td>\n",
       "      <td>[calonge, federico, matias, oracle, consultant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_VAISHALI BIJOY</td>\n",
       "      <td>VAISHALI BIJOY\\n \\nVAISHALI.BIJOY2016@vitstude...</td>\n",
       "      <td>vaishali bijoy vaishali bijoy vitstudent ac ed...</td>\n",
       "      <td>[vaishali, bijoy, vaishali, bijoy, vitstudent,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Test_Phoebe Buffay</td>\n",
       "      <td>Phoebe Buffay\\n \\n \\nPhobe.buffaycat@vit.ac.in...</td>\n",
       "      <td>phoebe buffay phobe buffaycat vit ac education...</td>\n",
       "      <td>[phoebe, buffay, phobe, buffaycat, vit, ac, ed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Candidate_Name                                         Content_CV  \\\n",
       "0         Test_Chandler  Chandler Bing\\n \\nchandler.bing@vit.ac.in\\n \\n...   \n",
       "1     Test_MeghnaLohani   \\n \\n \\n \\n \\nMeghna Lohani\\n \\nCampus Addres...   \n",
       "2       Test_AmanSharma  Aman Sharma\\n \\nCampus Address                ...   \n",
       "3  Federico_Calonge_HCM   \\n                                           ...   \n",
       "4   Test_VAISHALI BIJOY  VAISHALI BIJOY\\n \\nVAISHALI.BIJOY2016@vitstude...   \n",
       "5    Test_Phoebe Buffay  Phoebe Buffay\\n \\n \\nPhobe.buffaycat@vit.ac.in...   \n",
       "\n",
       "                                    clean_Content_CV  \\\n",
       "0  chandler bing chandler bing vit ac education l...   \n",
       "1  meghna lohani campus address meghna lohani vit...   \n",
       "2  aman sharma campus address aman sharma vitstud...   \n",
       "3  calonge federico matias oracle consultant page...   \n",
       "4  vaishali bijoy vaishali bijoy vitstudent ac ed...   \n",
       "5  phoebe buffay phobe buffaycat vit ac education...   \n",
       "\n",
       "                                   tokens_Content_CV  \n",
       "0  [chandler, bing, chandler, bing, vit, ac, educ...  \n",
       "1  [meghna, lohani, campus, address, meghna, loha...  \n",
       "2  [aman, sharma, campus, address, aman, sharma, ...  \n",
       "3  [calonge, federico, matias, oracle, consultant...  \n",
       "4  [vaishali, bijoy, vaishali, bijoy, vitstudent,...  \n",
       "5  [phoebe, buffay, phobe, buffaycat, vit, ac, ed...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_DF(df_Candidates,'Content_CV')\n",
    "tokenize_and_lemmatize(df_Candidates,'Content_CV')\n",
    "\n",
    "df_Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGREGADO: Funciones para tokenizar textos de una columna de un DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fedricio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fedricio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "#Esto de arriba se descarga LOCALMENTE en home/user/nltk_data.\n",
    "\n",
    "def tokenize(text):\n",
    "    #Convierte el texto recibido como entrada en una lista de tokens (palabras).\n",
    "    #Dentro pasamos todo a minusculas, se eliminan signos de puntuación, espacios en blanco y se eliminan las stop words.\n",
    "        #input  --> text: el documento como un STRING. \n",
    "        #output --> texts_tokens: cada palabra del texto en una LISTA de string. \n",
    "    texts_tokens=[]\n",
    "    for line in text:\n",
    "        tokens=word_tokenize(line)\n",
    "        tok=[w.lower() for w in tokens]\n",
    "        table=str.maketrans('','',string.punctuation)\n",
    "        strpp=[w.translate(table) for w in tok]\n",
    "        words=[word for word in strpp if word.isalpha()]\n",
    "        stop_words=set(stopwords.words('english'))\n",
    "        words=[w for w in words if not w in stop_words]\n",
    "        texts_tokens.append(words)\n",
    "    return texts_tokens\n",
    "\n",
    "def text_to_vector(text):\n",
    "    #Convierte el texto del documento en una \"term matrix\" donde todas las palabras estan listadas junto a \n",
    "    #la frecuencia en que aparecen en el texto.\n",
    "        #input  --> text: el documento como una LISTA de string. \n",
    "        #output --> Term matrix: una MATRIZ con cada palabra del documento junto a su frecuencia. \n",
    "        \n",
    "    texts_tokens = tokenize(text)\n",
    "    #print(texts_tokens)\n",
    "    return Counter(texts_tokens[0]) #[0] porque es una lista dentro de otra lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'collaborate': 1,\n",
       "         'multidisciplinary': 1,\n",
       "         'team': 3,\n",
       "         'gain': 1,\n",
       "         'insight': 1,\n",
       "         'complex': 1,\n",
       "         'biochemical': 1,\n",
       "         'systems': 1,\n",
       "         'design': 2,\n",
       "         'computational': 3,\n",
       "         'models': 4,\n",
       "         'study': 1,\n",
       "         'various': 2,\n",
       "         'interactions': 2,\n",
       "         'genomes': 1,\n",
       "         'proteins': 1,\n",
       "         'binding': 2,\n",
       "         'sites': 2,\n",
       "         'extract': 1,\n",
       "         'features': 1,\n",
       "         'communicate': 1,\n",
       "         'results': 2,\n",
       "         'back': 1,\n",
       "         'predict': 1,\n",
       "         'behaviour': 1,\n",
       "         'new': 1,\n",
       "         'protein': 1,\n",
       "         'structures': 1,\n",
       "         'certain': 1,\n",
       "         'using': 1,\n",
       "         'work': 1,\n",
       "         'software': 1,\n",
       "         'engineers': 1,\n",
       "         'embed': 1,\n",
       "         'production': 1,\n",
       "         'perform': 1,\n",
       "         'related': 1,\n",
       "         'duties': 1,\n",
       "         'keeping': 1,\n",
       "         'purpose': 1,\n",
       "         'accountabilities': 1,\n",
       "         'job': 1,\n",
       "         'msc': 1,\n",
       "         'phd': 1,\n",
       "         'engineeringcomputer': 1,\n",
       "         'science': 1,\n",
       "         'equivalent': 1,\n",
       "         'combination': 1,\n",
       "         'experience': 3,\n",
       "         'knowledge': 5,\n",
       "         'years': 2,\n",
       "         'applying': 1,\n",
       "         'machine': 4,\n",
       "         'learning': 6,\n",
       "         'deep': 2,\n",
       "         'concepts': 1,\n",
       "         'realworld': 1,\n",
       "         'problems': 1,\n",
       "         'solid': 1,\n",
       "         'programming': 2,\n",
       "         'skills': 2,\n",
       "         'focus': 1,\n",
       "         'writing': 1,\n",
       "         'cleanmaintainable': 1,\n",
       "         'code': 1,\n",
       "         'python': 1,\n",
       "         'preferred': 1,\n",
       "         'java': 1,\n",
       "         'c': 1,\n",
       "         'good': 1,\n",
       "         'libraries': 1,\n",
       "         'tensorflow': 1,\n",
       "         'keras': 1,\n",
       "         'pytorch': 1,\n",
       "         'sklearn': 1,\n",
       "         'etc': 1,\n",
       "         'strong': 3,\n",
       "         'analytical': 1,\n",
       "         'ability': 2,\n",
       "         'mathematical': 1,\n",
       "         'advanced': 1,\n",
       "         'theory': 1,\n",
       "         'state': 1,\n",
       "         'art': 1,\n",
       "         'practices': 2,\n",
       "         'matured': 1,\n",
       "         'communication': 1,\n",
       "         'critical': 1,\n",
       "         'thinking': 2,\n",
       "         'influence': 1,\n",
       "         'propose': 1,\n",
       "         'analytics': 1,\n",
       "         'strategies': 1,\n",
       "         'challenge': 1,\n",
       "         'status': 1,\n",
       "         'quo': 1,\n",
       "         'preferences': 1,\n",
       "         'background': 2,\n",
       "         'natural': 1,\n",
       "         'language': 1,\n",
       "         'processing': 1,\n",
       "         'andor': 1,\n",
       "         'application': 1,\n",
       "         'genomicproteomic': 1,\n",
       "         'data': 1,\n",
       "         'genetics': 1,\n",
       "         'virology': 1,\n",
       "         'microbiology': 1,\n",
       "         'biochemistry': 1,\n",
       "         'familiarity': 1,\n",
       "         'statistical': 1,\n",
       "         'analysis': 1,\n",
       "         'experiment': 1,\n",
       "         'hypothesis': 1,\n",
       "         'testing': 1,\n",
       "         'proficiency': 1,\n",
       "         'linux': 1,\n",
       "         'environments': 1,\n",
       "         'agile': 1,\n",
       "         'project': 1,\n",
       "         'management': 1,\n",
       "         'methodology': 1,\n",
       "         'portfolio': 1,\n",
       "         'projects': 1,\n",
       "         'available': 1,\n",
       "         'review': 1,\n",
       "         'aptitude': 1,\n",
       "         'interdisciplinary': 1,\n",
       "         'collaboration': 1,\n",
       "         'highly': 1,\n",
       "         'conscientious': 1,\n",
       "         'followthrough': 1,\n",
       "         'capable': 1,\n",
       "         'performing': 1,\n",
       "         'research': 1,\n",
       "         'best': 1,\n",
       "         'communicating': 1,\n",
       "         'nonexpert': 1,\n",
       "         'audience': 1,\n",
       "         'able': 1,\n",
       "         'apply': 1,\n",
       "         'domain': 1,\n",
       "         'ambiguous': 1,\n",
       "         'novel': 1,\n",
       "         'situations': 1})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PARA PROBAR LO DE ARRIBA COMO HACIA ANTES:\n",
    "#Tomamos como ejemplo la descripción del primer puesto del DF \"df_DS_Jobs\":\n",
    "text_description_1st_job = df_Jobs.iloc[1]['Job_Description']\n",
    "print(text_description_1st_job)\n",
    "\n",
    "#Usamos la funcion text_to_vector() para text description:\n",
    "#vector_test_description = text_to_vector([text_description_1st_job])  #Recordar mandar lo de adentro como una LISTA.\n",
    "#vector_test_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3.1.1-Agregando columna 'keywords' a nuestro DF obteniendolas mediante keyword extraction.\n",
    "\n",
    "Para esto ver previamente el ejemplo general (ejemplo número 5) del Notebook '1-WMD_And_Cosine_Similarity_General'.\n",
    "\n",
    "#### Resumen: \n",
    "Document -> Remove stop words -> Find Term Frequency (TF) -> Find Inverse Document Frequency (IDF) -> Find TF*IDF -> Get top N Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones principales a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "def get_top_n(dict_elem, n):\n",
    "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "    return result\n",
    "    \n",
    "def check_sent(word, sentences): \n",
    "    final = [all([w in x for w in word]) for x in sentences] \n",
    "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    return int(len(sent_len))\n",
    "   \n",
    "def get_keywords(doc,n):\n",
    "    from nltk import tokenize\n",
    "    from operator import itemgetter\n",
    "    import math\n",
    "\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize \n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    total_words = doc.split()\n",
    "    total_word_length = len(total_words)\n",
    "    # print(total_word_length)\n",
    "    \n",
    "    total_sentences = tokenize.sent_tokenize(doc)\n",
    "    total_sent_len = len(total_sentences)\n",
    "    print(total_sentences)\n",
    "    print(total_sent_len)\n",
    "    \n",
    "    #Calculo de TF:\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        each_word = each_word.replace('.','')\n",
    "        #if each_word not in stop_words:\n",
    "        if each_word in tf_score:\n",
    "            tf_score[each_word] += 1\n",
    "        else:\n",
    "            tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "    #print(tf_score)\n",
    "    \n",
    "    #Calculo de IDF:\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        each_word = each_word.replace('.','')\n",
    "        #if each_word not in stop_words:\n",
    "        if each_word in idf_score:\n",
    "            idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "        else:\n",
    "            idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "    #print(idf_score)\n",
    "\n",
    "    #Calculo de TF * IDF:\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "    # print(tf_idf_score)\n",
    "\n",
    "    #Obtenemos las 5 palabras top:\n",
    "    n_palabras_top = get_top_n(tf_idf_score, n)\n",
    "    # print(n_palabras_top)\n",
    "    \n",
    "    #Obtenemos la lista de keywords:\n",
    "    keyword_list = list(n_palabras_top.keys())   #n_palabras_top es un dic.\n",
    "    \n",
    "    return keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer programming is the process of designing and building an executable computer program to accomplish a specific computing result or to perform a specific task.', \"programming involves tasks such as: analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding).\", '[1][2] the source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit.', 'the purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem.', 'proficient programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.', 'tasks accompanying and related to programming include: testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs.', 'these might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code.', 'software engineering combines engineering techniques with software development practices.', 'reverse engineering is a related process used by designers, analysts and programmers to understand and re-create/re-implement.']\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['designing', 'building', 'executable', 'accomplish', 'computing']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo para comprobar el correcto funcionamiento de las funciones anteriores:\n",
    "doc_1 = \"Computer programming is the process of designing and building an executable computer program to accomplish a specific computing result or to perform a specific task. Programming involves tasks such as: analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding).[1][2] The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic. Tasks accompanying and related to programming include: testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices. Reverse engineering is a related process used by designers, analysts and programmers to understand and re-create/re-implement.\"\n",
    "doc = doc_1.lower()\n",
    "#'I am a graduate. I want to learn Python. I like learning Python. Python is easy. Python is interesting. Learning increases thinking. Everyone should invest time in learning'\n",
    "keyword_list = get_keywords(doc,5)  #Le pasamos el número de keywords que queremos y el documento/string.\n",
    "#dic.keys()\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>BSc/MSc/PhD in Physics, Mathematics, Biomedica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning Engineer 2</td>\n",
       "      <td>Collaborate with a multidisciplinary team to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Master’s degree or above in a STEM field, incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist 2</td>\n",
       "      <td>\\nReporting to the Director, Data &amp; Analytics,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Stack Web Developer</td>\n",
       "      <td>\\n\\n    Graduate Degree in Information Technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Full Stack Web Developer 2</td>\n",
       "      <td>\\n· Enter existing website codebases and exten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HCM Consultant</td>\n",
       "      <td>\\nThe Oracle Cloud HCM Absence Consultant will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HCM Consultant 2</td>\n",
       "      <td>\\nConducting Fit/Gap analysis and business /pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Security Specialist</td>\n",
       "      <td>Work in a fast-paced environment that combine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Security Specialist 2</td>\n",
       "      <td>\\n    Handling incoming requests for assistanc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Job_Title  \\\n",
       "0    Machine Learning Engineer   \n",
       "1  Machine Learning Engineer 2   \n",
       "2               Data Scientist   \n",
       "3             Data Scientist 2   \n",
       "4     Full Stack Web Developer   \n",
       "5   Full Stack Web Developer 2   \n",
       "6               HCM Consultant   \n",
       "7             HCM Consultant 2   \n",
       "8          Security Specialist   \n",
       "9        Security Specialist 2   \n",
       "\n",
       "                                     Job_Description  \n",
       "0  BSc/MSc/PhD in Physics, Mathematics, Biomedica...  \n",
       "1  Collaborate with a multidisciplinary team to g...  \n",
       "2  Master’s degree or above in a STEM field, incl...  \n",
       "3  \\nReporting to the Director, Data & Analytics,...  \n",
       "4  \\n\\n    Graduate Degree in Information Technol...  \n",
       "5  \\n· Enter existing website codebases and exten...  \n",
       "6  \\nThe Oracle Cloud HCM Absence Consultant will...  \n",
       "7  \\nConducting Fit/Gap analysis and business /pr...  \n",
       "8  Work in a fast-paced environment that combine ...  \n",
       "9  \\n    Handling incoming requests for assistanc...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahora aplicamos la funcion get_keywords(n,doc) a nuestro DF para generar la columna 'keywords':\n",
    "df_Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Description</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>bsc/msc/phd in physics, mathematics, biomedica...</td>\n",
       "      <td>[(eg,, bsc/msc/phd, physics,, mathematics,, bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning Engineer 2</td>\n",
       "      <td>collaborate with a multidisciplinary team to g...</td>\n",
       "      <td>[2+, collaborate, multidisciplinary, gain, ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>master’s degree or above in a stem field, incl...</td>\n",
       "      <td>[master’s, degree, or, above, in, a, stem, fie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist 2</td>\n",
       "      <td>\\nreporting to the director, data &amp; analytics,...</td>\n",
       "      <td>[projects, techniques, skills, organization’s,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Stack Web Developer</td>\n",
       "      <td>\\n\\n    graduate degree in information technol...</td>\n",
       "      <td>[experience, written, typescript), skills, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Full Stack Web Developer 2</td>\n",
       "      <td>\\n· enter existing website codebases and exten...</td>\n",
       "      <td>[5, (required), enter, existing, website, code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HCM Consultant</td>\n",
       "      <td>\\nthe oracle cloud hcm absence consultant will...</td>\n",
       "      <td>[requirements, experience, work, consulting, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HCM Consultant 2</td>\n",
       "      <td>\\nconducting fit/gap analysis and business /pr...</td>\n",
       "      <td>[and, experience, business, management,, abili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Security Specialist</td>\n",
       "      <td>work in a fast-paced environment that combine ...</td>\n",
       "      <td>[experience, with, work, fast-paced, combine, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Security Specialist 2</td>\n",
       "      <td>\\n    handling incoming requests for assistanc...</td>\n",
       "      <td>[security, of, handling, incoming, requests, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Job_Title  \\\n",
       "0    Machine Learning Engineer   \n",
       "1  Machine Learning Engineer 2   \n",
       "2               Data Scientist   \n",
       "3             Data Scientist 2   \n",
       "4     Full Stack Web Developer   \n",
       "5   Full Stack Web Developer 2   \n",
       "6               HCM Consultant   \n",
       "7             HCM Consultant 2   \n",
       "8          Security Specialist   \n",
       "9        Security Specialist 2   \n",
       "\n",
       "                                     Job_Description  \\\n",
       "0  bsc/msc/phd in physics, mathematics, biomedica...   \n",
       "1  collaborate with a multidisciplinary team to g...   \n",
       "2  master’s degree or above in a stem field, incl...   \n",
       "3  \\nreporting to the director, data & analytics,...   \n",
       "4  \\n\\n    graduate degree in information technol...   \n",
       "5  \\n· enter existing website codebases and exten...   \n",
       "6  \\nthe oracle cloud hcm absence consultant will...   \n",
       "7  \\nconducting fit/gap analysis and business /pr...   \n",
       "8  work in a fast-paced environment that combine ...   \n",
       "9  \\n    handling incoming requests for assistanc...   \n",
       "\n",
       "                                            Keywords  \n",
       "0  [(eg,, bsc/msc/phd, physics,, mathematics,, bi...  \n",
       "1  [2+, collaborate, multidisciplinary, gain, ins...  \n",
       "2  [master’s, degree, or, above, in, a, stem, fie...  \n",
       "3  [projects, techniques, skills, organization’s,...  \n",
       "4  [experience, written, typescript), skills, and...  \n",
       "5  [5, (required), enter, existing, website, code...  \n",
       "6  [requirements, experience, work, consulting, c...  \n",
       "7  [and, experience, business, management,, abili...  \n",
       "8  [experience, with, work, fast-paced, combine, ...  \n",
       "9  [security, of, handling, incoming, requests, f...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Jobs['Job_Description'] = df_Jobs['Job_Description'].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "\n",
    "\n",
    "df_Jobs['Keywords'] = df_Jobs['Job_Description'].apply(get_keywords, args=[10])\n",
    "df_Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos la lista de strings de la columna Keywords en strings separados por ', ':\n",
    "df_Jobs['Keywords'] = [', '.join(map(str, l)) for l in df_Jobs['Keywords']]\n",
    "\n",
    "#Minusculas:\n",
    "df_Jobs['Keywords'] = df_Jobs['Keywords'].apply(lambda x: x.lower() if isinstance(x,str) else x) #Ponemos todas en minusculas (importante para hacer las comparaciones futuras).\n",
    "df_Jobs['Job_Description'] = df_Jobs['Job_Description'].apply(lambda x: x.lower() if isinstance(x,str) else x) #Tambien en minuscula el Job_Description.\n",
    "\n",
    "df_Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 2 TF IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from nltk.tokenize import word_tokenize \n",
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "def get_top_n(dict_elem, n):\n",
    "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "    return result\n",
    "    \n",
    "def check_sent(word, sentences): \n",
    "    final = [all([w in x for w in word]) for x in sentences] \n",
    "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    return int(len(sent_len))\n",
    "\n",
    "def lower_text_2(text):\n",
    "    new_text = text.lower()\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation_and_special_characters_2(text):\n",
    "    new_text = re.sub('[^\\w-.!?]|(\\d+)',' ', text)\n",
    "    return new_text\n",
    "    #Todo lo que machee lo reemplaza por un espacio. Machea:\n",
    "        #1-Todas las non-words excepto el guion medio (-), el punto (.), signo de interrogación final (?) y signo de exclamación final (!). \n",
    "        #2-Y los números.\n",
    "\n",
    "def tokenize_and_lemmatization_2(total_words):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text=''\n",
    "    for each_word in total_words:\n",
    "        word_lemmatized = wordnet_lemmatizer.lemmatize(each_word).lower()\n",
    "        text = text+word_lemmatized+' '\n",
    "    return text\n",
    "\n",
    "def get_keywords_2(doc,n):\n",
    "    #Vamos a considerar a nuestro 'doc' como el total de todas nuestras palabras de la celda 'Job Description'.\n",
    "    #Para calcular el IDF dividimos nuestro 'doc' en 'sentences', donde cada 'sentence' es una oración del la columna 'Job Description'.\n",
    "    #Para realizar esta separación de oraciones utilizamos la función sent_tokenize (si detecta un punto (.), signo de interrogación final (?) o signo de exclamación final (!) se considera que termina una oración).\n",
    "    \n",
    "    # 1-Obtenemos nuestro doc y realizamos la limpieza del mismo:\n",
    "    doc = remove_punctuation_and_special_characters_2(doc)\n",
    "    doc = lower_text_2(doc)\n",
    "    total_words = doc.split()\n",
    "    doc = tokenize_and_lemmatization_2(total_words)\n",
    "    print(doc)\n",
    "    #Realizamos nuevamente el split, ya que algunas palabras cambiaron por la lematización:\n",
    "    total_words = doc.split()\n",
    "    total_word_length = len(total_words)\n",
    "    \n",
    "    # 2-Realizamos la división por oraciones:\n",
    "    total_sentences = tokenize.sent_tokenize(doc)\n",
    "    total_sent_len = len(total_sentences)\n",
    "    print(total_sentences)\n",
    "    print(total_sent_len)\n",
    "    \n",
    "    #Calculo de TF:\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        each_word = each_word.replace('.','')\n",
    "        each_word = each_word.replace('?','')\n",
    "        each_word = each_word.replace('!','')\n",
    "        print(each_word)\n",
    "        if each_word in tf_score:\n",
    "            tf_score[each_word] += 1\n",
    "        else:\n",
    "            tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "    #print(tf_score)\n",
    "    \n",
    "    #Calculo de IDF:\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        #Para eliminar los puntos (.), signos interrogación final (?) y signos de exclamación final (!) de las palabras que expresan fin de oración \n",
    "        #(por ejemplo en \"Hola Pepe, como estás? Yo bien, gracias.\" --> reemplazaría el \"estás?\" por \"estás\" y el \"gracias.\" por \"gracias\"). \n",
    "        each_word = each_word.replace('.','')\n",
    "        each_word = each_word.replace('?','')\n",
    "        each_word = each_word.replace('!','')\n",
    "        if each_word in idf_score:\n",
    "            idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "        else:\n",
    "            idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "    #print(idf_score)\n",
    "\n",
    "    #Calculo de TF * IDF:\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "    # print(tf_idf_score)\n",
    "\n",
    "    #Obtenemos las 5 palabras top:\n",
    "    n_palabras_top = get_top_n(tf_idf_score, n)\n",
    "    # print(n_palabras_top)\n",
    "    \n",
    "    #Obtenemos la lista de keywords:\n",
    "    keyword_list = list(n_palabras_top.keys())   #n_palabras_top es un dic.\n",
    "    \n",
    "    return keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is fast can you believe ß computer programming \n",
      "['python is fast can you believe ß', 'computer programming']\n",
      "2\n",
      "python\n",
      "is\n",
      "fast\n",
      "can\n",
      "you\n",
      "believe\n",
      "ß\n",
      "computer\n",
      "programming\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['python', 'is', 'fast', 'can', 'you']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo para comprobar el correcto funcionamiento de las funciones anteriores:\n",
    "doc_1 = \"Python is fast! Can you believe? ß Computers programming.\"\n",
    "#is the process of designing and building an executable computer program to accomplish a specific computing result or to perform a specific task. Programming involves tasks such as: analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding).[1][2] The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic. Tasks accompanying and related to programming include: testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices. Reverse engineering is a related process used by designers, analysts and programmers to understand and re-create/re-implement.\"\n",
    "doc = doc_1.lower()\n",
    "#'I am a graduate. I want to learn Python. I like learning Python. Python is easy. Python is interesting. Learning increases thinking. Everyone should invest time in learning'\n",
    "keyword_list = get_keywords_2(doc,5)  #Le pasamos el número de keywords que queremos y el documento/string.\n",
    "#dic.keys()\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborate with a multidisciplinary team to gain insight into complex biochemical systems.\n",
      "Design computational models to study various interactions such as interactions between genomes, proteins, and binding sites\n",
      "Extract various features from the computational models and communicate the results back to the team.\n",
      "Predict the behaviour of new protein structures on certain binding sites using the computational models.\n",
      "Work with the team of software engineers to embed your models into production.\n",
      "Perform other related duties in keeping with the purpose and accountabilities of the job.\n",
      "\n",
      "M.Sc. or Ph.D. in Engineering/Computer Science, or an equivalent combination of experience and knowledge.\n",
      "2+ years' experience applying machine learning and deep learning concepts to real-world problems.\n",
      "Solid programming skills with a focus on writing clean/maintainable code, with 2+ years of experience in Python (preferred), Java, or C++ programming.\n",
      "Good knowledge of machine learning libraries (Tensorflow, Keras, Pytorch, Sklearn, etc.).\n",
      "Strong analytical ability and mathematical skills.\n",
      "Advanced knowledge of machine learning theory and state of the art practices.\n",
      "Matured communication and critical thinking ability to influence and propose analytics strategies that challenge status quo thinking.\n",
      "Preferences:\n",
      "Strong background in natural language processing and/or the application of deep learning to genomic/proteomic data.\n",
      "Background in genetics, virology, microbiology, or biochemistry.\n",
      "Familiarity with statistical analysis (such as experiment design and hypothesis testing).\n",
      "Proficiency in Linux environments.\n",
      "Knowledge of the Agile project management methodology.\n",
      "Portfolio of machine learning projects available for review.\n",
      "\n",
      "Aptitude for interdisciplinary collaboration.\n",
      "Highly conscientious with strong follow-through.\n",
      "Capable of performing research on best practices and communicating results to a non-expert audience.\n",
      "Able to apply domain knowledge to ambiguous and novel situations.\n",
      "collaborate with a multidisciplinary team to gain insight into complex biochemical systems. design computational model to study various interaction such a interaction between genome protein and binding site extract various feature from the computational model and communicate the result back to the team. predict the behaviour of new protein structure on certain binding site using the computational models. work with the team of software engineer to embed your model into production. perform other related duty in keeping with the purpose and accountability of the job. m.sc. or ph.d. in engineering computer science or an equivalent combination of experience and knowledge. year experience applying machine learning and deep learning concept to real-world problems. solid programming skill with a focus on writing clean maintainable code with year of experience in python preferred java or c programming. good knowledge of machine learning library tensorflow kera pytorch sklearn etc. . strong analytical ability and mathematical skills. advanced knowledge of machine learning theory and state of the art practices. matured communication and critical thinking ability to influence and propose analytics strategy that challenge status quo thinking. preference strong background in natural language processing and or the application of deep learning to genomic proteomic data. background in genetics virology microbiology or biochemistry. familiarity with statistical analysis such a experiment design and hypothesis testing . proficiency in linux environments. knowledge of the agile project management methodology. portfolio of machine learning project available for review. aptitude for interdisciplinary collaboration. highly conscientious with strong follow-through. capable of performing research on best practice and communicating result to a non-expert audience. able to apply domain knowledge to ambiguous and novel situations. \n",
      "['collaborate with a multidisciplinary team to gain insight into complex biochemical systems.', 'design computational model to study various interaction such a interaction between genome protein and binding site extract various feature from the computational model and communicate the result back to the team.', 'predict the behaviour of new protein structure on certain binding site using the computational models.', 'work with the team of software engineer to embed your model into production.', 'perform other related duty in keeping with the purpose and accountability of the job.', 'm.sc.', 'or ph.d. in engineering computer science or an equivalent combination of experience and knowledge.', 'year experience applying machine learning and deep learning concept to real-world problems.', 'solid programming skill with a focus on writing clean maintainable code with year of experience in python preferred java or c programming.', 'good knowledge of machine learning library tensorflow kera pytorch sklearn etc.', '.', 'strong analytical ability and mathematical skills.', 'advanced knowledge of machine learning theory and state of the art practices.', 'matured communication and critical thinking ability to influence and propose analytics strategy that challenge status quo thinking.', 'preference strong background in natural language processing and or the application of deep learning to genomic proteomic data.', 'background in genetics virology microbiology or biochemistry.', 'familiarity with statistical analysis such a experiment design and hypothesis testing .', 'proficiency in linux environments.', 'knowledge of the agile project management methodology.', 'portfolio of machine learning project available for review.', 'aptitude for interdisciplinary collaboration.', 'highly conscientious with strong follow-through.', 'capable of performing research on best practice and communicating result to a non-expert audience.', 'able to apply domain knowledge to ambiguous and novel situations.']\n",
      "24\n",
      "collaborate\n",
      "with\n",
      "a\n",
      "multidisciplinary\n",
      "team\n",
      "to\n",
      "gain\n",
      "insight\n",
      "into\n",
      "complex\n",
      "biochemical\n",
      "systems\n",
      "design\n",
      "computational\n",
      "model\n",
      "to\n",
      "study\n",
      "various\n",
      "interaction\n",
      "such\n",
      "a\n",
      "interaction\n",
      "between\n",
      "genome\n",
      "protein\n",
      "and\n",
      "binding\n",
      "site\n",
      "extract\n",
      "various\n",
      "feature\n",
      "from\n",
      "the\n",
      "computational\n",
      "model\n",
      "and\n",
      "communicate\n",
      "the\n",
      "result\n",
      "back\n",
      "to\n",
      "the\n",
      "team\n",
      "predict\n",
      "the\n",
      "behaviour\n",
      "of\n",
      "new\n",
      "protein\n",
      "structure\n",
      "on\n",
      "certain\n",
      "binding\n",
      "site\n",
      "using\n",
      "the\n",
      "computational\n",
      "models\n",
      "work\n",
      "with\n",
      "the\n",
      "team\n",
      "of\n",
      "software\n",
      "engineer\n",
      "to\n",
      "embed\n",
      "your\n",
      "model\n",
      "into\n",
      "production\n",
      "perform\n",
      "other\n",
      "related\n",
      "duty\n",
      "in\n",
      "keeping\n",
      "with\n",
      "the\n",
      "purpose\n",
      "and\n",
      "accountability\n",
      "of\n",
      "the\n",
      "job\n",
      "msc\n",
      "or\n",
      "phd\n",
      "in\n",
      "engineering\n",
      "computer\n",
      "science\n",
      "or\n",
      "an\n",
      "equivalent\n",
      "combination\n",
      "of\n",
      "experience\n",
      "and\n",
      "knowledge\n",
      "year\n",
      "experience\n",
      "applying\n",
      "machine\n",
      "learning\n",
      "and\n",
      "deep\n",
      "learning\n",
      "concept\n",
      "to\n",
      "real-world\n",
      "problems\n",
      "solid\n",
      "programming\n",
      "skill\n",
      "with\n",
      "a\n",
      "focus\n",
      "on\n",
      "writing\n",
      "clean\n",
      "maintainable\n",
      "code\n",
      "with\n",
      "year\n",
      "of\n",
      "experience\n",
      "in\n",
      "python\n",
      "preferred\n",
      "java\n",
      "or\n",
      "c\n",
      "programming\n",
      "good\n",
      "knowledge\n",
      "of\n",
      "machine\n",
      "learning\n",
      "library\n",
      "tensorflow\n",
      "kera\n",
      "pytorch\n",
      "sklearn\n",
      "etc\n",
      "\n",
      "strong\n",
      "analytical\n",
      "ability\n",
      "and\n",
      "mathematical\n",
      "skills\n",
      "advanced\n",
      "knowledge\n",
      "of\n",
      "machine\n",
      "learning\n",
      "theory\n",
      "and\n",
      "state\n",
      "of\n",
      "the\n",
      "art\n",
      "practices\n",
      "matured\n",
      "communication\n",
      "and\n",
      "critical\n",
      "thinking\n",
      "ability\n",
      "to\n",
      "influence\n",
      "and\n",
      "propose\n",
      "analytics\n",
      "strategy\n",
      "that\n",
      "challenge\n",
      "status\n",
      "quo\n",
      "thinking\n",
      "preference\n",
      "strong\n",
      "background\n",
      "in\n",
      "natural\n",
      "language\n",
      "processing\n",
      "and\n",
      "or\n",
      "the\n",
      "application\n",
      "of\n",
      "deep\n",
      "learning\n",
      "to\n",
      "genomic\n",
      "proteomic\n",
      "data\n",
      "background\n",
      "in\n",
      "genetics\n",
      "virology\n",
      "microbiology\n",
      "or\n",
      "biochemistry\n",
      "familiarity\n",
      "with\n",
      "statistical\n",
      "analysis\n",
      "such\n",
      "a\n",
      "experiment\n",
      "design\n",
      "and\n",
      "hypothesis\n",
      "testing\n",
      "\n",
      "proficiency\n",
      "in\n",
      "linux\n",
      "environments\n",
      "knowledge\n",
      "of\n",
      "the\n",
      "agile\n",
      "project\n",
      "management\n",
      "methodology\n",
      "portfolio\n",
      "of\n",
      "machine\n",
      "learning\n",
      "project\n",
      "available\n",
      "for\n",
      "review\n",
      "aptitude\n",
      "for\n",
      "interdisciplinary\n",
      "collaboration\n",
      "highly\n",
      "conscientious\n",
      "with\n",
      "strong\n",
      "follow-through\n",
      "capable\n",
      "of\n",
      "performing\n",
      "research\n",
      "on\n",
      "best\n",
      "practice\n",
      "and\n",
      "communicating\n",
      "result\n",
      "to\n",
      "a\n",
      "non-expert\n",
      "audience\n",
      "able\n",
      "to\n",
      "apply\n",
      "domain\n",
      "knowledge\n",
      "to\n",
      "ambiguous\n",
      "and\n",
      "novel\n",
      "situations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['knowledge',\n",
       " 'of',\n",
       " 'with',\n",
       " 'project',\n",
       " 'experience',\n",
       " 'collaborate',\n",
       " 'multidisciplinary',\n",
       " 'gain',\n",
       " 'insight',\n",
       " 'complex',\n",
       " 'biochemical',\n",
       " 'systems',\n",
       " 'study',\n",
       " 'between',\n",
       " 'genome',\n",
       " 'extract',\n",
       " 'feature',\n",
       " 'from',\n",
       " 'communicate',\n",
       " 'back',\n",
       " 'predict',\n",
       " 'behaviour',\n",
       " 'new',\n",
       " 'structure',\n",
       " 'certain',\n",
       " 'using',\n",
       " 'models',\n",
       " 'work',\n",
       " 'software',\n",
       " 'engineer']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_Jobs['Keywords'] = df_Jobs['Job_Description'].apply(get_keywords_2, args=[3])\n",
    "#df_Jobs\n",
    "\n",
    "#df_Jobs['Keywords'] = df_Jobs['Job_Description'].apply(get_keywords_2, args=[3])\n",
    "\n",
    "text_description_1st_job = df_Jobs.iloc[1]['Job_Description']\n",
    "print(text_description_1st_job)\n",
    "\n",
    "keyword_list = get_keywords_2(text_description_1st_job,30)  #Le pasamos el número de keywords que queremos y el documento/string.\n",
    "#dic.keys()\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 3 TF-IDF:\n",
    "\n",
    "Ahora en lugar de considerar al DOC como solo la celda de 'Description', ahora consideramos TODAS las celdas de 'Description'. Asi, cada documento es una celda del job_description.\n",
    "\n",
    "El problema con la versión 2 es que generalmente en los jobs description no ponen \".\" para finalizar cada oraciones, entonces me toma todo el texto como 1 sola oración y al ahcer el IDF no tiene sentido porque solo tengo 1 oración.\n",
    "\n",
    "Cree una variable \"text\" con TODOS los textos. Y saco el keyword con todos esos textos partiendo en \"sentences\". Considero como caracter de fin de oración a 'ß'. Y luego si queremos las keywords de 1 job description en particular nos fijamos solo las keywords que aparezcan en este texto (esto lo hacemos en get_top_n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktLanguageVars\n",
    "class BulletPointLangVars(PunktLanguageVars):\n",
    "    sent_end_chars = ('ß')\n",
    "tokenizer = PunktSentenceTokenizer(lang_vars = BulletPointLangVars())\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "def get_top_n(dict_elem, text_description_1st_job, n):\n",
    "    # dict_elem.items() son diccionarios con key=palabra y value=valor tf-idf--> ('bsc', 0.0009448441087378112)\n",
    "    for key in dict_elem.copy():\n",
    "        if key not in text_description_1st_job:\n",
    "             #del dict_elem[key]\n",
    "            dict_elem.pop(key)\n",
    "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "    return result\n",
    "    \n",
    "def check_sent(word, sentences): \n",
    "    final = [all([w in x for w in word]) for x in sentences] \n",
    "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    return int(len(sent_len))\n",
    "\n",
    "def lower_text_2(text):\n",
    "    new_text = text.lower()\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation_and_special_characters_2(text):\n",
    "    new_text = re.sub('[^\\w-ß]|(\\d+)',' ', text)\n",
    "    return new_text\n",
    "    #Todo lo que machee lo reemplaza por un espacio. Machea:\n",
    "        #1-Todas las non-words excepto el guion medio (-) y el ß.\n",
    "        #2-Y los números.\n",
    "\n",
    "def tokenize_and_lemmatization_2(total_words):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text=''\n",
    "    for each_word in total_words:\n",
    "        word_lemmatized = wordnet_lemmatizer.lemmatize(each_word).lower()\n",
    "        text = text+word_lemmatized+' '\n",
    "    return text\n",
    "\n",
    "def get_keywords_2(doc,text_description_1st_job,n):\n",
    "    #Vamos a considerar a nuestro 'doc' como el total de todas nuestras palabras de la celda 'Job Description'.\n",
    "    #Para calcular el IDF dividimos nuestro 'doc' en 'sentences', donde cada 'sentence' es una oración del la columna 'Job Description'.\n",
    "    #Para realizar esta separación de oraciones utilizamos la función sent_tokenize (si detecta un punto (.), signo de interrogación final (?) o signo de exclamación final (!) se considera que termina una oración).\n",
    "    \n",
    "    # 1-Obtenemos nuestro doc y realizamos la limpieza del mismo:\n",
    "    doc = remove_punctuation_and_special_characters_2(doc)\n",
    "    doc = lower_text_2(doc)\n",
    "    total_words = doc.split()\n",
    "    doc = tokenize_and_lemmatization_2(total_words)\n",
    "    #print(doc)\n",
    "    #Realizamos nuevamente el split, ya que algunas palabras cambiaron por la lematización:\n",
    "    total_words = doc.split()\n",
    "    total_word_length = len(total_words)\n",
    "    \n",
    "    # 2-Realizamos la división por oraciones:\n",
    "    total_sentences = tokenizer.tokenize(doc)\n",
    "    total_sent_len = len(total_sentences)\n",
    "    #print(total_sentences)\n",
    "    #print(total_sent_len)\n",
    "    \n",
    "    #Calculo de TF:\n",
    "    tf_score = {}\n",
    "    for each_word in total_words:\n",
    "        each_word = each_word.replace('ß','')\n",
    "        #print(each_word)\n",
    "        if each_word in tf_score:\n",
    "            tf_score[each_word] += 1\n",
    "        else:\n",
    "            tf_score[each_word] = 1\n",
    "\n",
    "    # Dividing by total_word_length for each dictionary element\n",
    "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "    #print(tf_score)\n",
    "    \n",
    "    #Calculo de IDF:\n",
    "    idf_score = {}\n",
    "    for each_word in total_words:\n",
    "        each_word = each_word.replace('ß','')\n",
    "        if each_word in idf_score:\n",
    "            idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "        else:\n",
    "            idf_score[each_word] = 1\n",
    "\n",
    "    # Performing a log and divide\n",
    "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "    #print(idf_score)\n",
    "\n",
    "    #Calculo de TF * IDF:\n",
    "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "    # print(tf_idf_score)\n",
    "\n",
    "    #Obtenemos las 5 palabras top:\n",
    "    n_palabras_top = get_top_n(tf_idf_score, text_description_1st_job, n)\n",
    "    #print(n_palabras_top)\n",
    "    \n",
    "    #Obtenemos la lista de keywords:\n",
    "    keyword_list = list(n_palabras_top.keys())   #n_palabras_top es un dic.\n",
    "    \n",
    "    return keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multidisciplinary',\n",
       " 'gain',\n",
       " 'biochemical',\n",
       " 'genome',\n",
       " 'extract',\n",
       " 'back',\n",
       " 'predict',\n",
       " 'behaviour',\n",
       " 'structure',\n",
       " 'certain',\n",
       " 'engineer',\n",
       " 'embed',\n",
       " 'keeping',\n",
       " 'purpose',\n",
       " 'accountability',\n",
       " 'job',\n",
       " 'sc',\n",
       " 'ph',\n",
       " 'combination',\n",
       " 'applying']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_description_1st_job = df_Jobs.iloc[1]['tokens_Job_Description'] #COMPARAMOS CON ESTA, NO CON JOB_DESCRIPTION YA QUE AHI ESTAN LAS PALABRAS SIN LEMATIZAR ni aplicar ninguna limpieza.\n",
    "\n",
    "\n",
    "text = ' ß '.join(df_Jobs.Job_Description)\n",
    "text\n",
    "\n",
    "#Vamos a obtener las 20 keyword SOLO si estan en el documento 1 (o sea el 1er job description).\n",
    "keyword_list = get_keywords_2(text,text_description_1st_job,20)  #Le pasamos el número de keywords que queremos y el documento/string.\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2-Dataset CVs.\n",
    "\n",
    "Los armo extayendo todo el texto de los CVs en inglés ubicados en \"Archivos 2-Especificos/Ejemplo1_Dataset_CVs_And_Job_Desc/EN/PDFs_CVs\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leemos los CVs almacenados en nuestra carpeta y los extraemos uno por uno mediante la libreria PyPDF; devolviendonos\n",
    "#una secuencia de strings.\n",
    "import PyPDF2\n",
    "import os\n",
    "import collections\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "pathCVs='Archivos 2-Especificos/Ejemplo1_Dataset_CVs_And_Job_Desc/EN/PDFs_CVs' #Ruta Relativa, ubicacion de la carpeta.  #Antes, ruta absoluta: r'C:\\Users\\calon\\Desktop\\Notebooks\\Resume-Scoring-using-NLP-master\\Resume-Scoring-using-NLP-master\\Resumes'\n",
    "onlyfiles = [os.path.join(pathCVs, f) for f in os.listdir(pathCVs) if os.path.isfile(os.path.join(pathCVs, f))]\n",
    "\n",
    "print(\"Cantidad de CVs extraidos:\", len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para extraer las palabras del CV:\n",
    "import collections\n",
    "\n",
    "def pdfextract(file):\n",
    "    page_content = \"\"\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    c = collections.Counter(range(number_of_pages))\n",
    "    for i in c:\n",
    "        page = read_pdf.getPage(i)\n",
    "        page_content += page.extractText()\n",
    "    return (page_content.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file):\n",
    "    text = pdfextract(file).decode('utf-8')\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos todas las palabras del CV sin preprocesar ni nada:\n",
    "df_Candidates=pd.DataFrame(columns = ['Candidate_Name','Content_CV'])\n",
    "i=0\n",
    "while i < len(onlyfiles):\n",
    "    file=onlyfiles[i]\n",
    "    base = os.path.basename(file)  #Test_Phoebe Buffay.pdf\n",
    "    filename = os.path.splitext(base)[0]  #Test_Phoebe Buffay\n",
    "    dat=extract_text(file)\n",
    "    data = [{'Candidate_Name':filename, 'Content_CV':dat}]\n",
    "    df_Candidates=df_Candidates.append(data, ignore_index=True)\n",
    "    i+=1\n",
    "\n",
    "df_Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2.1-Limpiando datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Candidates.iloc[3]['Content_CV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_content_cv = df_Candidates.iloc[0]['Content_CV']\n",
    "first_content_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4- Realizando Comparaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1- Usamos el Word2vec descargado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link del cual descargamos el archivo: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "EMBEDDING_FILE = '/home/fedricio/Desktop/Embeddings_Utilizados/Word2vec/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.4.2- Funciones necesarias para Cosine Sim y WDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#Esto de arriba se descarga LOCALMENTE en home/user/nltk_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    #Convierte el texto recibido como entrada en una lista de tokens (palabras).\n",
    "    #Dentro pasamos todo a minusculas, se eliminan signos de puntuación, espacios en blanco y se eliminan las stop words.\n",
    "        #input  --> text: el documento como un STRING. \n",
    "        #output --> texts_tokens: cada palabra del texto en una LISTA de string. \n",
    "    texts_tokens=[]\n",
    "    for line in text:\n",
    "        tokens=word_tokenize(line)\n",
    "        tok=[w.lower() for w in tokens]\n",
    "        table=str.maketrans('','',string.punctuation)\n",
    "        strpp=[w.translate(table) for w in tok]\n",
    "        words=[word for word in strpp if word.isalpha()]\n",
    "        stop_words=set(stopwords.words('english'))\n",
    "        words=[w for w in words if not w in stop_words]\n",
    "        texts_tokens.append(words)\n",
    "    return texts_tokens\n",
    "\n",
    "def text_to_vector(text):\n",
    "    #Convierte el texto del documento en una \"term matrix\" donde todas las palabras estan listadas junto a \n",
    "    #la frecuencia en que aparecen en el texto.\n",
    "        #input  --> text: el documento como una LISTA de string. \n",
    "        #output --> Term matrix: una MATRIZ con cada palabra del documento junto a su frecuencia. \n",
    "        \n",
    "    texts_tokens = tokenize(text)\n",
    "    #print(texts_tokens)\n",
    "    return Counter(texts_tokens[0]) #[0] porque es una lista dentro de otra lista.\n",
    "\n",
    "def get_cosine(doc1, doc2):\n",
    "    #Get the cosine similarity between two documents.\n",
    "    #Depends on the angle between two non zero vectors which are constructed by each word frequency in the two documents.\n",
    "        #input  --> doc1: the first document as STRING.\n",
    "                    #doc2: the second document as STRING.\n",
    "        #output --> cosine similarity score\n",
    "\n",
    "    vec1 = text_to_vector([doc1])\n",
    "    vec2 = text_to_vector([doc2])\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "    \n",
    "def WMD(doc1,doc2):\n",
    "      #Preprocess the document first and remove english stopwords then call the function that calculates the word mover distance\n",
    "      #input --> doc1: the first document as STRING.\n",
    "                 #doc2: the second document as STRING.\n",
    "      #output --> Word Mover's Distance score\n",
    "    #print(doc1)\n",
    "    first_doc = tokenize([doc1])[0]     #[0] porque es una lista dentro de otra lista.\n",
    "    #print(first_doc)\n",
    "    second_doc = tokenize([doc2])[0]    #[0] porque es una lista dentro de otra lista.\n",
    "    return (word2vec.wmdistance(first_doc, second_doc))  #'wmdistance' return the word mover distance between two documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test para comprobar el funcionamiento de la función text_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomamos como ejemplo la descripción del primer puesto del DF \"df_DS_Jobs\":\n",
    "text_description_1st_job = df_Jobs.iloc[0]['Job_Description']\n",
    "text_description_1st_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos la funcion text_to_vector() para text description:\n",
    "vector_test_description = text_to_vector([text_description_1st_job])  #Recordar mandar lo de adentro como una LISTA.\n",
    "vector_test_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.4.3- Comparaciones.\n",
    "\n",
    "Las comparaciones a realizar son:\n",
    "\n",
    "    1-Entre el contenido de los CVs y las descripciones de los puestos.\n",
    "    2-Entre el contenido de los CVs y las keywords de puestos obtenidas mediante Keyword extraction.\n",
    "    3-Entre el contenido de los CVs y keywords de puestos puestas \"a mano\" /personalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3.1-Entre el contenido de los CVs y las descripciones de los puestos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un nuevo DF concatenando los 2 dataframe de manera que se comparen todos los CVs con todos los Jobs:\n",
    "#Nos quedarán 3 x 6 = 18 filas.\n",
    "df_Jobs_and_Candidates = pd.merge(df_Candidates.assign(A=1), df_Jobs.assign(A=1), on='A').drop('A', 1)\n",
    "df_Jobs_and_Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como prueba obtenemos el WMD y Cosine Sim comparando para la 1ra fila (entre las columnas Content_CV y Job_Description)\n",
    "first_job_descr = df_Jobs_and_Candidates.loc[0,'Job_Description']\n",
    "first_content_cv = df_Jobs_and_Candidates.loc[0,'Content_CV']\n",
    "\n",
    "cosine_result = round(get_cosine(first_job_descr,first_content_cv),3)\n",
    "wmd_result = round(WMD(first_job_descr,first_content_cv),3)\n",
    "print(cosine_result)\n",
    "print(wmd_result)\n",
    " \n",
    "similarity_wdm = round((1/(1+wmd_result)),3)  #similarity = 1 / (1 + distance) https://groups.google.com/g/gensim/c/-pRZnsOEaPQ \n",
    "print(similarity_wdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aplicamos las funciones get_cosine y WMD para TODO el DF... entre el contenido del CV y la descripcion del \n",
    "#puesto; y el resultado lo guardamos en  las columnas 'Cosine_Job_Desc' y 'WMD_Job_Desc'.\n",
    "\n",
    "import time\n",
    "\n",
    "inicio_cosine = time.time()\n",
    "df_Jobs_and_Candidates['Cosine_Job_Desc'] = df_Jobs_and_Candidates.apply(lambda row: round(get_cosine(row['Content_CV'],row['Job_Description']),3), axis=1)\n",
    "fin_cosine = time.time()\n",
    "print(fin_cosine-inicio_cosine)\n",
    "\n",
    "inicio_WMD = time.time()\n",
    "#df_Jobs_and_Candidates['WMD_Job_Desc'] = df_Jobs_and_Candidates.apply(lambda row: round(WMD(row['Content_CV'],row['Job_Description']),3), axis=1)\n",
    "#APlicando lo de  --> similarity = 1 / (1 + distance):\n",
    "df_Jobs_and_Candidates['WMD_Job_Desc'] = df_Jobs_and_Candidates.apply(lambda row: round((1/(1+(WMD(row['Content_CV'],row['Job_Description'])))),3), axis=1)\n",
    "fin_WMD = time.time()\n",
    "print(fin_WMD-inicio_WMD)\n",
    "\n",
    "df_Jobs_and_Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3.2-Entre el contenido de los CVs y las keywords de puestos obtenidas mediante Keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APlicamos las funciones get_cosine y WMD entre el contenido del CV y las keywords que obtuvimos previamente \n",
    "#del puesto y el resultado lo guardamos en  las columnas 'Cosine_Job_Keywords' y 'WMD_Job_Keywords'.\n",
    "df_Jobs_and_Candidates['Cosine_Job_Keywords'] = df_Jobs_and_Candidates.apply(lambda row: round(get_cosine(row['Content_CV'],row['Keywords']),3), axis=1)\n",
    "#df_Jobs_and_Candidates['WMD_Job_Keywords'] = df_Jobs_and_Candidates.apply(lambda row: round(WMD(row['Content_CV'],row['Keywords']),3), axis=1)\n",
    "#APlicando lo de  --> similarity = 1 / (1 + distance):\n",
    "df_Jobs_and_Candidates['WMD_Job_Keywords'] = df_Jobs_and_Candidates.apply(lambda row: round((1/(1+(WMD(row['Content_CV'],row['Keywords'])))),3), axis=1)\n",
    "\n",
    "df_Jobs_and_Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3.3-Entre el contenido de los CVs y keywords de puestos puestas \"a mano\" / personalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Content_CV_Phoebe_Buffay = df_Jobs_and_Candidates.loc[17,'Content_CV']\n",
    "print(Content_CV_Phoebe_Buffay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregamos 2 filas más al DF con las keywords personalizadas (las segundas keywords no tendrán nada que ver\n",
    "#al contenido del CV). Y luego aplicamos sim cosine y WDM para estos casos y vemos los resultados.\n",
    "\n",
    "df_Jobs_and_Candidates.loc[18] = ['Test',Content_CV_Phoebe_Buffay,'','','data science, machine learning, pandas, python, sql','','','',''] \n",
    "#Es lo mismo poner las Keywords con o sin comas (da igual el resultado, ya que el procesamiento se encarga de esto). \n",
    "df_Jobs_and_Candidates.loc[19] = ['Test',Content_CV_Phoebe_Buffay,'','','football, tennis, spanish','','','',''] \n",
    "\n",
    "df_Jobs_and_Candidates.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST: Usamos la funcion text_to_vector() para las keywords:\n",
    "vector_test_description = text_to_vector(df_Jobs_and_Candidates.iloc[18]['Keywords'])\n",
    "vector_test_description\n",
    "\n",
    "#Ahora si aplicamos sim cosine y WDM para estos casos y vemos los resultados. \n",
    "df_Jobs_and_Candidates.loc[18,'Cosine_Job_Keywords'] = round(get_cosine(df_Jobs_and_Candidates.loc[18,'Content_CV'],df_Jobs_and_Candidates.loc[18,'Keywords']),3)\n",
    "\n",
    "#df_Jobs_and_Candidates.loc[18,'WMD_Job_Keywords'] = round(WMD(df_Jobs_and_Candidates.loc[18,'Content_CV'],df_Jobs_and_Candidates.loc[18,'Keywords']),3)\n",
    "#APlicando lo de  --> similarity = 1 / (1 + distance):\n",
    "df_Jobs_and_Candidates.loc[18,'WMD_Job_Keywords'] = round((1/(1+(WMD(df_Jobs_and_Candidates.loc[18,'Content_CV'],df_Jobs_and_Candidates.loc[18,'Keywords'])))),3)\n",
    "\n",
    "df_Jobs_and_Candidates.loc[19,'Cosine_Job_Keywords'] = round(get_cosine(df_Jobs_and_Candidates.loc[19,'Content_CV'],df_Jobs_and_Candidates.loc[19,'Keywords']),3)\n",
    "\n",
    "#df_Jobs_and_Candidates.loc[19,'WMD_Job_Keywords'] = round(WMD(df_Jobs_and_Candidates.loc[19,'Content_CV'],df_Jobs_and_Candidates.loc[19,'Keywords']),3)\n",
    "#APlicando lo de  --> similarity = 1 / (1 + distance):\n",
    "df_Jobs_and_Candidates.loc[19,'WMD_Job_Keywords'] = round((1/(1+(WMD(df_Jobs_and_Candidates.loc[19,'Content_CV'],df_Jobs_and_Candidates.loc[19,'Keywords'])))),3)\n",
    "\n",
    "df_Jobs_and_Candidates.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5- Utilizando glove en lugar de word2vec para aplicar WDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word Embedding Model\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "print('gensim version: %s' % gensim.__version__)\n",
    "#glove_model = gensim.models.KeyedVectors.load_word2vec_format('../model/text/stanford/glove/glove.6B.50d.vec')\n",
    "glove_model = KeyedVectors.load_word2vec_format('/home/fedricio/Desktop/Embeddings_Utilizados/Glove/glove.6B.50d.txt', binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_text = df_Jobs_and_Candidates.loc[19,'Content_CV']\n",
    "CV_text_2 = df_Jobs_and_Candidates.loc[9,'Content_CV']\n",
    "CV_text_3 = df_Jobs_and_Candidates.loc[12,'Content_CV']\n",
    "Job_Keyw_text = \"data science machine learning pandas python sql\"\n",
    "Job_Keyw_text_2 = \"football, tennis, spanish\"\n",
    "texts_to_compare = [Job_Keyw_text,CV_text,CV_text_2,CV_text_3,Job_Keyw_text_2]\n",
    "texts_tokens = tokenize(texts_to_compare)   #Función definida en las secciones anteriores. \n",
    "print(texts_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tokens[1]   #CV_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprimimos las distancias entre Job_Keyw_text y los 3 CVs:\n",
    "\n",
    "tokens_job_keyw = texts_tokens[0]\n",
    "tokens_CV_1 = texts_tokens[1]\n",
    "tokens_CV_2 = texts_tokens[2]\n",
    "tokens_CV_3 = texts_tokens[3]\n",
    "tokens_job_keyw_2 = texts_tokens[4]\n",
    "\n",
    "distance_1 = glove_model.wmdistance(tokens_job_keyw, tokens_CV_1)\n",
    "print('Distance with CV_1= %.4f' % distance_1)\n",
    "\n",
    "distance_2 = glove_model.wmdistance(tokens_job_keyw, tokens_CV_2)\n",
    "print('Distance with CV_2= %.4f' % distance_2)\n",
    "\n",
    "distance_3 = glove_model.wmdistance(tokens_job_keyw, tokens_CV_3)\n",
    "print('Distance with CV_3= %.4f' % distance_3)\n",
    "\n",
    "#Esta última comparación no debe dar buen score, ya que las keywords no tienen nada que ver con el CV:\n",
    "distance_4 = glove_model.wmdistance(tokens_job_keyw_2, tokens_CV_1)\n",
    "print('Distance keywords_2 with CV_1= %.4f' % distance_4)\n",
    "\n",
    "print('#'*40)\n",
    "############################################################\n",
    "#Ahora aplicamos lo de --> similarity = 1 / (1 + distance):\n",
    "\n",
    "sim_1 = round((1 / (1 + distance_1)),3)\n",
    "print('Similarity with CV_1= %.4f' % sim_1)\n",
    "\n",
    "sim_2 = round((1 / (1 + distance_2)),3)\n",
    "print('Similarity with CV_2= %.4f' % sim_2)\n",
    "\n",
    "sim_3 = round((1 / (1 + distance_3)),3)\n",
    "print('Similarity with CV_3= %.4f' % sim_3)\n",
    "\n",
    "#Esta última comparación no debe dar buen score, ya que las keywords no tienen nada que ver con el CV:\n",
    "sim_4 = round((1 / (1 + distance_4)),3)\n",
    "print('Similarity keywords_2 with CV_1= %.4f' % sim_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6- Utilizando spacy en lugar de word2vec o glove para aplicar WDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo de la doc oficial modificado: doc oficial: https://spacy.io/universe/project/wmd-relax\n",
    "#Para esto descargue spacy: >conda install -c conda-forge spacy\n",
    "#Y el modelo: python -m spacy download en_core_web_lg\n",
    "import spacy\n",
    "import wmd\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', create_pipeline=wmd.WMD.create_spacy_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo:\n",
    "doc1 = nlp(\"Politician speaks to the media in Illinois.\")\n",
    "doc2 = nlp(\"The president greets the press in Chicago.\")\n",
    "doc3 = nlp(\"The president greets the press in Chicago.\")\n",
    "print(doc1.similarity(doc2))\n",
    "print(doc2.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nuestro caso:\n",
    "\n",
    "#Preprocesamos el doc_1 a comparar (el doc_2 no hace falta porque son keywords):\n",
    "doc_1 = df_Jobs_and_Candidates.loc[19,'Content_CV']\n",
    "doc_1_tokenize = tokenize([doc_1])              #Usando la funcion definida en las secciones previas.\n",
    "new_text_doc_1 = ' '.join(doc_1_tokenize[0])    #Pasamos la lista a string.\n",
    "new_text_doc_1                                  #Nuevo texto sin espacios, stop words y demas (ver todo lo que hace 'tokenize')\n",
    "\n",
    "doc1 = nlp(new_text_doc_1)\n",
    "doc2 = nlp('data science machine learning pandas python sql')\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos lo mismo que arriba solo que ahora nuestras keywords serán otras que no tendrán que ver con el CV:\n",
    "\n",
    "doc_1 = df_Jobs_and_Candidates.loc[19,'Content_CV']\n",
    "doc_1_tokenize = tokenize([doc_1])              \n",
    "new_text_doc_1 = ' '.join(doc_1_tokenize[0])    \n",
    "new_text_doc_1                                 \n",
    "\n",
    "doc1 = nlp(new_text_doc_1)\n",
    "doc2 = nlp('football, tennis, spanish')  #CAMBIAMOS ESTO.\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
