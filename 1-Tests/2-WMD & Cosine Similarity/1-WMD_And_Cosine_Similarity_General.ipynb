{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMD And Cosine Similarity General.\n",
    "\n",
    "Ejemplos GENERALES de WDM y Cosine Similarity:\n",
    "1. Ejemplo 1 WMD basado en https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632 con código fuente en https://github.com/makcedward/nlp/blob/master/sample/nlp-word_mover_distance.ipynb\n",
    "2. Ejemplo 2 Cosine Similarity & TF IDF de https://leantechblog.wordpress.com/2020/08/23/como-estimar-la-similitud-entre-documentos-con-python/ con código fuente en https://github.com/cjcarvajal/text-similarity-obama-trump\n",
    "3. Ejemplo 3 WMD using Spacy basado en https://stackoverflow.com/questions/54535535/how-to-improve-word-mover-distance-similarity-in-python-and-provide-similarity-s (el código es el de la 1ra respuesta).\n",
    "4. Ejemplo 4 WMD BIEN completo: https://github.com/Seif-Tarek/Document-Similarity-using-Word-Mover-Distance/blob/master/WMD_TextSimilarity.ipynb\n",
    "5. Keyword extraction using TF*IDF: https://www.analyticsvidhya.com/blog/2020/11/words-that-matter-a-simple-guide-to-keyword-extraction-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1 WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Mover's Distance (WMD) is proposed fro distance measurement between 2 documents (or sentences). It leverages Word Embeddings power to overcome those basic distance measurement limitations. \n",
    "WMD was introduced by Kusner et al. in 2015. Instead of using Euclidean Distance and other bag-of-words based distance measurement, they proposed to use word embeddings to calculate the similarities. To be precise, it uses normalized Bag-of-Words and Word Embeddings to calculate the distance between documents.\n",
    "\n",
    "In the previous blog, I shared how we can use simple way to find the \"similarity\" between two documents (or sentences). At that time, Euclidean Distance, Cosine Distance and Jaccard Similarity are introduced but it has some limitations.  WMD is designed to __overcome synonym problem__.\n",
    "\n",
    "The typical example is \n",
    "- Sentence 1: Obama speaks to the media in Illinois\n",
    "- Sentence 2: The president greets the press in Chicago\n",
    "\n",
    "Except the stop words, there is no common words among two sentences but both of them are taking about same topic (at that time).\n",
    "\n",
    "WMD use word embeddings to calculate the distance so that it can calculate even though there is no common word. The assumption is that similar words should have similar vectors.\n",
    "\n",
    "First of all, lower case and removing stopwords is an essential step to reduce complexity and preventing misleading. \n",
    "- Sentence 1: obama speaks media illinois\n",
    "- Sentence 2: president greets press chicago\n",
    "\n",
    "Retrieve vectors from any pre-trained word embeddings models. It can be GloVe, word2vec, fasttext or custom vectors. After that it using normalized bag-of-words (nBOW) to represent the weight or importance. It assumes that higher frequency implies that it is more important.\n",
    "\n",
    "It allows transfer every word from sentence 1 to sentence 2 because algorithm does not know \"obama\" should transfer to \"president\". At the end it will choose the minimum transportation cost to transport every word from sentence 1 to sentence 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WMD Implementation\n",
    "By using gensim, we only need to provide two list of tokens then it will take the rest of calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    News headline get from \n",
    "    \n",
    "    https://www.reuters.com/article/us-musk-tunnel/elon-musks-boring-co-to-build-high-speed-airport-link-in-chicago-idUSKBN1JA224\n",
    "    http://money.cnn.com/2018/06/14/technology/elon-musk-boring-company-chicago/index.html\n",
    "    https://www.theverge.com/2018/6/13/17462496/elon-musk-boring-company-approved-tunnel-chicago\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "news_headline1 = \"Elon Musk's Boring Co to build high-speed airport link in Chicago\"\n",
    "news_headline2 = \"Elon Musk's Boring Company to build high-speed Chicago airport link\"\n",
    "news_headline3 = \"Elon Musk’s Boring Company approved to build high-speed transit between downtown Chicago and O’Hare Airport\"\n",
    "news_headline4 = \"Both apple and orange are fruit\"\n",
    "\n",
    "news_headlines = [news_headline1, news_headline2, news_headline3, news_headline4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fedricio/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Load Word Embedding Model\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "print('gensim version: %s' % gensim.__version__)\n",
    "#glove_model = gensim.models.KeyedVectors.load_word2vec_format('../model/text/stanford/glove/glove.6B.50d.vec')\n",
    "glove_model = KeyedVectors.load_word2vec_format('/home/fedricio/Desktop/Embeddings_Utilizados/Glove/glove.6B.50d.txt', binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['elon', 'musk', 'boring', 'co', 'build', 'high', '-', 'speed', 'airport', 'link', 'chicago'], ['elon', 'musk', 'boring', 'company', 'build', 'high', '-', 'speed', 'chicago', 'airport', 'link'], ['elon', 'musk', 'boring', 'company', 'approved', 'build', 'high', '-', 'speed', 'transit', 'downtown', 'chicago', 'o’hare', 'airport'], ['apple', 'orange', 'fruit']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "headline_tokens = []\n",
    "for news_headline in news_headlines:\n",
    "    headline_tokens.append([token.text.lower() for token in spacy_nlp(news_headline) if not token.is_stop])\n",
    "\n",
    "print(headline_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline:  Elon Musk's Boring Co to build high-speed airport link in Chicago\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Comparing to: Elon Musk's Boring Co to build high-speed airport link in Chicago\n",
      "distance = 0.0000\n",
      "--------------------------------------------------\n",
      "Comparing to: Elon Musk's Boring Company to build high-speed Chicago airport link\n",
      "distance = 0.0734\n",
      "--------------------------------------------------\n",
      "Comparing to: Elon Musk’s Boring Company approved to build high-speed transit between downtown Chicago and O’Hare Airport\n",
      "distance = 0.3675\n",
      "--------------------------------------------------\n",
      "Comparing to: Both apple and orange are fruit\n",
      "distance = 1.1590\n"
     ]
    }
   ],
   "source": [
    "subject_headline = news_headlines[0]\n",
    "subject_token = headline_tokens[0]\n",
    "\n",
    "print('Headline: ', subject_headline)\n",
    "print('=' * 50)\n",
    "print()\n",
    "\n",
    "for token, headline in zip(headline_tokens, news_headlines):\n",
    "    print('-' * 50)\n",
    "    print('Comparing to:', headline)\n",
    "    distance = glove_model.wmdistance(subject_token, token)\n",
    "    print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gensim implementation, OOV will be removed so that it will not throw an exception or using random vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2 Cosine Similarity & TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Leantechblog article similarity:\n",
      "----------------------------------\n",
      "[[1.         0.217227   0.05744137]\n",
      " [0.217227   1.         0.04773379]\n",
      " [0.05744137 0.04773379 1.        ]]\n",
      "-----------------------------------------\n",
      "Melania and Michelle speeches similarity:\n",
      "-----------------------------------------\n",
      "[[1.         0.29814417]\n",
      " [0.29814417 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "language_stopwords = stopwords.words('english')\n",
    "non_words = list(punctuation)\n",
    "\n",
    "def remove_stop_words(dirty_text):\n",
    "    cleaned_text = ''\n",
    "    for word in dirty_text.split():\n",
    "        if word in language_stopwords or word in non_words:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_text += word + ' '\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation(dirty_string):\n",
    "    for word in non_words:\n",
    "        dirty_string = dirty_string.replace(word, '')\n",
    "    return dirty_string\n",
    "\n",
    "def process_file(file_name):\n",
    "    file_content = open(file_name, \"r\").read()\n",
    "    # All to lower case\n",
    "    file_content = file_content.lower()\n",
    "    # Remove punctuation and spanish stopwords\n",
    "    file_content = remove_punctuation(file_content)\n",
    "    file_content = remove_stop_words(file_content)\n",
    "    return file_content\n",
    "\n",
    "nlp_article = process_file(\"Archivos 1-General/Ejemplo2/nlp.txt\")\n",
    "sentiment_analysis_article = process_file(\"Archivos 1-General/Ejemplo2/sentiment_analysis.txt\")\n",
    "java_certification_article = process_file(\"Archivos 1-General/Ejemplo2/java_cert.txt\")\n",
    "\n",
    "#TF-IDF\n",
    "vectorizer = TfidfVectorizer ()\n",
    "X = vectorizer.fit_transform([nlp_article,sentiment_analysis_article,java_certification_article])\n",
    "#X = count.fit_transform([nlp_article,sentiment_analysis_article,java_certification_article])\n",
    "similarity_matrix = cosine_similarity(X,X)\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Leantechblog article similarity:')\n",
    "print('----------------------------------')\n",
    "print(similarity_matrix)\n",
    "\n",
    "michelle_speech = process_file(\"Archivos 1-General/Ejemplo2/michelle_speech.txt\")\n",
    "melania_speech = process_file(\"Archivos 1-General/Ejemplo2/melania_speech.txt\")\n",
    "\n",
    "#TF-IDF\n",
    "vectorizer = TfidfVectorizer ()\n",
    "X = vectorizer.fit_transform([michelle_speech,melania_speech])\n",
    "similarity_matrix = cosine_similarity(X,X)\n",
    "\n",
    "print('-----------------------------------------')\n",
    "print('Melania and Michelle speeches similarity:')\n",
    "print('-----------------------------------------')\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3 WMD using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9536311618244545\n"
     ]
    }
   ],
   "source": [
    "#Para desto descargue spacy: >conda install -c conda-forge spacy\n",
    "#Y el modelo: python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_lg')\n",
    "text = \"Some hotel description\"\n",
    "doc = spacy_nlp(text)\n",
    "current_tokens = [token.text for token in doc]\n",
    "#\n",
    "#for item in doc:\n",
    "#   if item.ent_type_ == \"the_type_to_be_removed\":\n",
    "     # remove word from `current_tokens` list\n",
    "new_text = \" \".join(current_tokens)\n",
    "doc = spacy_nlp(new_text)\n",
    "\n",
    "#Descargue wdm: pip install wmd\n",
    "import wmd\n",
    "spacy_nlp.add_pipe(wmd.WMD.SpacySimilarityHook(spacy_nlp), last=True)\n",
    "doc_2 = spacy_nlp(\"Another hotel description\")\n",
    "print(doc.similarity(doc_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 4 WMD BIEN completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fedricio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as  plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usamos el Word2vec descargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link del cual descargamos el archivo: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "EMBEDDING_FILE = '/home/fedricio/Desktop/Embeddings_Utilizados/Word2vec/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leyendo el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as republicans wrestle with how to oppose pres...</td>\n",
       "      <td>group: chinese police have required some forei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>three faculty members were killed and three ot...</td>\n",
       "      <td>the mary rose, flagship of henry viii, was rai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all lyle petersen wanted to do was get his mai...</td>\n",
       "      <td>more than 1.5 million people have been infecte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a somali suspect in the hijacking of the u.s.-...</td>\n",
       "      <td>u.n. chief calls pakistan floods \"a global dis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can we predict the future of medicine? althoug...</td>\n",
       "      <td>the cleveland clinic has published its top 10 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles  \\\n",
       "0  as republicans wrestle with how to oppose pres...   \n",
       "1  three faculty members were killed and three ot...   \n",
       "2  all lyle petersen wanted to do was get his mai...   \n",
       "3  a somali suspect in the hijacking of the u.s.-...   \n",
       "4  can we predict the future of medicine? althoug...   \n",
       "\n",
       "                                           abstracts  similarity  \n",
       "0  group: chinese police have required some forei...           0  \n",
       "1  the mary rose, flagship of henry viii, was rai...           0  \n",
       "2  more than 1.5 million people have been infecte...           1  \n",
       "3  u.n. chief calls pakistan floods \"a global dis...           0  \n",
       "4  the cleveland clinic has published its top 10 ...           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Archivos 1-General/Ejemplo4/DocumentSimilarity_Dataset.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicaciones Cosine SImilarity y WDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cosine Similarity\n",
    "Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.\n",
    "\n",
    "![Alt Text](https://i.imgur.com/HqKjGoQ.jpg)\n",
    "\n",
    "\n",
    "#### 2. Word Mover's Distance\n",
    "Word Mover's Distance (WMD) uses the word embeddings of the words in two texts to measure the minimum distance that the words in one text need to travel in semantic space to reach the words in the other text.\n",
    "\n",
    "The WMD is measured by measuring the minimum euclidean distance between each word in the two documents in word2vec space. if the distance is small then words in the two documents are close to each other.\n",
    "\n",
    "So, If I have the same two sentences:\n",
    "- sentence 1: \"Obama speaks to the media in Illinois\"\n",
    "- sentence 2: \"The president greets the press in Chicago\"\n",
    "\n",
    "After removing stopwords, The word mover distance is small as mentioned in the figure.\n",
    "\n",
    "\n",
    "![Alt Text](https://imgur.com/L1QNfPK.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'\\w+', re.UNICODE)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "WORD    #VER QUE ES ESTO que lo usan abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    '''\n",
    "        converting the document to a term matrix where all words are listed and beside it the frequency of it.\n",
    "\n",
    "        -- input:\n",
    "                    text: the document as string \n",
    "        -- output:\n",
    "                    Term matrix: Each word in the two documents and its frequency \n",
    "    '''\n",
    "    words = WORD.findall(text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    return Counter(words)\n",
    "\n",
    "def get_cosine(doc1, doc2):\n",
    "    '''\n",
    "        Get the cosine similarity between two documents.\n",
    "        Depends on the angle between two non zero vectors which are constructed by each word frequency in the two documents.\n",
    "\n",
    "        -- input:\n",
    "                      doc1: the first document as string\n",
    "                      doc2: the second document as string\n",
    "        -- output:\n",
    "                      cosine similarity score\n",
    "\n",
    "    '''\n",
    "    vec1 = text_to_vector(doc1)\n",
    "    vec2 = text_to_vector(doc2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "        \n",
    "def wordMdistance(doc1,doc2):\n",
    "  '''\n",
    "      return the word mover distance between two documents \n",
    "\n",
    "      -- input:\n",
    "                      doc1: the first document as list of words\n",
    "                      doc2: the second document as list of words\n",
    "      -- output:\n",
    "                      Word Mover's Distance score\n",
    "  '''\n",
    "  sum_dist = 0\n",
    "  i = 0\n",
    "  for word in sent1:\n",
    "    mindist = 1000.0\n",
    "    for word2 in sent2:\n",
    "      try:\n",
    "        j = np.copy(word2vec.get_vector(word))\n",
    "        t = np.copy(word2vec.get_vector(word2))\n",
    "        dista = np.sqrt(sum((j-t)**2))\n",
    "        if(dista < mindist):\n",
    "          mindist = dista\n",
    "      except:\n",
    "        continue\n",
    "    sum_dist+=mindist\n",
    "    i+=1\n",
    "  return sum_dist/i\n",
    "\n",
    "def WMD(doc1,doc2):\n",
    "  '''\n",
    "      Preprocess the document first and remove english stopwords then call the function that calculates the word mover distance\n",
    "     \n",
    "      -- input:\n",
    "                      doc1: the first document as string\n",
    "                      doc2: the second document as string\n",
    "      -- output:\n",
    "                      Word Mover's Distance score\n",
    "  '''\n",
    "  first_doc = doc1.lower().split()\n",
    "  second_doc = doc2.lower().split()\n",
    "  stopwords = nltk.corpus.stopwords.words('english')\n",
    "  first_doc = [w for w in first_doc if w not in stopwords]\n",
    "  second_doc = [w for w in second_doc if w not in stopwords]\n",
    "  return (word2vec.wmdistance(second_doc, first_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizamos nuevamente el DS y aplicamos las funciones anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a 6.2-magnitude earthquake struck off the sout...</td>\n",
       "      <td>there is no tsunami threat . the quake was al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook ceo mark zuckerberg said in an interv...</td>\n",
       "      <td>new york police officer seen in video kicking ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>andre villas-boas' troubles as chelsea manager...</td>\n",
       "      <td>chelsea manager andre villas-boas under furthe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>university park, pennsylvania   the fatal expl...</td>\n",
       "      <td>the training materials are a result of a 2013 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>after five months of detention in north korea,...</td>\n",
       "      <td>yohan blake beats usain bolt over 200m at the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles  \\\n",
       "0  a 6.2-magnitude earthquake struck off the sout...   \n",
       "1  facebook ceo mark zuckerberg said in an interv...   \n",
       "2  andre villas-boas' troubles as chelsea manager...   \n",
       "3  university park, pennsylvania   the fatal expl...   \n",
       "4  after five months of detention in north korea,...   \n",
       "\n",
       "                                           abstracts  similarity  \n",
       "0   there is no tsunami threat . the quake was al...           1  \n",
       "1  new york police officer seen in video kicking ...           0  \n",
       "2  chelsea manager andre villas-boas under furthe...           1  \n",
       "3  the training materials are a result of a 2013 ...           0  \n",
       "4  yohan blake beats usain bolt over 200m at the ...           0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a 6.2-magnitude earthquake struck off the southern coast of panama on sunday, the u.s. geological survey said. the center of the quake was roughly 215 miles south of david, panama, and almost 330 miles southwest of the capital, panama city, the usgs said. it was an estimated six miles deep. the usgs had initially given the quake a magnitude of 6.6. there was no tsunami threat, the pacific tsunami warning center said. there were also no immediate reports of injuries or damage.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Article_1 = data['articles'][0]\n",
    "Article_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' there is no tsunami threat . the quake was almost 330 miles southwest of panama city . there are no immediate reports of injuries or damage .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstract_1 = data['abstracts'][0]\n",
    "Abstract_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sim_1 = data['similarity'][0]\n",
    "Sim_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Article_2 = data['articles'][1]\n",
    "Abstract_2 = data['abstracts'][1]\n",
    "Sim_2 = data['similarity'][1]\n",
    "Sim_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get_cosine de a uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.565"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(get_cosine(Article_1,Abstract_1),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(get_cosine(Article_2,Abstract_2),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WDM de a uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.694"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(WMD(Article_1,Abstract_1),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.193"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(WMD(Article_2,Abstract_2),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuevo DF con Get_Cosine y WDM para todo el DF anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-c44c36e84e3f>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  New_DF_Acotado['Cosine'] = New_DF_Acotado.apply(lambda row: round(get_cosine(row['article'],row['summary']),3), axis=1)\n",
      "<ipython-input-51-c44c36e84e3f>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  New_DF_Acotado['WMD'] = New_DF_Acotado.apply(lambda row: round(WMD(row['article'],row['summary']),3), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>sim</th>\n",
       "      <th>Cosine</th>\n",
       "      <th>WMD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a 6.2-magnitude earthquake struck off the sout...</td>\n",
       "      <td>there is no tsunami threat . the quake was al...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook ceo mark zuckerberg said in an interv...</td>\n",
       "      <td>new york police officer seen in video kicking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>andre villas-boas' troubles as chelsea manager...</td>\n",
       "      <td>chelsea manager andre villas-boas under furthe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>university park, pennsylvania   the fatal expl...</td>\n",
       "      <td>the training materials are a result of a 2013 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>after five months of detention in north korea,...</td>\n",
       "      <td>yohan blake beats usain bolt over 200m at the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a pair of georgia men faced more than a half-h...</td>\n",
       "      <td>ballet opening thursday features live performa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>washington president barack obama's keystone p...</td>\n",
       "      <td>the senate blocked a keystone bill from advanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tina fey's follow-up to \"30 rock\" is getting a...</td>\n",
       "      <td>tina fey's series \"the unbreakable kimmy schmi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.229</td>\n",
       "      <td>1.193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>as north koreans face an uncertain future with...</td>\n",
       "      <td>u.s. secretary of state, israeli prime ministe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>long gone are the days of ice sculptures and c...</td>\n",
       "      <td>much of premium class airline food is hand-pre...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.385</td>\n",
       "      <td>1.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>did you ever wonder why charlie brown kept cha...</td>\n",
       "      <td>the dog was sweeping a parking deck near where...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              article  \\\n",
       "0   a 6.2-magnitude earthquake struck off the sout...   \n",
       "1   facebook ceo mark zuckerberg said in an interv...   \n",
       "2   andre villas-boas' troubles as chelsea manager...   \n",
       "3   university park, pennsylvania   the fatal expl...   \n",
       "4   after five months of detention in north korea,...   \n",
       "5   a pair of georgia men faced more than a half-h...   \n",
       "6   washington president barack obama's keystone p...   \n",
       "7   tina fey's follow-up to \"30 rock\" is getting a...   \n",
       "8   as north koreans face an uncertain future with...   \n",
       "9   long gone are the days of ice sculptures and c...   \n",
       "10  did you ever wonder why charlie brown kept cha...   \n",
       "\n",
       "                                              summary  sim  Cosine    WMD  \n",
       "0    there is no tsunami threat . the quake was al...    1   0.565  0.694  \n",
       "1   new york police officer seen in video kicking ...    0   0.017  1.193  \n",
       "2   chelsea manager andre villas-boas under furthe...    1   0.466  0.933  \n",
       "3   the training materials are a result of a 2013 ...    0   0.031  1.246  \n",
       "4   yohan blake beats usain bolt over 200m at the ...    0   0.020  1.230  \n",
       "5   ballet opening thursday features live performa...    0   0.051  1.205  \n",
       "6   the senate blocked a keystone bill from advanc...    1   0.414  1.119  \n",
       "7   tina fey's series \"the unbreakable kimmy schmi...    1   0.229  1.193  \n",
       "8   u.s. secretary of state, israeli prime ministe...    0   0.014  1.167  \n",
       "9   much of premium class airline food is hand-pre...    1   0.385  1.021  \n",
       "10  the dog was sweeping a parking deck near where...    0   0.000  1.246  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = data['articles'].astype('str')\n",
    "abstracts = data['abstracts'].astype('str')\n",
    "similarities = data['similarity']\n",
    "\n",
    "NewDF = pd.DataFrame({\"article\":articles,\"summary\":abstracts, \"sim\":similarities})\n",
    "New_DF_Acotado = NewDF.iloc[0:11]\n",
    "\n",
    "New_DF_Acotado['Cosine'] = New_DF_Acotado.apply(lambda row: round(get_cosine(row['article'],row['summary']),3), axis=1)\n",
    "New_DF_Acotado['WMD'] = New_DF_Acotado.apply(lambda row: round(WMD(row['article'],row['summary']),3), axis=1)\n",
    "\n",
    "New_DF_Acotado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 5 - Keyword extraction using TF*IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son 11 pasos de ejemplo.\n",
    "\n",
    "#### En resumen: Document -> Remove stop words -> Find Term Frequency (TF) -> Find Inverse Document Frequency (IDF) -> Find TF*IDF -> Get top N Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importamos los paquetes necesarios.\n",
    "\n",
    "We need to tokenize to create word tokens, itemgetter to sort the dictionary, and math to perform log base e operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Declaracion de variables.\n",
    "\n",
    "We will declare a string variable. It will be a placeholder for the sample text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'I am a graduate. I want to learn Python. I like learning Python. Python is easy. Python is interesting. Learning increases thinking. Everyone should invest time in learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Removemos las stopwords\n",
    "\n",
    "Stopwords are the frequently occurring words that may not carry significance to our analysis. We can remove the using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Encontramos el número total de palabras (total_words) en el documento. \n",
    "\n",
    "This will be required while calculating Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "total_words = doc.split()\n",
    "total_word_length = len(total_words)\n",
    "print(total_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Encontramos el número total de oraciones (total_sent_len).\n",
    "\n",
    "This will be required while calculating Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "total_sentences = tokenize.sent_tokenize(doc)\n",
    "total_sent_len = len(total_sentences)\n",
    "print(total_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Calculamos TF para cada palabra\n",
    "\n",
    "We will begin by calculating the word count for each non-stop words and finally divide each element by the result of step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0.10714285714285714, 'graduate': 0.03571428571428571, 'want': 0.03571428571428571, 'learn': 0.03571428571428571, 'Python': 0.14285714285714285, 'like': 0.03571428571428571, 'learning': 0.07142857142857142, 'easy': 0.03571428571428571, 'interesting': 0.03571428571428571, 'Learning': 0.03571428571428571, 'increases': 0.03571428571428571, 'thinking': 0.03571428571428571, 'Everyone': 0.03571428571428571, 'invest': 0.03571428571428571, 'time': 0.03571428571428571}\n"
     ]
    }
   ],
   "source": [
    "tf_score = {}\n",
    "for each_word in total_words:\n",
    "    each_word = each_word.replace('.','')\n",
    "    if each_word not in stop_words:\n",
    "        if each_word in tf_score:\n",
    "            tf_score[each_word] += 1\n",
    "        else:\n",
    "            tf_score[each_word] = 1\n",
    "            \n",
    "# Dividing by total_word_length for each dictionary element\n",
    "tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "print(tf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Funcion para chequear si la palabra (word) está presente en la lista de oraciones (sentences).\n",
    "\n",
    "This method will be required when calculating IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent(word, sentences): \n",
    "    final = [all([w in x for w in word]) for x in sentences] \n",
    "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    return int(len(sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Calculamos IDF para cada palabra,\n",
    "\n",
    "We will use the function in step 7 to iterate the non-stop word and store the result for Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0.8472978603872037, 'graduate': 1.9459101490553132, 'want': 1.9459101490553132, 'learn': 1.9459101490553132, 'Python': 0.5596157879354227, 'like': 1.9459101490553132, 'learning': 1.252762968495368, 'easy': 1.9459101490553132, 'interesting': 1.9459101490553132, 'Learning': 1.9459101490553132, 'increases': 1.9459101490553132, 'thinking': 1.9459101490553132, 'Everyone': 1.9459101490553132, 'invest': 1.9459101490553132, 'time': 1.9459101490553132}\n"
     ]
    }
   ],
   "source": [
    "idf_score = {}\n",
    "for each_word in total_words:\n",
    "    each_word = each_word.replace('.','')\n",
    "    if each_word not in stop_words:\n",
    "        if each_word in idf_score:\n",
    "            idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "        else:\n",
    "            idf_score[each_word] = 1\n",
    "\n",
    "# Performing a log and divide\n",
    "idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "print(idf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Calculamos TF * IDF\n",
    "\n",
    "Since the key of both the dictionary is the same, we can iterate one dictionary to get the keys and multiply the values of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0.09078191361291467, 'graduate': 0.06949679103768976, 'want': 0.06949679103768976, 'learn': 0.06949679103768976, 'Python': 0.07994511256220323, 'like': 0.06949679103768976, 'learning': 0.08948306917824057, 'easy': 0.06949679103768976, 'interesting': 0.06949679103768976, 'Learning': 0.06949679103768976, 'increases': 0.06949679103768976, 'thinking': 0.06949679103768976, 'Everyone': 0.06949679103768976, 'invest': 0.06949679103768976, 'time': 0.06949679103768976}\n"
     ]
    }
   ],
   "source": [
    "tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "print(tf_idf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Creamos una funcion para obtener las N más importantes palabras en el documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(dict_elem, n):\n",
    "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Como prueba obtenemos las 5 palabras top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0.09078191361291467, 'learning': 0.08948306917824057, 'Python': 0.07994511256220323, 'graduate': 0.06949679103768976, 'want': 0.06949679103768976}\n"
     ]
    }
   ],
   "source": [
    "print(get_top_n(tf_idf_score, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
