\babel@toc {spanish}{}
\contentsline {section}{\numberline {1}Introducción.}{7}{section.1}%
\contentsline {subsection}{\numberline {1.1}Algoritmos y técnicas utilizadas.}{9}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Objetivos del Proyecto.}{10}{subsection.1.2}%
\contentsline {subsubsection}{\numberline {1.2.1}Objetivo general.}{10}{subsubsection.1.2.1}%
\contentsline {subsubsection}{\numberline {1.2.2}Objetivos específicos.}{10}{subsubsection.1.2.2}%
\contentsline {subsection}{\numberline {1.3}Alcance del Proyecto.}{11}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Organización.}{11}{subsection.1.4}%
\contentsline {section}{\numberline {2}Reclutamiento y selección laboral.}{13}{section.2}%
\contentsline {subsection}{\numberline {2.1}Introducción.}{13}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Reclutamiento vs selección.}{13}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Evolución de los procesos de reclutamiento y selección laboral.}{15}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Cribado o screening.}{16}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Screening manual vs screening automatizado}{16}{subsubsection.2.4.1}%
\contentsline {subsection}{\numberline {2.5}Sistemas de screening: Estado del arte.}{17}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}Enfoque del Proyecto.}{19}{subsection.2.6}%
\contentsline {section}{\numberline {3}Algoritmos de Machine Learning.}{20}{section.3}%
\contentsline {subsection}{\numberline {3.1}Introducción.}{20}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Machine Learning (ML).}{20}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Aprendizaje supervisado y no supervisado.}{21}{subsubsection.3.2.1}%
\contentsline {paragraph}{\numberline {3.2.1.1}Regresión.}{22}{paragraph.3.2.1.1}%
\contentsline {paragraph}{\numberline {3.2.1.2}Clasificación.}{23}{paragraph.3.2.1.2}%
\contentsline {paragraph}{\numberline {3.2.1.3}Agrupación (clustering).}{24}{paragraph.3.2.1.3}%
\contentsline {paragraph}{\numberline {3.2.1.4}Algoritmos más conocidos.}{25}{paragraph.3.2.1.4}%
\contentsline {subsubsection}{\numberline {3.2.2}Aprendizaje transductivo}{26}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Separación de los datos.}{27}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}¿Cómo implementar un modelo de ML?}{28}{subsubsection.3.2.4}%
\contentsline {paragraph}{\numberline {3.2.4.1}Cross Validation.}{32}{paragraph.3.2.4.1}%
\contentsline {paragraph}{\numberline {3.2.4.2}Los Problemas de ML: Overfitting y Underfitting.}{33}{paragraph.3.2.4.2}%
\contentsline {subsection}{\numberline {3.3}K-Nearest Neighbor (KNN).}{34}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}Principal limitación KNN.}{35}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}Funcionamiento y ejemplo de KNN.}{35}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}Métrica de distancia a emplear.}{36}{subsubsection.3.3.3}%
\contentsline {subsubsection}{\numberline {3.3.4}Eligiendo el valor de k: overfitting y underfitting.}{38}{subsubsection.3.3.4}%
\contentsline {subsection}{\numberline {3.4}K-means.}{40}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}Funcionamiento y ejemplo de K-means.}{40}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}Objetivo de k-means y su función de coste.}{42}{subsubsection.3.4.2}%
\contentsline {subsubsection}{\numberline {3.4.3}Limitaciones K-means.}{44}{subsubsection.3.4.3}%
\contentsline {paragraph}{\numberline {3.4.3.1}Obtención del k mediante Elbow Method.}{45}{paragraph.3.4.3.1}%
\contentsline {paragraph}{\numberline {3.4.3.2}Inicialización de los centroides: k-means++.}{46}{paragraph.3.4.3.2}%
\contentsline {subsection}{\numberline {3.5}Redes Neuronales Artificiales (ANN).}{48}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}Perceptrón simple.}{48}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}Desventaja del Perceptrón simple. }{50}{subsubsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.3}Perceptrón multicapa (MLP).}{51}{subsubsection.3.5.3}%
\contentsline {subsubsection}{\numberline {3.5.4}Funciones de activación en MLP.}{53}{subsubsection.3.5.4}%
\contentsline {paragraph}{\numberline {3.5.4.1}Softmax en la capa de salida.}{54}{paragraph.3.5.4.1}%
\contentsline {paragraph}{\numberline {3.5.4.2}Sigmoide en la capa de salida.}{55}{paragraph.3.5.4.2}%
\contentsline {subsubsection}{\numberline {3.5.5}Entrenamiento en una red neuronal.}{56}{subsubsection.3.5.5}%
\contentsline {paragraph}{\numberline {3.5.5.1}Hiper-parámetro n.}{58}{paragraph.3.5.5.1}%
\contentsline {section}{\numberline {4}Natural Language Processing.}{59}{section.4}%
\contentsline {subsection}{\numberline {4.1}Introducción.}{59}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Aplicaciones NLP.}{60}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}NLP en la práctica.}{62}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Preprocesamiento de textos.}{63}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Limpieza y normalización.}{64}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}Tokenización.}{65}{subsubsection.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.3}Stemming y lemmatization.}{66}{subsubsection.4.4.3}%
\contentsline {subsubsection}{\numberline {4.4.4}N-grams.}{67}{subsubsection.4.4.4}%
\contentsline {subsection}{\numberline {4.5}Obtención de representaciones vectoriales.}{67}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Bag of Words (BoW).}{68}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}TF-IDF.}{70}{subsubsection.4.5.2}%
\contentsline {subsubsection}{\numberline {4.5.3}Desventajas BoW \& TF-IDF.}{72}{subsubsection.4.5.3}%
\contentsline {subsubsection}{\numberline {4.5.4}Word embeddings.}{73}{subsubsection.4.5.4}%
\contentsline {paragraph}{\numberline {4.5.4.1}Embeddings.}{73}{paragraph.4.5.4.1}%
\contentsline {paragraph}{\numberline {4.5.4.2}Word2vec.}{75}{paragraph.4.5.4.2}%
\contentsline {paragraph}{\numberline {4.5.4.3}¿Cómo obtener nuestros Word embeddings?}{77}{paragraph.4.5.4.3}%
\contentsline {paragraph}{\numberline {4.5.4.4}CBOW vs Skipgram.}{78}{paragraph.4.5.4.4}%
\contentsline {paragraph}{\numberline {4.5.4.5}One hot encoding.}{80}{paragraph.4.5.4.5}%
\contentsline {paragraph}{\numberline {4.5.4.6}Obteniendo nuestros Word embeddings con Skipgram.}{82}{paragraph.4.5.4.6}%
\contentsline {paragraph}{\numberline {4.5.4.7}Arquitectura del modelo Skipgram.}{85}{paragraph.4.5.4.7}%
\contentsline {paragraph}{\numberline {4.5.4.8}Entrenamiento y función de costo con Softmax.}{87}{paragraph.4.5.4.8}%
\contentsline {paragraph}{\numberline {4.5.4.9}Optimizaciones: Muestreo Negativo.}{91}{paragraph.4.5.4.9}%
\contentsline {paragraph}{\numberline {4.5.4.10}Entrenamiento y función de costo con Sigmoide.}{94}{paragraph.4.5.4.10}%
\contentsline {paragraph}{\numberline {4.5.4.11}Desventajas Word2Vec.}{96}{paragraph.4.5.4.11}%
\contentsline {subsection}{\numberline {4.6}Obtención de las mediciones de similitud entre textos.}{98}{subsection.4.6}%
\contentsline {subsubsection}{\numberline {4.6.1}Maneras de medir la similitud entre textos.}{99}{subsubsection.4.6.1}%
\contentsline {subsubsection}{\numberline {4.6.2}¿Por qué decidimos utilizar Cosine Similarity y WMD?}{104}{subsubsection.4.6.2}%
\contentsline {subsubsection}{\numberline {4.6.3}Cosine Similarity.}{105}{subsubsection.4.6.3}%
\contentsline {subsubsection}{\numberline {4.6.4}Word Mover’s Distance (WMD).}{107}{subsubsection.4.6.4}%
\contentsline {section}{\numberline {5}Implementación.}{110}{section.5}%
\contentsline {subsection}{\numberline {5.1}Obtención del modelo de clasificación.}{111}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Esquema.}{111}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Obtención de sets de datos.}{111}{subsubsection.5.1.2}%
\contentsline {paragraph}{\numberline {5.1.2.1}Curriculum Vitae.}{112}{paragraph.5.1.2.1}%
\contentsline {paragraph}{\numberline {5.1.2.2}Descripciones Puestos Laborales.}{114}{paragraph.5.1.2.2}%
\contentsline {subsubsection}{\numberline {5.1.3}Preprocesamiento de textos.}{115}{subsubsection.5.1.3}%
\contentsline {subsubsection}{\numberline {5.1.4}Cantidad final del set de datos y su uso en las distintas etapas.}{119}{subsubsection.5.1.4}%
\contentsline {subsection}{\numberline {5.2}Comparando textos y obteniendo similitudes.}{121}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}TF-IDF \& Cosine Similarity.}{121}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Word embeddings (Word2vec) \& WMD.}{123}{subsubsection.5.2.2}%
\contentsline {paragraph}{\numberline {5.2.2.1}Elección de hiper-parámetros Word2vec.}{124}{paragraph.5.2.2.1}%
\contentsline {paragraph}{\numberline {5.2.2.2}Word Mover's Distance (WMD).}{129}{paragraph.5.2.2.2}%
\contentsline {paragraph}{\numberline {5.2.2.3}Observaciones valores de Cosine Similarity y WMD.}{131}{paragraph.5.2.2.3}%
\contentsline {subsection}{\numberline {5.3}Armado del modelo de clasificación KNN.}{133}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}Preparación de los datos.}{133}{subsubsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.2}K-means.}{135}{subsubsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.3}KNN.}{138}{subsubsection.5.3.3}%
\contentsline {subsection}{\numberline {5.4}Clasificación de nuevas muestras y resultados obtenidos.}{143}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Integración al Sistema Web.}{145}{subsection.5.5}%
\contentsline {subsection}{\numberline {5.6}Integración al Sistema Web.}{145}{subsection.5.6}%
\contentsline {subsubsection}{\numberline {5.6.1}Django.}{146}{subsubsection.5.6.1}%
\contentsline {paragraph}{\numberline {5.6.1.1}Ventajas de Django.}{146}{paragraph.5.6.1.1}%
\contentsline {paragraph}{\numberline {5.6.1.2}Arquitectura en Django.}{147}{paragraph.5.6.1.2}%
\contentsline {subsubsection}{\numberline {5.6.2}Base de datos.}{148}{subsubsection.5.6.2}%
\contentsline {subsubsection}{\numberline {5.6.3}Secciones del sistema}{151}{subsubsection.5.6.3}%
\contentsline {subsubsection}{\numberline {5.6.4}Manejo de los datos.}{154}{subsubsection.5.6.4}%
\contentsline {paragraph}{\numberline {5.6.4.1}Modelado.}{155}{paragraph.5.6.4.1}%
\contentsline {paragraph}{\numberline {5.6.4.2}Filtrado.}{156}{paragraph.5.6.4.2}%
\contentsline {paragraph}{\numberline {5.6.4.3}Visualización.}{157}{paragraph.5.6.4.3}%
\contentsline {subsection}{\numberline {5.7}Workflow final del Sistema.}{158}{subsection.5.7}%
\contentsline {subsection}{\numberline {5.8}Caso de Uso.}{159}{subsection.5.8}%
\contentsline {subsection}{\numberline {5.9}Limitaciones del sistema.}{160}{subsection.5.9}%
\contentsline {section}{\numberline {6}Conclusiones.}{161}{section.6}%
\contentsline {section}{\numberline {7}Anexos.}{162}{section.7}%
\contentsline {subsection}{\numberline {7.1}Ejemplo de funcionamiento KNN.}{162}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Ejemplo de funcionamiento K-means.}{163}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Posición inicial de los centroides en K-means.}{166}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Funciones de activación.}{168}{subsection.7.4}%
\contentsline {subsection}{\numberline {7.5}Ejemplo de obtención de Word embeddings mediante Skipgram y softmax.}{169}{subsection.7.5}%
\contentsline {subsection}{\numberline {7.6}Palabras repetidas en nuestro Corpus y palabras polisémicas.}{177}{subsection.7.6}%
