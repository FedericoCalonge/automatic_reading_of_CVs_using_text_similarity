% ----------------------------------
% 1-Preambulo.
% ----------------------------------
\documentclass[12pt,a4paper]{article}
\usepackage[spanish]{babel}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[procnames]{listings} 	%Para escribir códigos.
% OJO: se agregaron procnames para usarlos en Python (VER).

\usepackage[bottom]{footmisc} 	 	%Para poner las footnote al final de cada página.
\usepackage[hidelinks]{hyperref} 	%Para que el indice pueda ser linkeado.
\usepackage{amssymb}			 	%Para ecuaciones matemáticas.
\usepackage{amsmath}				%Para matrices.
\usepackage{mathtools}
\usepackage{amsfonts} 
\usepackage{verbatim}				%Para usar comentarios.
\parskip 0.1in 						%Distancia parrafos.

%Biliografías:
%\usepackage[style=authoryear]{biblatex}
%\addbibresource{bibliografias.bib}

\usepackage{float} 							%Para que no se muevan las imágenes de lugar.

\usepackage[
  separate-uncertainty = true,
  multi-part-units = repeat
]{siunitx} 									%Para el \SI del +- .

\usepackage[margin=0.984252in]{geometry} 	%Para los márgenes.
\usepackage{subcaption}
\usepackage{appendix} 						%Para los anexos.

% ----------------------------------
% 1.1-Anexos.
% ----------------------------------

%begin anexos
\makeatletter
\def\@seccntformat#1{\@ifundefined{#1@cntformat}  	%"\@seccntformat" es un comando auxiliar.
   {\csname the#1\endcsname\quad}  					%Default.
   {\csname #1@cntformat\endcsname}					%Enable individual control.
}

\let\oldappendix\appendix 							%Guarda la definicion vigente de \appendix
\renewcommand\appendix{%
    \oldappendix
    \newcommand{\section@cntformat}{\appendixname~\thesection\quad}
}
\makeatother
%\renewcommand{\appendixname}{Anexos}
%\renewcommand{\appendixtocname}{Anexos}
%\renewcommand{\appendixpagename}{Anexos}
%end anexos

% ----------------------------------
% 1.2-Para código Python. 
% ----------------------------------
\usepackage{color}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}

% ----------------------------------
% 1.3-Índice. 
% ----------------------------------

\setcounter{secnumdepth}{3} 		%Para que ponga 1.1.1.1.
\setcounter{tocdepth}{4} 			%Para que añadir las secciones en el Índice.
\usepackage{chngcntr}				%Para que el número de las figuras esten acordes a la sección.
\counterwithin{figure}{section}

\author{
  Calonge, Federico Matias\\
  \text{calongefederico@gmail.com}
}

\title{
  Tesis \\
  \large Automatización de lectura de Currículum Vitae  \\
    para selección de personal en el Sector IT}
    
%Para modificar los parrafos y para que se pueda poner subsections:
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
            {-2.5ex\@plus -1ex \@minus -.25ex}
            {1.25ex \@plus .25ex}
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4} 	%How many sectioning levels to assign numbers to.
\setcounter{tocdepth}{4}    	%How many sectioning levels to show in ToC.
% ----------------------------------
% 2-Documento
% ----------------------------------

\begin{document}

\begin{figure}
  \centering
  \includegraphics[width=0.2\textwidth]{images/undav-logo.png} 	%Incluyendo logo de la Undav.
  \label{fig:undav-logo}
\end{figure}
\maketitle       		%Para generar el título definido arriba.

\cleardoublepage    %Nueva página

\begin{center}
    \Large
    \vspace{0.9cm}
    \textbf{Resumen}
    
\end{center}

En la Tesis de Ingeniería en Informática que se presenta, se diseña un \textit{sistema de lectura automática de Curriculum Vitae}. La finalidad del mismo es ayudar al reclutador laboral a elegir a los mejores candidatos para los puestos laborales que tenga disponible mediante una medición de similitud entre textos: Curriculum Vitae de los candidatos por un lado, y descripciones de puestos laborales por el otro.
El sistema esta desarrollado utilizando el lenguaje de programación Python, permitiendo verificar la teoría desarrollada.

\begin{center}
    \Large
    \vspace{0.9cm}
    \textbf{Abstract}
\end{center}

This Computer Engineering Thesis introduces an \textit{automatic Curriculum Vitae reading system}. The purpose of it is to help the job recruiter to choose the best candidates for the available job positions by measuring similarity between texts: Curriculum Vitae of the candidates on the one hand, and job descriptions on the other. The system is developed using the Python programming language allowing to verify the developed theory.

\cleardoublepage    %Nueva página

\tableofcontents 	%Para insertar el índice general.

\cleardoublepage    %Nueva página

\section{Introducción.}
El proceso de \textbf{selección de personal} se ha vuelto crucial para el manejo de recursos humanos en el mundo laboral moderno. Con la transformación digital de las empresas y del mercado laboral en general, identificar los perfiles más acordes a las necesidades de la empresa se convirtió en uno de los retos más ambiciosos de Recursos Humanos, en especial cuando hablamos del \textbf{Sector IT}. 

En estos últimos años se implementaron una gran cantidad de \textbf{herramientas de Software que permiten automatizar y gestionar información de los candidatos de una manera mucho más intuitiva e inteligente}. Gracias a la ayuda de este tipo de sistemas, el reclutador consigue a los candidatos más cualificados para cada puesto.

El tema de este Proyecto de Tesis será desarrollar un \textbf{Sistema de lectura automática de Curriculum Vitae} que ayude al reclutador laboral a elegir a los mejores candidatos para los puestos laborales que tenga disponible mediante una \textbf{medición de similitud} entre textos: los Curriculum Vitae de los candidatos por un lado, y las descripciones de puestos laborales por el otro.

La medición de similitudes de documentos es uno de los problemas más cruciales del \textbf{Procesamiento del Lenguaje Natural (NLP)}. Encontrar similitudes entre documentos se utiliza en varios dominios, tales como recomendación de películas, libros o artículos similares, identificación de documentos plagiados o documentos legales, etc. 

Para que las máquinas puedan descubrir esta similitud entre documentos, se necesita definir una forma de medir matemáticamente la similitud, la cual debe ser comparable para que la máquina pueda identificar qué documentos son más similares (o menos). Previamente a esto necesitamos representar el texto de los documentos en una forma cuantificable (que suele ser en forma vectorial), de modo que podamos realizar cálculos de similitud sobre él.

Por lo tanto, los pasos necesarios para que las máquinas puedan medir la similitud entre documentos son:
\begin{enumerate}
\item Convertir un documento en un objeto matemático (vector).
\item Definir y emplear una medida de similitud.
\end{enumerate}

Para el primer paso se utilizarán los algoritmos de vectorización \textbf{TF-IDF} y \textbf{Word Embeddings}; y para el segundo paso se emplearán las técnicas \textbf{Cosine Similarity} y \textbf{Word Mover's Distance -WMD-}.

\colorbox{red}{Ver si agregar KNN y Kmeans}\\

\cleardoublepage    %Nueva página

\subsection{Objetivos del Proyecto.}

\subsubsection{Objetivo general.}
El objetivo de este Proyecto de Tesis es lograr la implementación de un Sistema de lectura automática de Curriculum Vitae accesible vía Web, que servirá para la elección de los mejores candidatos para cada puesto laboral; basándose en la \textbf{similitud} entre el Currículum Vitae del candidato y el puesto laboral. 

\subsubsection{Objetivos específicos.}
Los objetivos específicos de este Proyecto de Tesis son:
\begin{itemize}
\item Describir el estado del arte actual de los Sistemas de lectura y análisis de Curriculum Vitae en Recruiting. 
\item Implementar un Sistema de lectura automática de Curriculum Vitae basado en la comparación entre textos, para finalmente obtener una visualización de los mejores candidatos para un puesto laboral determinado.  
\item Aprender los conceptos y técnicas principales utilizadas dentro del procesamiento de lenguaje natural (NLP) aplicando técnicas de preprocesamiento y limpieza de textos.
\item Implementar diferentes técnicas para medir similitudes entre los textos (Cosine Similarity y  Word Mover's Distance -WMD-) y diferentes algoritmos de vectorización (TF-IDF y Word Embeddings), analizando su funcionamiento tanto teórica como matemáticamente, ventajas y desventajas.
\item Evaluar los resultados de cada técnica y algoritmo e implementar la mejor solución en el Sistema.
\item Evaluar los Frameworks disponibles para tener una UI accesible vía web e integrar el mismo al Sistema.
\item Almacenar datos de Candidatos, Reclutadores y puestos laborales en una base de datos; contando con un sistema de login y registración para los mismos.
\end{itemize} 


\cleardoublepage    %Nueva página

\subsection{Alcance del Proyecto.}
El alcance de esta Tesis de Grado de Ingeniería incluye el desarrollo de conceptos de análisis de datos, procesamiento de lenguaje natural, técnicas de preprocesamiento y limpieza de los datos, algoritmos de vectorización y técnicas para medir la similitud entre textos, integración con frameworks, visualización de datos, y gestión de Base de Datos, de acuerdo a lo enunciado en los objetivos específicos.

\subsection{Organización.}
Este Proyecto de Tesis consiste en tres grandes secciones:
\begin{enumerate}

\item Análisis e investigación inicial. Esta sección abarca principalmente la parte teórica del trabajo, haciendo hincapié en el análisis e investigación de:
\begin{itemize}
	\item El estado del arte (actual y pasado) de los sistemas de lectura y análisis de Curriculum Vitae.
	\item Proyectos y librerias existentes.
	\item Técnicas usadas para el procesamiento del lenguaje natural (NLP).
	\item Técnicas para medir similitudes entre textos:  .... y .... .
	\item Algoritmos de Vectorización: .... y .... .
\end{itemize} 

\colorbox{red}{Agregar KNN y Kmeans}\\

\item Investigación y desarrollo del algoritmo core del sistema. Esta sección hace referencia a la aplicación práctica dentro del marco teórico desarrollado en la primera sección, la cual abarca:
\begin{itemize}
	\item Obtención de sets de datos: curriculums vitaes y descripciones laborales. 
	\item Preprocesamiento de los textos.
	\item Implementación del algoritmo CORE: utilizando las técnicas para medir similitudes entre textos (... y ....) y algoritmos de vectorización (... y ...).
	\item Pruebas entre distintas técnicas y librerías disponibles. 
	\item Análisis y primeras visualizaciones de los resultados.
\end{itemize} 

\colorbox{red}{Agregar KNN y Kmeans}\\
	
\item Investigación y desarrollo de la parte funcional del sistema. Esta última sección abarca lo que es la interfaz de usuario y su integración a un sistema web capaz de manejar las actividades entre los Candidatos y Reclutadores dentro del sistema. Los items principales son:
\begin{itemize}
	\item Definición de usuarios y roles.
	\item Integración de frameworks y bases de datos.  
	\item Modelado, filtrado y visualización de los datos.
\end{itemize} 

\end{enumerate}

\colorbox{red}{VER si poner de primero hacer análisis en Jupyter notebooks -sección 2-}\\
\colorbox{red}{y despues la parte web en django integrando estos jupyters -sección 3-}\\

\cleardoublepage    %Nueva página

\section{Reclutamiento laboral en IT.}
En este capítulo se va a realizar un acercamiento a la historia del Reclutamiento laboral, especialmente en el sector de IT, sus etapas y problemáticas.
Luego se realizará un análisis en el Estado de Arte actual de los Sistemas de lectura y análisis de CV actuales.

\subsection{Introducción.}
\colorbox{red}{FALTA}

\subsection{Problemáticas.}
\colorbox{red}{FALTA}

\subsection{Sistemas de lectura y análisis de CV : Estado del arte.}
\colorbox{red}{FALTA}

\section{Aprendizaje supervisado y no supervisado en Machine Learning.}
Para comenzar el marco teórico de esta tesis, es necesario explicar qué es Machine Learning y cómo se pueden clasificar a los distintos algoritmos según su tipo de aprendizaje. 
En este capítulo se va a explicar el objetivo del aprendizaje supervisado y no supervisado en Machine Leanring haciendo énfasis en los algoritmos K-Nearest Neighbor (KNN) y K-means.

\subsection{Introducción.}
\colorbox{red}{FALTA}
Los métodos que utilizaremos en este Proyecto de Tesis serán...

\subsection{K-Nearest Neighbor (KNN).}
\colorbox{red}{FALTA}
K-Nearest Neighbor (o K Vecinos más Próximos en español), blablabal....

\subsection{K-Means.}
\colorbox{red}{FALTA}
K-Means (o K-Medias en español), blablaba...

\subsubsection{Elbow Method.}
\colorbox{red}{FALTA}
Elbow Method (o Método del codo en español), blablaba...

\section{Natural Language Processing.}
\colorbox{red}{FALTA}

\subsection{Introducción.}
\colorbox{red}{FALTA}

\subsection{Preprocesamiento de textos.}
\colorbox{red}{FALTA}

\subsection{Similitud entre textos.}
\colorbox{red}{FALTA}

\subsection{Técnicas para medir Similitud entre textos.}
\colorbox{red}{FALTA}

\subsubsection{Cosine Similarity.}
\colorbox{red}{FALTA}

\subsubsection{Word Mover's Distance (WMD).}
\colorbox{red}{FALTA}

\subsection{Algoritmos de vectorización.}
\colorbox{red}{FALTA}

\subsubsection{TF-IDF.}
\colorbox{red}{FALTA}

\subsubsection{Word Embeddings.}
\colorbox{red}{FALTA}

\paragraph{¿Cómo entrenar Word Embeddings?}
\colorbox{red}{FALTA}

\section{Desarrollo.}
\colorbox{red}{FALTA}
El desarrollo de este Sistema se trabajó en dos grandes partes:
	1-Obtención del modelo predictivo. En esta primera parte se obtuvieron y preprocesaron datasets de Curriculum Vitae de distintos Candidatos y Descripciones de puestos de trabajo de IT publicados por distintas empresas, para luego ser comparados y obtener similitudes entre los textos utilizando las técnicas para medir distancias y obtener dichas similitudes (WMD y Cosine Similarity) y los algoritmos de aprendizaje (KNN y K-Means) anteriormente mencionados. De esta manera, el resultado final es la obtención de un modelo predictivo capaz de predecir nuevas muestras (en nuestro caso, predecir nuevos Candidatos para los distintos puestos laborales de IT disponibles).
		
	2-Integración al Sistema Web: Esta fue una etapa posterior a la primera parte, donde se integró el modelo junto con la lógica de los algoritmos utilizados, a un Sistema Web. De esta manera, nuestro sistema cuenta con una interfaz gráfica permitiendo interactuar entre Candidatos y Reclutadores. 
	
\subsection{Obtención del modelo predictivo.}

\subsubsection{Introducción.}
Como inicio definamos qué es un modelo. En nuestro caso, un modelo representa ....
Este modelo se construyó en base a ...
Que sea predictivo quiere decir que...
El desarrollo de este Sistema se realizó en Python...

\subsubsection{Esquema.}
Como podemos ver la figura \ref{fig:FlowCoreSystem} representa el procedimiento utilizado para la obtención de nuestro modelo predictivo. 

\begin{figure}[h]    %[h] es para que se ubique justo debajo del texto anterior. 
  \centering
  \includegraphics[width=1\textwidth]{images/flow-core.png} 	%Incluyendo imagen Flow Core.
  \caption{Flow del Core del Sistema}  
  \label{fig:FlowCoreSystem}
\end{figure}

\subsubsection{Obtención de sets de datos.}
En primer lugar debemos definir qué es un set o conjunto de datos.
Un set o conjunto de datos es una tabla de una base de datos o, matemáticamente, una matriz estadística de datos. Cada columna de la tabla representa una variable del set de datos; y cada fila representa a un miembro determinado del mismo.

Para este Proyecto utilizamos dos grandes sets de datos que se obtuvieron mediante la recolección de distintos archivos alojados en la Web, los cuales estan descriptos a continuación.

\paragraph{Curriculum Vitae.}
Los set de datos de Curriculum Vitae de Candidatos se obtuvieron de las siguientes fuentes:

\begin{enumerate}
\item 228 Curriculums en formato docx y posteriormente convertidos a pdf, obtenidos del sitio Kaggle (https://www.kaggle.com/palaksood97/resume-dataset). Estos pdfs son Candidatos de la India con experiencia en el rubro de IT.
\item 2484 Curriculums en formato CSV, obtenidos del sitio Kagle (https://www.kaggle.com/snehaanbhawal/resume-dataset). Este CSV cuenta con CVs obtenidos del sitio web de postulación de trabajos 'livecareer.com'.
\item 962 Curriculumns en formato CSV, obtenidos del sitio Kaggle (https://www.kaggle.com/gauravduttakiit/resume-dataset). Este CSV cuenta con CVs repartidos en distintas categorías de IT.
\item 10 Curriculums en formato PDF, los cuales los cuales fueron obtenidos como ejemplos mediante una recolección propia de distintos sitios web. 
\end{enumerate}

\paragraph{Descripciones Puestos Laborales.}

Los set de datos de descripciones de puestos laborales se obtuvieron de las siguientes fuentes:

\begin{enumerate}
\item 22.000 descripciones en formato CSV; obtenido del sitio Kaggle (https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom). El CSV cuenta con descripciones de puestos obtenidos del sitio web de USA de postulación de trabajos del rubro de IT 'Dice.com'.
\item 10 descripciones en formato CSV; obtenidas como ejemplos mediante una recolección propia del sitio Indeed (https://www.indeed.com/q-USA-jobs.html) para puestos de trabajo de IT.
\end{enumerate}
 

\subsubsection{Preprocesamiento y limpieza de datos.}
Previamente a utilizar las técnicas para medir distancias y obtener similitudes entre textos (WMD y Cosine Similarity) y los algoritmos de aprendizaje (KNN y K-Means) necesitamos que los datos que comparemos e introduzcamos en los algoritmos estén lo más limpios posible; ya que de lo contrario las mismos podrían clasificar o predecir de forma errónea. Este análisis previo sobre los datos debe ser minucioso ya que puede haber valores incoherentes o absurdos.

El procedimiento para la Limpieza de los Curriculum Vitae y las descripciones de los puestos laborales fue el siguiente:

\begin{enumerate}
\item Convertimos todo a minúscula.
\item Eliminamos datos no relevantes para nuestros análisis (mails y páginas web).
\item Eliminamos signos de puntuación y caracterés especiales (incluyendo números).
\item Eliminamos stop words.
\item Eliminamos common words no relevantes para nuestros análisis.
\item Aplicamos Lematización y Tokenización.
\item Eliminamos repetidos.
\item Obtenemos y usamos bi-gramas.
\end{enumerate}

Luego de aplicar preprocesamiento y limpieza de datos nos quedarán los siguientes tamaños de nuestros datasets:
\begin{itemize}
\item 624 CVs de Candidatos (en formato pdf y csv).
\item 20593 Descripciones de Puestos de IT (en formato csv).
\end{itemize}

\subsubsection{Cantidad final del set de datos y su uso en las distintas etapas.}

El total de CVs de Candidatos (624) y Descripciones de Puestos de IT (20593) que mencionamos previamente, serán utilizados para el entrenamiento y obtención de vectores mediante TF-IDF (para el futuro cálculo de Cosine Similarity) y para el entrenamiento de Word2Vec y obtención de los Word Embeddings (para el futuro cálculo de WMD).

Por otro lado, para calcular Cosine Similarity y WMD, para utilizarlos en K-means y para entrenar a nuestro algoritmo KNN, utilizaremos únicamente una porción de nuestros datasets:

    1-Para el cálculo de Cosine Similarity y WMD:
        301 CVs de Candidatos.
        201 Descripciones de Puestos de IT.
        Nota: No obstante, al realizar los cálculos de distancias compararemos cada CV con cada Job Description, obteniendo un dataframe total de 3131 filas con sus respectivo valores de WMD y Cosine Sim.

    2-Para el uso de K-means y entrenamiento con KNN (eliminamos un CV y una Descripción de Puesto IT que los utilizamos en '3-'):
        300 CVs de Candidatos.
        200 Descripciones de Puestos de IT.
        Nota: como se comentó previamente, nos quedarán 3000 filas / puntos para usar en K-means y entrenar KNN; llegando a representar estos 3000 puntos en un plano de 2 dimensiones.

    3-Para la clasificación de nuevas muestras prediciendo con KNN:
        1 CV de Candidatos.
        1 Descripción de Puesto de IT.
        Nota: como se comentó previamente, nos quedarán 131 filas para clasificar.

¿Por qué utilizamos solo una porción de nuestros datasets?: Esto es debido a los drawbacks de WMD y KNN.

    WMD: posee una alta complejidad en el cálculo de la distancia, teniendo un tiempo de ejecución muy elevado. Como ejemplo, al correrlo localmente, el cálculo de WMD para 3131 filas tardó 7 horas; frente a los 3 segundos que tardó el cálculo de Cosine Similarity para la misma cantidad de filas.
    KNN: KNN es una gran opción para datasets pequeños con pocas variables de entrada; pero tiene problemas cuando la cantidad de entradas es muy grande. Cada variable de entrada puede considerarse una dimensión de un espacio de entrada p-dimensional. En grandes dimensiones, los puntos que pueden ser similares pueden tener distancias muy grandes. Además, cada vez que se va a hacer una predicción con KNN, busca al vecino más cercano en el conjunto de entrenamiento completo. Por esto, se debe utilizar un dataset pequeño para que el clasificador KNN completa su ejecución rápidamente.

En conclusión, al utilizar solo una porción de nuestros datasets para obtener los distintos cálculos de distancias y entrenar KNN, el cálculo de WMD se podrá realizar en un tiempo finito, y nuestro clasificador KNN funcionará rápida y eficientemente al realizar predicciones.

\subsection{Comparando textos y obteniendo similitudes.}
\colorbox{red}{FALTA}

\subsection{Armado del modelo predictivo KNN.}
\colorbox{red}{FALTA}

\subsection{Predicción de nuevas muestras y resultados obtenidos.}
\colorbox{red}{FALTA}

\subsection{Integración al Sistema Web.}
\colorbox{red}{FALTA}

\subsubsection{Framework Web.}
\colorbox{red}{FALTA}

\subsubsection{Base de datos.}
\colorbox{red}{FALTA}

\subsubsection{Roles y Usuarios.}
\colorbox{red}{FALTA}

\subsubsection{Manejo de los datos.}
\colorbox{red}{FALTA}

\paragraph{Modelado.}
\colorbox{red}{FALTA}

\paragraph{Filtrado.}
\colorbox{red}{FALTA}

\paragraph{Visualización.}
\colorbox{red}{FALTA}

\subsection{Conclusiones.}
\colorbox{red}{FALTA}

\subsection{Caso de Uso.}
\colorbox{red}{FALTA}

\subsection{Limitaciones del sistema.}
\colorbox{red}{FALTA}
 
\section{Próximos pasos.}  
\colorbox{red}{FALTA - acá poner mejoras}

\section{Glosario.}
\colorbox{red}{FALTA}

\section{Anexo.}
\colorbox{red}{FALTA}

\section{Bibliografía.}
\colorbox{red}{FALTA}

\section{Agradecimientos.}
\colorbox{red}{FALTA}

\end{document}


%Para poner pie de paginas --> \footnote{Un conjunto de datos, conocido también como dataset, es una colección de datos habitualmente tabulada.})

%Para poner en Las citas del anexo --> información\cite{iot}. 

