\BOOKMARK [1][-]{section.1}{Introducci\363n.}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Algoritmos y t\351cnicas utilizadas.}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Objetivos del Proyecto.}{section.1}% 3
\BOOKMARK [3][-]{subsubsection.1.2.1}{Objetivo general.}{subsection.1.2}% 4
\BOOKMARK [3][-]{subsubsection.1.2.2}{Objetivos espec\355ficos.}{subsection.1.2}% 5
\BOOKMARK [2][-]{subsection.1.3}{Alcance del Proyecto.}{section.1}% 6
\BOOKMARK [2][-]{subsection.1.4}{Organizaci\363n.}{section.1}% 7
\BOOKMARK [1][-]{section.2}{Reclutamiento y selecci\363n laboral.}{}% 8
\BOOKMARK [2][-]{subsection.2.1}{Introducci\363n.}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.2}{Reclutamiento vs selecci\363n.}{section.2}% 10
\BOOKMARK [2][-]{subsection.2.3}{Evoluci\363n de los procesos de reclutamiento y selecci\363n laboral.}{section.2}% 11
\BOOKMARK [2][-]{subsection.2.4}{Cribado o screening.}{section.2}% 12
\BOOKMARK [3][-]{subsubsection.2.4.1}{Screening manual vs screening automatizado}{subsection.2.4}% 13
\BOOKMARK [2][-]{subsection.2.5}{Sistemas de screening: Estado del arte.}{section.2}% 14
\BOOKMARK [2][-]{subsection.2.6}{Enfoque del Proyecto.}{section.2}% 15
\BOOKMARK [1][-]{section.3}{Algoritmos de Machine Learning.}{}% 16
\BOOKMARK [2][-]{subsection.3.1}{Introducci\363n.}{section.3}% 17
\BOOKMARK [2][-]{subsection.3.2}{Machine Learning \(ML\).}{section.3}% 18
\BOOKMARK [3][-]{subsubsection.3.2.1}{Aprendizaje supervisado y no supervisado.}{subsection.3.2}% 19
\BOOKMARK [4][-]{paragraph.3.2.1.1}{Regresi\363n.}{subsubsection.3.2.1}% 20
\BOOKMARK [4][-]{paragraph.3.2.1.2}{Clasificaci\363n.}{subsubsection.3.2.1}% 21
\BOOKMARK [4][-]{paragraph.3.2.1.3}{Agrupaci\363n \(clustering\).}{subsubsection.3.2.1}% 22
\BOOKMARK [4][-]{paragraph.3.2.1.4}{Algoritmos m\341s conocidos.}{subsubsection.3.2.1}% 23
\BOOKMARK [3][-]{subsubsection.3.2.2}{Aprendizaje transductivo}{subsection.3.2}% 24
\BOOKMARK [3][-]{subsubsection.3.2.3}{Separaci\363n de los datos.}{subsection.3.2}% 25
\BOOKMARK [3][-]{subsubsection.3.2.4}{\277C\363mo implementar un modelo de ML?}{subsection.3.2}% 26
\BOOKMARK [4][-]{paragraph.3.2.4.1}{Cross Validation.}{subsubsection.3.2.4}% 27
\BOOKMARK [4][-]{paragraph.3.2.4.2}{Los Problemas de ML: Overfitting y Underfitting.}{subsubsection.3.2.4}% 28
\BOOKMARK [2][-]{subsection.3.3}{K-Nearest Neighbor \(KNN\).}{section.3}% 29
\BOOKMARK [3][-]{subsubsection.3.3.1}{Principal limitaci\363n KNN.}{subsection.3.3}% 30
\BOOKMARK [3][-]{subsubsection.3.3.2}{Funcionamiento y ejemplo de KNN.}{subsection.3.3}% 31
\BOOKMARK [3][-]{subsubsection.3.3.3}{M\351trica de distancia a emplear.}{subsection.3.3}% 32
\BOOKMARK [3][-]{subsubsection.3.3.4}{Eligiendo el valor de k: overfitting y underfitting.}{subsection.3.3}% 33
\BOOKMARK [2][-]{subsection.3.4}{K-means.}{section.3}% 34
\BOOKMARK [3][-]{subsubsection.3.4.1}{Funcionamiento y ejemplo de K-means.}{subsection.3.4}% 35
\BOOKMARK [3][-]{subsubsection.3.4.2}{Objetivo de k-means y su funci\363n de coste.}{subsection.3.4}% 36
\BOOKMARK [3][-]{subsubsection.3.4.3}{Limitaciones K-means.}{subsection.3.4}% 37
\BOOKMARK [4][-]{paragraph.3.4.3.1}{Obtenci\363n del k mediante Elbow Method.}{subsubsection.3.4.3}% 38
\BOOKMARK [4][-]{paragraph.3.4.3.2}{Inicializaci\363n de los centroides: k-means++.}{subsubsection.3.4.3}% 39
\BOOKMARK [2][-]{subsection.3.5}{Redes Neuronales Artificiales \(ANN\).}{section.3}% 40
\BOOKMARK [3][-]{subsubsection.3.5.1}{Perceptr\363n simple.}{subsection.3.5}% 41
\BOOKMARK [3][-]{subsubsection.3.5.2}{Desventaja del Perceptr\363n simple. }{subsection.3.5}% 42
\BOOKMARK [3][-]{subsubsection.3.5.3}{Perceptr\363n multicapa \(MLP\).}{subsection.3.5}% 43
\BOOKMARK [3][-]{subsubsection.3.5.4}{Funciones de activaci\363n en MLP.}{subsection.3.5}% 44
\BOOKMARK [4][-]{paragraph.3.5.4.1}{Softmax en la capa de salida.}{subsubsection.3.5.4}% 45
\BOOKMARK [4][-]{paragraph.3.5.4.2}{Sigmoide en la capa de salida.}{subsubsection.3.5.4}% 46
\BOOKMARK [3][-]{subsubsection.3.5.5}{Entrenamiento en una red neuronal.}{subsection.3.5}% 47
\BOOKMARK [4][-]{paragraph.3.5.5.1}{Hiper-par\341metro n.}{subsubsection.3.5.5}% 48
\BOOKMARK [1][-]{section.4}{Natural Language Processing.}{}% 49
\BOOKMARK [2][-]{subsection.4.1}{Introducci\363n.}{section.4}% 50
\BOOKMARK [2][-]{subsection.4.2}{Aplicaciones NLP.}{section.4}% 51
\BOOKMARK [2][-]{subsection.4.3}{NLP en la pr\341ctica.}{section.4}% 52
\BOOKMARK [2][-]{subsection.4.4}{Preprocesamiento de textos.}{section.4}% 53
\BOOKMARK [3][-]{subsubsection.4.4.1}{Limpieza y normalizaci\363n.}{subsection.4.4}% 54
\BOOKMARK [3][-]{subsubsection.4.4.2}{Tokenizaci\363n.}{subsection.4.4}% 55
\BOOKMARK [3][-]{subsubsection.4.4.3}{Stemming y lemmatization.}{subsection.4.4}% 56
\BOOKMARK [3][-]{subsubsection.4.4.4}{N-grams.}{subsection.4.4}% 57
\BOOKMARK [2][-]{subsection.4.5}{Obtenci\363n de representaciones vectoriales.}{section.4}% 58
\BOOKMARK [3][-]{subsubsection.4.5.1}{Bag of Words \(BoW\).}{subsection.4.5}% 59
\BOOKMARK [3][-]{subsubsection.4.5.2}{TF-IDF.}{subsection.4.5}% 60
\BOOKMARK [3][-]{subsubsection.4.5.3}{Desventajas BoW \046 TF-IDF.}{subsection.4.5}% 61
\BOOKMARK [3][-]{subsubsection.4.5.4}{Word embeddings.}{subsection.4.5}% 62
\BOOKMARK [4][-]{paragraph.4.5.4.1}{Embeddings.}{subsubsection.4.5.4}% 63
\BOOKMARK [4][-]{paragraph.4.5.4.2}{Word2vec.}{subsubsection.4.5.4}% 64
\BOOKMARK [4][-]{paragraph.4.5.4.3}{\277C\363mo obtener nuestros Word embeddings?}{subsubsection.4.5.4}% 65
\BOOKMARK [4][-]{paragraph.4.5.4.4}{CBOW vs Skipgram.}{subsubsection.4.5.4}% 66
\BOOKMARK [4][-]{paragraph.4.5.4.5}{One hot encoding.}{subsubsection.4.5.4}% 67
\BOOKMARK [4][-]{paragraph.4.5.4.6}{Obteniendo nuestros Word embeddings con Skipgram.}{subsubsection.4.5.4}% 68
\BOOKMARK [4][-]{paragraph.4.5.4.7}{Arquitectura del modelo Skipgram.}{subsubsection.4.5.4}% 69
\BOOKMARK [4][-]{paragraph.4.5.4.8}{Entrenamiento y funci\363n de costo con Softmax.}{subsubsection.4.5.4}% 70
\BOOKMARK [4][-]{paragraph.4.5.4.9}{Optimizaciones: Muestreo Negativo.}{subsubsection.4.5.4}% 71
\BOOKMARK [4][-]{paragraph.4.5.4.10}{Entrenamiento y funci\363n de costo con Sigmoide.}{subsubsection.4.5.4}% 72
\BOOKMARK [4][-]{paragraph.4.5.4.11}{Desventajas Word2Vec.}{subsubsection.4.5.4}% 73
\BOOKMARK [2][-]{subsection.4.6}{Obtenci\363n de las mediciones de similitud entre textos.}{section.4}% 74
\BOOKMARK [3][-]{subsubsection.4.6.1}{Maneras de medir la similitud entre textos.}{subsection.4.6}% 75
\BOOKMARK [3][-]{subsubsection.4.6.2}{\277Por qu\351 decidimos utilizar Cosine Similarity y WMD?}{subsection.4.6}% 76
\BOOKMARK [3][-]{subsubsection.4.6.3}{Cosine Similarity.}{subsection.4.6}% 77
\BOOKMARK [3][-]{subsubsection.4.6.4}{Word Mover\220s Distance \(WMD\).}{subsection.4.6}% 78
\BOOKMARK [1][-]{section.5}{Implementaci\363n.}{}% 79
\BOOKMARK [2][-]{subsection.5.1}{Obtenci\363n del modelo de clasificaci\363n.}{section.5}% 80
\BOOKMARK [3][-]{subsubsection.5.1.1}{Esquema.}{subsection.5.1}% 81
\BOOKMARK [3][-]{subsubsection.5.1.2}{Obtenci\363n de sets de datos.}{subsection.5.1}% 82
\BOOKMARK [4][-]{paragraph.5.1.2.1}{Curriculum Vitae.}{subsubsection.5.1.2}% 83
\BOOKMARK [4][-]{paragraph.5.1.2.2}{Descripciones Puestos Laborales.}{subsubsection.5.1.2}% 84
\BOOKMARK [3][-]{subsubsection.5.1.3}{Preprocesamiento de textos.}{subsection.5.1}% 85
\BOOKMARK [3][-]{subsubsection.5.1.4}{Cantidad final del set de datos y su uso en las distintas etapas.}{subsection.5.1}% 86
\BOOKMARK [2][-]{subsection.5.2}{Comparando textos y obteniendo similitudes.}{section.5}% 87
\BOOKMARK [3][-]{subsubsection.5.2.1}{TF-IDF \046 Cosine Similarity.}{subsection.5.2}% 88
\BOOKMARK [3][-]{subsubsection.5.2.2}{Word embeddings \(Word2vec\) \046 WMD.}{subsection.5.2}% 89
\BOOKMARK [4][-]{paragraph.5.2.2.1}{Elecci\363n de hiper-par\341metros Word2vec.}{subsubsection.5.2.2}% 90
\BOOKMARK [4][-]{paragraph.5.2.2.2}{Word Mover's Distance \(WMD\).}{subsubsection.5.2.2}% 91
\BOOKMARK [4][-]{paragraph.5.2.2.3}{Observaciones valores de Cosine Similarity y WMD.}{subsubsection.5.2.2}% 92
\BOOKMARK [2][-]{subsection.5.3}{Armado del modelo de clasificaci\363n KNN.}{section.5}% 93
\BOOKMARK [3][-]{subsubsection.5.3.1}{Preparaci\363n de los datos.}{subsection.5.3}% 94
\BOOKMARK [3][-]{subsubsection.5.3.2}{K-means.}{subsection.5.3}% 95
\BOOKMARK [3][-]{subsubsection.5.3.3}{KNN.}{subsection.5.3}% 96
\BOOKMARK [2][-]{subsection.5.4}{Clasificaci\363n de nuevas muestras y resultados obtenidos.}{section.5}% 97
\BOOKMARK [2][-]{subsection.5.5}{Integraci\363n al Sistema Web.}{section.5}% 98
\BOOKMARK [2][-]{subsection.5.6}{Integraci\363n al Sistema Web.}{section.5}% 99
\BOOKMARK [3][-]{subsubsection.5.6.1}{Django.}{subsection.5.6}% 100
\BOOKMARK [4][-]{paragraph.5.6.1.1}{Ventajas de Django.}{subsubsection.5.6.1}% 101
\BOOKMARK [4][-]{paragraph.5.6.1.2}{Arquitectura en Django.}{subsubsection.5.6.1}% 102
\BOOKMARK [3][-]{subsubsection.5.6.2}{Base de datos.}{subsection.5.6}% 103
\BOOKMARK [3][-]{subsubsection.5.6.3}{Secciones del sistema}{subsection.5.6}% 104
\BOOKMARK [3][-]{subsubsection.5.6.4}{Workflow y l\363gica final del Sistema.}{subsection.5.6}% 105
\BOOKMARK [4][-]{paragraph.5.6.4.1}{Caso de Uso.}{subsubsection.5.6.4}% 106
\BOOKMARK [1][-]{section.6}{Conclusiones.}{}% 107
\BOOKMARK [1][-]{section.7}{Anexos.}{}% 108
\BOOKMARK [2][-]{subsection.7.1}{Ejemplo de funcionamiento KNN.}{section.7}% 109
\BOOKMARK [2][-]{subsection.7.2}{Ejemplo de funcionamiento K-means.}{section.7}% 110
\BOOKMARK [2][-]{subsection.7.3}{Posici\363n inicial de los centroides en K-means.}{section.7}% 111
\BOOKMARK [2][-]{subsection.7.4}{Funciones de activaci\363n.}{section.7}% 112
\BOOKMARK [2][-]{subsection.7.5}{Ejemplo de obtenci\363n de Word embeddings mediante Skipgram y softmax.}{section.7}% 113
\BOOKMARK [2][-]{subsection.7.6}{Palabras repetidas en nuestro Corpus y palabras polis\351micas.}{section.7}% 114
